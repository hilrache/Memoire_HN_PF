{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e595cb2-8419-4835-ac8e-c7d82d324297",
   "metadata": {},
   "source": [
    "## Lemmatisation et mots/lemmes les plus fréquents du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4afc378-8a0e-45b1-bdf5-7a2c95e86382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba050e9-76ba-4b03-a5e7-551c86f2c957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4370ee23-d1db-4a3b-a6bb-2fa390975049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b94264b5-2508-4c72-84e2-4422172e7c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: fr-core-news-md\n",
      "Successfully installed fr-core-news-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_md ## chargement du modèle en français de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c776f-153c-4c33-ad7e-6c2f8f0ac84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51149f78-f024-4474-8d36-f530f438f84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1954d-c55d-4355-b950-da57a6fe70b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle spaCy français...\n",
      "Chargement du fichier /Users/charlielezin/Desktop/Candidatures58-81-290825.csv...\n",
      "Fichier chargé: 303 lignes\n",
      "\n",
      "============================================================\n",
      "COMPTAGE DES LEMMES\n",
      "============================================================\n",
      "Stopwords chargés: 707 mots\n",
      "Comptage des lemmes pour la colonne 'texte'...\n",
      "  Comptage: 1/303 textes\n",
      "  Comptage: 101/303 textes\n",
      "  Comptage: 201/303 textes\n",
      "  Comptage: 301/303 textes\n",
      "\n",
      "==================================================\n",
      "STATISTIQUES DES LEMMES - Colonne 'texte'\n",
      "==================================================\n",
      "Textes traités           :        303\n",
      "Total des lemmes         :     103628\n",
      "Lemmes uniques           :       6999\n",
      "Moyenne lemmes/texte     :     342.01\n",
      "==================================================\n",
      "Stopwords chargés: 707 mots\n",
      "Comptage des lemmes pour la colonne 'discours'...\n",
      "  Comptage: 1/303 textes\n",
      "  Comptage: 101/303 textes\n",
      "  Comptage: 201/303 textes\n",
      "  Comptage: 301/303 textes\n",
      "\n",
      "==================================================\n",
      "STATISTIQUES DES LEMMES - Colonne 'discours'\n",
      "==================================================\n",
      "Textes traités           :        303\n",
      "Total des lemmes         :      89452\n",
      "Lemmes uniques           :       6422\n",
      "Moyenne lemmes/texte     :     295.22\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "ANALYSE DES MOTS\n",
      "============================================================\n",
      "\n",
      "Analyse des mots dans la colonne 'texte'...\n",
      "Stopwords chargés: 707 mots\n",
      "\n",
      "TOP 30 - MOTS - Colonne 'texte'\n",
      "===============================\n",
      " 1. politique            :    872 occurrences\n",
      " 2. candidat             :    722 occurrences\n",
      " 3. france               :    623 occurrences\n",
      " 4. parti                :    547 occurrences\n",
      " 5. république           :    521 occurrences\n",
      " 6. travailleurs         :    495 occurrences\n",
      " 7. loir                 :    461 occurrences\n",
      " 8. gauche               :    459 occurrences\n",
      " 9. communiste           :    407 occurrences\n",
      "10. union                :    404 occurrences\n",
      "11. programme            :    401 occurrences\n",
      "12. faut                 :    388 occurrences\n",
      "13. pays                 :    376 occurrences\n",
      "14. économique           :    368 occurrences\n",
      "15. français             :    368 occurrences\n",
      "16. vie                  :    361 occurrences\n",
      "17. général              :    355 occurrences\n",
      "18. gouvernement         :    353 occurrences\n",
      "19. nationale            :    342 occurrences\n",
      "20. maire                :    331 occurrences\n",
      "21. président            :    325 occurrences\n",
      "22. élections            :    313 occurrences\n",
      "23. sociale              :    311 occurrences\n",
      "24. socialiste           :    311 occurrences\n",
      "25. travail              :    307 occurrences\n",
      "26. majorité             :    294 occurrences\n",
      "27. électeurs            :    288 occurrences\n",
      "28. législatives         :    283 occurrences\n",
      "29. député               :    282 occurrences\n",
      "30. paix                 :    279 occurrences\n",
      "\n",
      "Analyse des mots dans la colonne 'discours'...\n",
      "Stopwords chargés: 707 mots\n",
      "\n",
      "TOP 30 - MOTS - Colonne 'discours'\n",
      "==================================\n",
      " 1. politique            :    867 occurrences\n",
      " 2. france               :    578 occurrences\n",
      " 3. travailleurs         :    489 occurrences\n",
      " 4. parti                :    422 occurrences\n",
      " 5. gauche               :    406 occurrences\n",
      " 6. programme            :    389 occurrences\n",
      " 7. faut                 :    387 occurrences\n",
      " 8. pays                 :    365 occurrences\n",
      " 9. vie                  :    361 occurrences\n",
      "10. république           :    355 occurrences\n",
      "11. économique           :    351 occurrences\n",
      "12. gouvernement         :    347 occurrences\n",
      "13. communiste           :    320 occurrences\n",
      "14. travail              :    306 occurrences\n",
      "15. nationale            :    305 occurrences\n",
      "16. français             :    295 occurrences\n",
      "17. électeurs            :    282 occurrences\n",
      "18. sociale              :    280 occurrences\n",
      "19. candidat             :    274 occurrences\n",
      "20. paix                 :    273 occurrences\n",
      "21. majorité             :    263 occurrences\n",
      "22. jeunes               :    252 occurrences\n",
      "23. union                :    245 occurrences\n",
      "24. confiance            :    240 occurrences\n",
      "25. tour                 :    235 occurrences\n",
      "26. commun               :    233 occurrences\n",
      "27. socialiste           :    232 occurrences\n",
      "28. progrès              :    231 occurrences\n",
      "29. hommes               :    230 occurrences\n",
      "30. général              :    224 occurrences\n",
      "\n",
      "============================================================\n",
      "ANALYSE DES LEMMES\n",
      "============================================================\n",
      "\n",
      "Analyse des lemmes dans la colonne 'texte'...\n",
      "Stopwords chargés: 707 mots\n",
      "Analyse des lemmes pour la colonne 'texte'...\n",
      "  Traitement: 1/303 textes\n",
      "  Traitement: 101/303 textes\n",
      "  Traitement: 201/303 textes\n",
      "  Traitement: 301/303 textes\n",
      "\n",
      "TOP 30 - LEMMES - Colonne 'texte'\n",
      "=================================\n",
      " 1. politique            :   1023 occurrences\n",
      " 2. candidat             :    965 occurrences\n",
      " 3. social               :    787 occurrences\n",
      " 4. parti                :    648 occurrences\n",
      " 5. français             :    630 occurrences\n",
      " 6. france               :    620 occurrences\n",
      " 7. voter                :    615 occurrences\n",
      " 8. national             :    594 occurrences\n",
      " 9. communiste           :    592 occurrences\n",
      "10. travailleur          :    545 occurrences\n",
      "11. république           :    520 occurrences\n",
      "12. gauche               :    463 occurrences\n",
      "13. loir                 :    461 occurrences\n",
      "14. économique           :    435 occurrences\n",
      "15. socialiste           :    430 occurrences\n",
      "16. gouvernement         :    422 occurrences\n",
      "17. programme            :    420 occurrences\n",
      "18. général              :    408 occurrences\n",
      "19. union                :    404 occurrences\n",
      "20. liberté              :    401 occurrences\n",
      "21. grand                :    401 occurrences\n",
      "22. pays                 :    376 occurrences\n",
      "23. homme                :    374 occurrences\n",
      "24. maire                :    371 occurrences\n",
      "25. agricole             :    369 occurrences\n",
      "26. élection             :    368 occurrences\n",
      "27. vie                  :    362 occurrences\n",
      "28. vouloir              :    341 occurrences\n",
      "29. président            :    340 occurrences\n",
      "30. député               :    340 occurrences\n",
      "\n",
      "Analyse des lemmes dans la colonne 'discours'...\n",
      "Stopwords chargés: 707 mots\n",
      "Analyse des lemmes pour la colonne 'discours'...\n",
      "  Traitement: 1/303 textes\n",
      "  Traitement: 101/303 textes\n",
      "  Traitement: 201/303 textes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def load_and_prepare_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Charge le fichier CSV et prépare les données.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    \n",
    "    # Vérifier que les colonnes existent\n",
    "    required_columns = ['texte', 'discours']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing_columns}\")\n",
    "    \n",
    "    # Remplacer les valeurs NaN par des chaînes vides\n",
    "    df['texte'] = df['texte'].fillna('')\n",
    "    df['discours'] = df['discours'].fillna('')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Nettoie et préprocesse le texte.\"\"\"\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer la ponctuation et les caractères spéciaux (garder seulement lettres, chiffres, espaces)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def get_french_stopwords(stopwords_file_path: str = 'stopwords.txt') -> set:\n",
    "    \"\"\"\n",
    "    Retourne un ensemble de mots-outils français à partir d'un fichier,\n",
    "    augmenté avec des chaînes spécifiques à exclure.\n",
    "    \"\"\"\n",
    "    stopwords = set()\n",
    "    \n",
    "    # Charger les stopwords depuis le fichier\n",
    "    try:\n",
    "        with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Nettoyer chaque ligne (enlever espaces et retours à la ligne)\n",
    "                word = line.strip().lower()\n",
    "                if word:  # Ignorer les lignes vides\n",
    "                    stopwords.add(word)\n",
    "        print(f\"Stopwords chargés: {len(stopwords)} mots\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Attention: Le fichier {stopwords_file_path} n'a pas été trouvé.\")\n",
    "        print(\"Utilisation d'un ensemble vide de stopwords.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "        print(\"Utilisation d'un ensemble vide de stopwords.\")\n",
    "    \n",
    "    # Ajouter les chaînes spécifiques à exclure (en minuscules pour la cohérence)\n",
    "    custom_stopwords = {\n",
    "        'sciences po / fonds cevipof',\n",
    "        'page 1/1',\n",
    "        'page 1/2', \n",
    "        'page 2/2'\n",
    "    }\n",
    "    \n",
    "    return stopwords.union(custom_stopwords)\n",
    "\n",
    "def analyze_words(text_series: pd.Series, column_name: str, stopwords_file_path: str = 'stopwords.txt') -> Dict[str, int]:\n",
    "    \"\"\"Analyse les occurrences de mots dans une série de textes.\"\"\"\n",
    "    stopwords = get_french_stopwords(stopwords_file_path)\n",
    "    word_counter = Counter()\n",
    "    \n",
    "    for text in text_series:\n",
    "        if pd.isna(text) or text == '':\n",
    "            continue\n",
    "            \n",
    "        # Préprocesser le texte\n",
    "        clean_text = preprocess_text(str(text))\n",
    "        \n",
    "        # Diviser en mots\n",
    "        words = clean_text.split()\n",
    "        \n",
    "        # Filtrer les mots-outils et les mots trop courts\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if len(word) > 2 and word not in stopwords and word.isalpha()\n",
    "        ]\n",
    "        \n",
    "        word_counter.update(filtered_words)\n",
    "    \n",
    "    return dict(word_counter)\n",
    "\n",
    "def analyze_lemmas(text_series: pd.Series, column_name: str, nlp, stopwords_file_path: str = 'stopwords.txt') -> Dict[str, int]:\n",
    "    \"\"\"Analyse les occurrences de lemmes dans une série de textes.\"\"\"\n",
    "    stopwords = get_french_stopwords(stopwords_file_path)\n",
    "    lemma_counter = Counter()\n",
    "    \n",
    "    print(f\"Analyse des lemmes pour la colonne '{column_name}'...\")\n",
    "    \n",
    "    for i, text in enumerate(text_series):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Traitement: {i+1}/{len(text_series)} textes\")\n",
    "            \n",
    "        if pd.isna(text) or text == '':\n",
    "            continue\n",
    "            \n",
    "        # Préprocesser le texte\n",
    "        clean_text = preprocess_text(str(text))\n",
    "        \n",
    "        # Traitement avec spaCy (par chunks pour éviter les limites de mémoire)\n",
    "        max_length = 1000000  # Limite de spaCy\n",
    "        if len(clean_text) > max_length:\n",
    "            chunks = [clean_text[i:i+max_length] for i in range(0, len(clean_text), max_length)]\n",
    "        else:\n",
    "            chunks = [clean_text]\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                doc = nlp(chunk)\n",
    "                \n",
    "                # Extraire les lemmes\n",
    "                lemmas = [\n",
    "                    token.lemma_.lower() for token in doc \n",
    "                    if (not token.is_stop and \n",
    "                        not token.is_punct and \n",
    "                        not token.is_space and \n",
    "                        len(token.lemma_) > 2 and\n",
    "                        token.lemma_.lower() not in stopwords and\n",
    "                        token.lemma_.isalpha())\n",
    "                ]\n",
    "                \n",
    "                lemma_counter.update(lemmas)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement d'un chunk: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return dict(lemma_counter)\n",
    "\n",
    "def count_lemmas_in_series(text_series: pd.Series, column_name: str, nlp, stopwords_file_path: str = 'stopwords.txt') -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Compte le nombre total de lemmes dans une série de textes.\n",
    "    \"\"\"\n",
    "    stopwords = get_french_stopwords(stopwords_file_path)\n",
    "    total_lemmas = 0\n",
    "    total_unique_lemmas = set()\n",
    "    processed_texts = 0\n",
    "    \n",
    "    print(f\"Comptage des lemmes pour la colonne '{column_name}'...\")\n",
    "    \n",
    "    for i, text in enumerate(text_series):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Comptage: {i+1}/{len(text_series)} textes\")\n",
    "            \n",
    "        if pd.isna(text) or text == '':\n",
    "            continue\n",
    "            \n",
    "        processed_texts += 1\n",
    "        \n",
    "        # Préprocesser le texte\n",
    "        clean_text = preprocess_text(str(text))\n",
    "        \n",
    "        # Traitement avec spaCy (par chunks pour éviter les limites de mémoire)\n",
    "        max_length = 1000000  # Limite de spaCy\n",
    "        if len(clean_text) > max_length:\n",
    "            chunks = [clean_text[i:i+max_length] for i in range(0, len(clean_text), max_length)]\n",
    "        else:\n",
    "            chunks = [clean_text]\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                doc = nlp(chunk)\n",
    "                \n",
    "                # Compter et collecter les lemmes\n",
    "                for token in doc:\n",
    "                    if (not token.is_stop and \n",
    "                        not token.is_punct and \n",
    "                        not token.is_space and \n",
    "                        len(token.lemma_) > 2 and\n",
    "                        token.lemma_.lower() not in stopwords and\n",
    "                        token.lemma_.isalpha()):\n",
    "                        \n",
    "                        total_lemmas += 1\n",
    "                        total_unique_lemmas.add(token.lemma_.lower())\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement d'un chunk: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return {\n",
    "        'total_lemmas': total_lemmas,\n",
    "        'unique_lemmas': len(total_unique_lemmas),\n",
    "        'processed_texts': processed_texts,\n",
    "        'average_lemmas_per_text': total_lemmas / processed_texts if processed_texts > 0 else 0\n",
    "    }\n",
    "\n",
    "def display_top_results(results: Dict[str, int], title: str, top_n: int = 30):\n",
    "    \"\"\"Affiche les top résultats.\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    \n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (item, count) in enumerate(sorted_results[:top_n], 1):\n",
    "        print(f\"{i:2d}. {item:<20} : {count:>6} occurrences\")\n",
    "\n",
    "def display_lemma_stats(stats: Dict[str, int], column_name: str):\n",
    "    \"\"\"Affiche les statistiques de comptage des lemmes.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"STATISTIQUES DES LEMMES - Colonne '{column_name}'\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Textes traités           : {stats['processed_texts']:>10}\")\n",
    "    print(f\"Total des lemmes         : {stats['total_lemmas']:>10}\")\n",
    "    print(f\"Lemmes uniques           : {stats['unique_lemmas']:>10}\")\n",
    "    print(f\"Moyenne lemmes/texte     : {stats['average_lemmas_per_text']:>10.2f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    file_path = \"/Users/charlielezin/Desktop/Candidatures58-81-290825.csv\"\n",
    "    stopwords_file_path = \"/Users/charlielezin/Desktop/stopwords.txt\"\n",
    "    \n",
    "    try:\n",
    "        # Charger spaCy pour le français\n",
    "        print(\"Chargement du modèle spaCy français...\")\n",
    "        try:\n",
    "            nlp = spacy.load(\"fr_core_news_md\")\n",
    "        except OSError:\n",
    "            print(\"Erreur: Le modèle spaCy français n'est pas installé.\")\n",
    "            print(\"Installez-le avec: python -m spacy download fr_core_news_md\")\n",
    "            return\n",
    "        \n",
    "        # Charger les données\n",
    "        print(f\"Chargement du fichier {file_path}...\")\n",
    "        df = load_and_prepare_data(file_path)\n",
    "        print(f\"Fichier chargé: {len(df)} lignes\")\n",
    "        \n",
    "        # Comptage des lemmes pour chaque colonne\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPTAGE DES LEMMES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for column in ['texte', 'discours']:\n",
    "            lemma_stats = count_lemmas_in_series(df[column], column, nlp, stopwords_file_path)\n",
    "            display_lemma_stats(lemma_stats, column)\n",
    "        \n",
    "        # Analyser les mots pour chaque colonne\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSE DES MOTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for column in ['texte', 'discours']:\n",
    "            print(f\"\\nAnalyse des mots dans la colonne '{column}'...\")\n",
    "            word_results = analyze_words(df[column], column, stopwords_file_path)\n",
    "            display_top_results(word_results, f\"TOP 30 - MOTS - Colonne '{column}'\", 30)\n",
    "        \n",
    "        # Analyser les lemmes pour chaque colonne\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSE DES LEMMES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for column in ['texte', 'discours']:\n",
    "            print(f\"\\nAnalyse des lemmes dans la colonne '{column}'...\")\n",
    "            lemma_results = analyze_lemmas(df[column], column, nlp, stopwords_file_path)\n",
    "            display_top_results(lemma_results, f\"TOP 30 - LEMMES - Colonne '{column}'\", 30)\n",
    "        \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur: Le fichier '{file_path}' est introuvable.\")\n",
    "        print(\"Vérifiez que le fichier est dans le même répertoire que le script.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846da591-bfc3-4e8a-bd8d-c907f9517a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5337f-2bc4-4f6e-a24e-e3879ab9777d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97c7c7-2565-4490-9aad-c01963a4d9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49020ef-05ef-45c7-aad5-0b9b9ba1ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script pour afficher 50 lemmes au hasard avec leurs mots associés\n",
    "Basé sur l'analyse existante des discours politiques\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "class LemmaWordMapper:\n",
    "    def __init__(self, filepath='/Users/charlielezin/Desktop/Candidatures58-81-290825.csv'):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "        self.nlp = None\n",
    "        self.lemma_to_words = defaultdict(set)\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Charge les données des discours\"\"\"\n",
    "        print(\"Chargement des données...\")\n",
    "        self.df = pd.read_csv(self.filepath, delimiter=';', encoding='utf-8')\n",
    "        \n",
    "        # Filtrer pour les tours 1 et 2 avec discours\n",
    "        self.df = self.df[self.df['tour'].isin([1, 2])].copy()\n",
    "        self.df = self.df[\n",
    "            (self.df['discours'].notna()) & \n",
    "            (self.df['discours'].str.strip() != '') &\n",
    "            (self.df['discours'].str.len() > 100)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Nombre de discours chargés : {len(self.df)}\")\n",
    "        return self.df\n",
    "    \n",
    "    def load_spacy_model(self):\n",
    "        \"\"\"Charge le modèle spaCy français\"\"\"\n",
    "        print(\"Chargement du modèle spaCy français...\")\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"fr_core_news_md\")\n",
    "            print(\"Modèle spaCy chargé avec succès\")\n",
    "        except OSError:\n",
    "            print(\"Erreur: Le modèle spaCy français n'est pas installé.\")\n",
    "            print(\"Installez-le avec: python -m spacy download fr_core_news_md\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Nettoie le texte\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def get_french_stopwords(self, stopwords_file_path='/Users/charlielezin/Desktop/stopwords.txt'):\n",
    "        \"\"\"Charge les mots vides français depuis un fichier\"\"\"\n",
    "        stopwords = set()\n",
    "        \n",
    "        try:\n",
    "            with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    word = line.strip().lower()\n",
    "                    if word:\n",
    "                        stopwords.add(word)\n",
    "            print(f\"Stopwords chargés depuis {stopwords_file_path}: {len(stopwords)} mots\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Attention: Le fichier {stopwords_file_path} n'a pas été trouvé.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier stopwords: {e}\")\n",
    "        \n",
    "        return stopwords\n",
    "    \n",
    "    def extract_lemma_word_mapping(self):\n",
    "        \"\"\"Extrait le mapping lemme -> mots pour tous les discours\"\"\"\n",
    "        print(\"Extraction des lemmes et mots associés...\")\n",
    "        \n",
    "        stopwords = self.get_french_stopwords()\n",
    "        \n",
    "        for i, row in self.df.iterrows():\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"  Traitement: {i+1}/{len(self.df)} discours\")\n",
    "            \n",
    "            text = self.preprocess_text(row['discours'])\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            # Traiter par chunks pour éviter les limites de spaCy\n",
    "            max_length = 500000\n",
    "            if len(text) > max_length:\n",
    "                chunks = [text[j:j+max_length] for j in range(0, len(text), max_length)]\n",
    "            else:\n",
    "                chunks = [text]\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                try:\n",
    "                    doc = self.nlp(chunk)\n",
    "                    \n",
    "                    for token in doc:\n",
    "                        if (not token.is_stop and \n",
    "                            not token.is_punct and \n",
    "                            not token.is_space and \n",
    "                            len(token.text) > 2 and\n",
    "                            len(token.lemma_) > 2 and\n",
    "                            token.text.lower() not in stopwords and\n",
    "                            token.lemma_.lower() not in stopwords and\n",
    "                            token.text.isalpha() and\n",
    "                            token.lemma_.isalpha()):\n",
    "                            \n",
    "                            lemma = token.lemma_.lower()\n",
    "                            word = token.text.lower()\n",
    "                            \n",
    "                            # Ajouter seulement si le mot est différent du lemme\n",
    "                            if word != lemma:\n",
    "                                self.lemma_to_words[lemma].add(word)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du traitement d'un chunk: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Extraction terminée. {len(self.lemma_to_words)} lemmes collectés\")\n",
    "    \n",
    "    def display_random_lemmas(self, n=50):\n",
    "        \"\"\"Affiche n lemmes au hasard avec leurs mots associés\"\"\"\n",
    "        print(f\"AFFICHAGE DE {n} LEMMES ALÉATOIRES AVEC LEURS MOTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Filtrer les lemmes qui ont au moins un mot associé différent\n",
    "        valid_lemmas = {lemma: words for lemma, words in self.lemma_to_words.items() if len(words) > 0}\n",
    "        \n",
    "        if len(valid_lemmas) == 0:\n",
    "            print(\"Aucun lemme avec des mots associés trouvé\")\n",
    "            return\n",
    "        \n",
    "        # Sélectionner aléatoirement n lemmes\n",
    "        selected_lemmas = random.sample(list(valid_lemmas.keys()), min(n, len(valid_lemmas)))\n",
    "        \n",
    "        for i, lemma in enumerate(selected_lemmas, 1):\n",
    "            associated_words = sorted(list(valid_lemmas[lemma]))\n",
    "            words_str = \", \".join(associated_words)\n",
    "            \n",
    "            print(f\"{i:2d}. {lemma:<15} -> {words_str}\")\n",
    "        \n",
    "        print(f\"\\nTotal de lemmes disponibles : {len(valid_lemmas)}\")\n",
    "        print(f\"Lemmes affichés : {len(selected_lemmas)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Analyse principale\"\"\"\n",
    "    print(\"ANALYSE LEMMES-MOTS - DISCOURS POLITIQUES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        mapper = LemmaWordMapper()\n",
    "        \n",
    "        # Chargement du modèle spaCy\n",
    "        if not mapper.load_spacy_model():\n",
    "            return\n",
    "        \n",
    "        # Chargement des données\n",
    "        mapper.load_data()\n",
    "        \n",
    "        if len(mapper.df) == 0:\n",
    "            print(\"Aucun discours trouvé!\")\n",
    "            return\n",
    "        \n",
    "        # Extraction du mapping lemmes-mots\n",
    "        mapper.extract_lemma_word_mapping()\n",
    "        \n",
    "        # Affichage aléatoire\n",
    "        mapper.display_random_lemmas(50)\n",
    "    \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0146bdf-ecc7-45c5-8705-60b80072aa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135b9c63-5a0a-45a3-a7a0-58320f37301a",
   "metadata": {},
   "source": [
    "## Modèle large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35dffaa8-d380-4190-9e1d-3d87450372d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.8.0/fr_core_news_lg-3.8.0-py3-none-any.whl (571.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.8/571.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr-core-news-lg\n",
      "Successfully installed fr-core-news-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bf0c3ed-dfd8-451a-8538-923ea25e2e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle spaCy français...\n",
      "Chargement du fichier /Users/charlielezin/Desktop/Candidatures58-81-290825.csv...\n",
      "Fichier chargé: 303 lignes\n",
      "\n",
      "============================================================\n",
      "COMPTAGE DES LEMMES\n",
      "============================================================\n",
      "Stopwords chargés: 707 mots\n",
      "Comptage des lemmes pour la colonne 'texte'...\n",
      "  Comptage: 1/303 textes\n",
      "  Comptage: 101/303 textes\n",
      "  Comptage: 201/303 textes\n",
      "  Comptage: 301/303 textes\n",
      "\n",
      "==================================================\n",
      "STATISTIQUES DES LEMMES - Colonne 'texte'\n",
      "==================================================\n",
      "Textes traités           :        303\n",
      "Total des lemmes         :     103629\n",
      "Lemmes uniques           :       6892\n",
      "Moyenne lemmes/texte     :     342.01\n",
      "==================================================\n",
      "Stopwords chargés: 707 mots\n",
      "Comptage des lemmes pour la colonne 'discours'...\n",
      "  Comptage: 1/303 textes\n",
      "  Comptage: 101/303 textes\n",
      "  Comptage: 201/303 textes\n",
      "  Comptage: 301/303 textes\n",
      "\n",
      "==================================================\n",
      "STATISTIQUES DES LEMMES - Colonne 'discours'\n",
      "==================================================\n",
      "Textes traités           :        303\n",
      "Total des lemmes         :      89455\n",
      "Lemmes uniques           :       6317\n",
      "Moyenne lemmes/texte     :     295.23\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "ANALYSE DES MOTS\n",
      "============================================================\n",
      "\n",
      "Analyse des mots dans la colonne 'texte'...\n",
      "Stopwords chargés: 707 mots\n",
      "\n",
      "TOP 30 - MOTS - Colonne 'texte'\n",
      "===============================\n",
      " 1. politique            :    872 occurrences\n",
      " 2. candidat             :    722 occurrences\n",
      " 3. france               :    623 occurrences\n",
      " 4. parti                :    547 occurrences\n",
      " 5. république           :    521 occurrences\n",
      " 6. travailleurs         :    495 occurrences\n",
      " 7. loir                 :    461 occurrences\n",
      " 8. gauche               :    459 occurrences\n",
      " 9. communiste           :    407 occurrences\n",
      "10. union                :    404 occurrences\n",
      "11. programme            :    401 occurrences\n",
      "12. faut                 :    388 occurrences\n",
      "13. pays                 :    376 occurrences\n",
      "14. économique           :    368 occurrences\n",
      "15. français             :    368 occurrences\n",
      "16. vie                  :    361 occurrences\n",
      "17. général              :    355 occurrences\n",
      "18. gouvernement         :    353 occurrences\n",
      "19. nationale            :    342 occurrences\n",
      "20. maire                :    331 occurrences\n",
      "21. président            :    325 occurrences\n",
      "22. élections            :    313 occurrences\n",
      "23. socialiste           :    311 occurrences\n",
      "24. sociale              :    311 occurrences\n",
      "25. travail              :    307 occurrences\n",
      "26. majorité             :    294 occurrences\n",
      "27. électeurs            :    288 occurrences\n",
      "28. législatives         :    283 occurrences\n",
      "29. député               :    282 occurrences\n",
      "30. paix                 :    279 occurrences\n",
      "\n",
      "Analyse des mots dans la colonne 'discours'...\n",
      "Stopwords chargés: 707 mots\n",
      "\n",
      "TOP 30 - MOTS - Colonne 'discours'\n",
      "==================================\n",
      " 1. politique            :    867 occurrences\n",
      " 2. france               :    578 occurrences\n",
      " 3. travailleurs         :    489 occurrences\n",
      " 4. parti                :    422 occurrences\n",
      " 5. gauche               :    406 occurrences\n",
      " 6. programme            :    389 occurrences\n",
      " 7. faut                 :    387 occurrences\n",
      " 8. pays                 :    365 occurrences\n",
      " 9. vie                  :    361 occurrences\n",
      "10. république           :    355 occurrences\n",
      "11. économique           :    351 occurrences\n",
      "12. gouvernement         :    347 occurrences\n",
      "13. communiste           :    320 occurrences\n",
      "14. travail              :    306 occurrences\n",
      "15. nationale            :    305 occurrences\n",
      "16. français             :    295 occurrences\n",
      "17. électeurs            :    282 occurrences\n",
      "18. sociale              :    280 occurrences\n",
      "19. candidat             :    274 occurrences\n",
      "20. paix                 :    273 occurrences\n",
      "21. majorité             :    263 occurrences\n",
      "22. jeunes               :    252 occurrences\n",
      "23. union                :    245 occurrences\n",
      "24. confiance            :    240 occurrences\n",
      "25. tour                 :    235 occurrences\n",
      "26. commun               :    233 occurrences\n",
      "27. socialiste           :    232 occurrences\n",
      "28. progrès              :    231 occurrences\n",
      "29. hommes               :    230 occurrences\n",
      "30. général              :    224 occurrences\n",
      "\n",
      "============================================================\n",
      "ANALYSE DES LEMMES\n",
      "============================================================\n",
      "\n",
      "Analyse des lemmes dans la colonne 'texte'...\n",
      "Stopwords chargés: 707 mots\n",
      "Analyse des lemmes pour la colonne 'texte'...\n",
      "  Traitement: 1/303 textes\n",
      "  Traitement: 101/303 textes\n",
      "  Traitement: 201/303 textes\n",
      "  Traitement: 301/303 textes\n",
      "\n",
      "TOP 30 - LEMMES - Colonne 'texte'\n",
      "=================================\n",
      " 1. politique            :   1023 occurrences\n",
      " 2. candidat             :    965 occurrences\n",
      " 3. social               :    787 occurrences\n",
      " 4. voter                :    681 occurrences\n",
      " 5. parti                :    658 occurrences\n",
      " 6. français             :    630 occurrences\n",
      " 7. france               :    623 occurrences\n",
      " 8. national             :    593 occurrences\n",
      " 9. communiste           :    592 occurrences\n",
      "10. travailleur          :    545 occurrences\n",
      "11. république           :    521 occurrences\n",
      "12. loir                 :    464 occurrences\n",
      "13. gauche               :    463 occurrences\n",
      "14. économique           :    435 occurrences\n",
      "15. socialiste           :    430 occurrences\n",
      "16. gouvernement         :    422 occurrences\n",
      "17. programme            :    420 occurrences\n",
      "18. électeur             :    418 occurrences\n",
      "19. général              :    408 occurrences\n",
      "20. union                :    404 occurrences\n",
      "21. grand                :    401 occurrences\n",
      "22. liberté              :    401 occurrences\n",
      "23. maire                :    378 occurrences\n",
      "24. homme                :    376 occurrences\n",
      "25. pays                 :    376 occurrences\n",
      "26. agricole             :    369 occurrences\n",
      "27. élection             :    368 occurrences\n",
      "28. vie                  :    362 occurrences\n",
      "29. vouloir              :    342 occurrences\n",
      "30. président            :    340 occurrences\n",
      "\n",
      "Analyse des lemmes dans la colonne 'discours'...\n",
      "Stopwords chargés: 707 mots\n",
      "Analyse des lemmes pour la colonne 'discours'...\n",
      "  Traitement: 1/303 textes\n",
      "  Traitement: 101/303 textes\n",
      "  Traitement: 201/303 textes\n",
      "  Traitement: 301/303 textes\n",
      "\n",
      "TOP 30 - LEMMES - Colonne 'discours'\n",
      "====================================\n",
      " 1. politique            :    998 occurrences\n",
      " 2. social               :    727 occurrences\n",
      " 3. france               :    578 occurrences\n",
      " 4. parti                :    539 occurrences\n",
      " 5. travailleur          :    539 occurrences\n",
      " 6. voter                :    525 occurrences\n",
      " 7. national             :    521 occurrences\n",
      " 8. communiste           :    504 occurrences\n",
      " 9. candidat             :    474 occurrences\n",
      "10. français             :    418 occurrences\n",
      "11. économique           :    417 occurrences\n",
      "12. gouvernement         :    416 occurrences\n",
      "13. programme            :    408 occurrences\n",
      "14. gauche               :    407 occurrences\n",
      "15. grand                :    394 occurrences\n",
      "16. électeur             :    394 occurrences\n",
      "17. homme                :    367 occurrences\n",
      "18. pays                 :    365 occurrences\n",
      "19. vie                  :    362 occurrences\n",
      "20. république           :    355 occurrences\n",
      "21. liberté              :    353 occurrences\n",
      "22. socialiste           :    350 occurrences\n",
      "23. vouloir              :    338 occurrences\n",
      "24. travail              :    336 occurrences\n",
      "25. assurer              :    320 occurrences\n",
      "26. agricole             :    303 occurrences\n",
      "27. petit                :    278 occurrences\n",
      "28. jeune                :    277 occurrences\n",
      "29. général              :    275 occurrences\n",
      "30. paix                 :    273 occurrences\n",
      "\n",
      "Analyse terminée!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def load_and_prepare_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Charge le fichier CSV et prépare les données.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    \n",
    "    # Vérifier que les colonnes existent\n",
    "    required_columns = ['texte', 'discours']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing_columns}\")\n",
    "    \n",
    "    # Remplacer les valeurs NaN par des chaînes vides\n",
    "    df['texte'] = df['texte'].fillna('')\n",
    "    df['discours'] = df['discours'].fillna('')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Nettoie et préprocesse le texte.\"\"\"\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer la ponctuation et les caractères spéciaux (garder seulement lettres, chiffres, espaces)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def get_french_stopwords(stopwords_file_path: str = 'stopwords.txt') -> set:\n",
    "    \"\"\"\n",
    "    Retourne un ensemble de mots-outils français à partir d'un fichier,\n",
    "    augmenté avec des chaînes spécifiques à exclure.\n",
    "    \"\"\"\n",
    "    stopwords = set()\n",
    "    \n",
    "    # Charger les stopwords depuis le fichier\n",
    "    try:\n",
    "        with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Nettoyer chaque ligne (enlever espaces et retours à la ligne)\n",
    "                word = line.strip().lower()\n",
    "                if word:  # Ignorer les lignes vides\n",
    "                    stopwords.add(word)\n",
    "        print(f\"Stopwords chargés: {len(stopwords)} mots\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Attention: Le fichier {stopwords_file_path} n'a pas été trouvé.\")\n",
    "        print(\"Utilisation d'un ensemble vide de stopwords.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "        print(\"Utilisation d'un ensemble vide de stopwords.\")\n",
    "    \n",
    "    # Ajouter les chaînes spécifiques à exclure (en minuscules pour la cohérence)\n",
    "    custom_stopwords = {\n",
    "        'sciences po / fonds cevipof',\n",
    "        'page 1/1',\n",
    "        'page 1/2', \n",
    "        'page 2/2'\n",
    "    }\n",
    "    \n",
    "    return stopwords.union(custom_stopwords)\n",
    "\n",
    "def analyze_words(text_series: pd.Series, column_name: str, stopwords_file_path: str = 'stopwords.txt') -> Dict[str, int]:\n",
    "    \"\"\"Analyse les occurrences de mots dans une série de textes.\"\"\"\n",
    "    stopwords = get_french_stopwords(stopwords_file_path)\n",
    "    word_counter = Counter()\n",
    "    \n",
    "    for text in text_series:\n",
    "        if pd.isna(text) or text == '':\n",
    "            continue\n",
    "            \n",
    "        # Préprocesser le texte\n",
    "        clean_text = preprocess_text(str(text))\n",
    "        \n",
    "        # Diviser en mots\n",
    "        words = clean_text.split()\n",
    "        \n",
    "        # Filtrer les mots-outils et les mots trop courts\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if len(word) > 2 and word not in stopwords and word.isalpha()\n",
    "        ]\n",
    "        \n",
    "        word_counter.update(filtered_words)\n",
    "    \n",
    "    return dict(word_counter)\n",
    "\n",
    "def analyze_lemmas(text_series: pd.Series, column_name: str, nlp, stopwords_file_path: str = 'stopwords.txt') -> Dict[str, int]:\n",
    "    \"\"\"Analyse les occurrences de lemmes dans une série de textes.\"\"\"\n",
    "    stopwords = get_french_stopwords(stopwords_file_path)\n",
    "    lemma_counter = Counter()\n",
    "    \n",
    "    print(f\"Analyse des lemmes pour la colonne '{column_name}'...\")\n",
    "    \n",
    "    for i, text in enumerate(text_series):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Traitement: {i+1}/{len(text_series)} textes\")\n",
    "            \n",
    "        if pd.isna(text) or text == '':\n",
    "            continue\n",
    "            \n",
    "        # Préprocesser le texte\n",
    "        clean_text = preprocess_text(str(text))\n",
    "        \n",
    "        # Traitement avec spaCy (par chunks pour éviter les limites de mémoire)\n",
    "        max_length = 1000000  # Limite de spaCy\n",
    "        if len(clean_text) > max_length:\n",
    "            chunks = [clean_text[i:i+max_length] for i in range(0, len(clean_text), max_length)]\n",
    "        else:\n",
    "            chunks = [clean_text]\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                doc = nlp(chunk)\n",
    "                \n",
    "                # Extraire les lemmes\n",
    "                lemmas = [\n",
    "                    token.lemma_.lower() for token in doc \n",
    "                    if (not token.is_stop and \n",
    "                        not token.is_punct and \n",
    "                        not token.is_space and \n",
    "                        len(token.lemma_) > 2 and\n",
    "                        token.lemma_.lower() not in stopwords and\n",
    "                        token.lemma_.isalpha())\n",
    "                ]\n",
    "                \n",
    "                lemma_counter.update(lemmas)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement d'un chunk: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return dict(lemma_counter)\n",
    "\n",
    "def count_lemmas_in_series(text_series: pd.Series, column_name: str, nlp, stopwords_file_path: str = 'stopwords.txt') -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Compte le nombre total de lemmes dans une série de textes.\n",
    "    \"\"\"\n",
    "    stopwords = get_french_stopwords(stopwords_file_path)\n",
    "    total_lemmas = 0\n",
    "    total_unique_lemmas = set()\n",
    "    processed_texts = 0\n",
    "    \n",
    "    print(f\"Comptage des lemmes pour la colonne '{column_name}'...\")\n",
    "    \n",
    "    for i, text in enumerate(text_series):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Comptage: {i+1}/{len(text_series)} textes\")\n",
    "            \n",
    "        if pd.isna(text) or text == '':\n",
    "            continue\n",
    "            \n",
    "        processed_texts += 1\n",
    "        \n",
    "        # Préprocesser le texte\n",
    "        clean_text = preprocess_text(str(text))\n",
    "        \n",
    "        # Traitement avec spaCy (par chunks pour éviter les limites de mémoire)\n",
    "        max_length = 1000000  # Limite de spaCy\n",
    "        if len(clean_text) > max_length:\n",
    "            chunks = [clean_text[i:i+max_length] for i in range(0, len(clean_text), max_length)]\n",
    "        else:\n",
    "            chunks = [clean_text]\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                doc = nlp(chunk)\n",
    "                \n",
    "                # Compter et collecter les lemmes\n",
    "                for token in doc:\n",
    "                    if (not token.is_stop and \n",
    "                        not token.is_punct and \n",
    "                        not token.is_space and \n",
    "                        len(token.lemma_) > 2 and\n",
    "                        token.lemma_.lower() not in stopwords and\n",
    "                        token.lemma_.isalpha()):\n",
    "                        \n",
    "                        total_lemmas += 1\n",
    "                        total_unique_lemmas.add(token.lemma_.lower())\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement d'un chunk: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return {\n",
    "        'total_lemmas': total_lemmas,\n",
    "        'unique_lemmas': len(total_unique_lemmas),\n",
    "        'processed_texts': processed_texts,\n",
    "        'average_lemmas_per_text': total_lemmas / processed_texts if processed_texts > 0 else 0\n",
    "    }\n",
    "\n",
    "def display_top_results(results: Dict[str, int], title: str, top_n: int = 30):\n",
    "    \"\"\"Affiche les top résultats.\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * len(title))\n",
    "    \n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (item, count) in enumerate(sorted_results[:top_n], 1):\n",
    "        print(f\"{i:2d}. {item:<20} : {count:>6} occurrences\")\n",
    "\n",
    "def display_lemma_stats(stats: Dict[str, int], column_name: str):\n",
    "    \"\"\"Affiche les statistiques de comptage des lemmes.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"STATISTIQUES DES LEMMES - Colonne '{column_name}'\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Textes traités           : {stats['processed_texts']:>10}\")\n",
    "    print(f\"Total des lemmes         : {stats['total_lemmas']:>10}\")\n",
    "    print(f\"Lemmes uniques           : {stats['unique_lemmas']:>10}\")\n",
    "    print(f\"Moyenne lemmes/texte     : {stats['average_lemmas_per_text']:>10.2f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    file_path = \"/Users/charlielezin/Desktop/Candidatures58-81-290825.csv\"\n",
    "    stopwords_file_path = \"/Users/charlielezin/Desktop/stopwords.txt\"\n",
    "    \n",
    "    try:\n",
    "        # Charger spaCy pour le français\n",
    "        print(\"Chargement du modèle spaCy français...\")\n",
    "        try:\n",
    "            nlp = spacy.load(\"fr_core_news_lg\")\n",
    "        except OSError:\n",
    "            print(\"Erreur: Le modèle spaCy français n'est pas installé.\")\n",
    "            print(\"Installez-le avec: python -m spacy download fr_core_news_lg\")\n",
    "            return\n",
    "        \n",
    "        # Charger les données\n",
    "        print(f\"Chargement du fichier {file_path}...\")\n",
    "        df = load_and_prepare_data(file_path)\n",
    "        print(f\"Fichier chargé: {len(df)} lignes\")\n",
    "        \n",
    "        # Comptage des lemmes pour chaque colonne\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPTAGE DES LEMMES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for column in ['texte', 'discours']:\n",
    "            lemma_stats = count_lemmas_in_series(df[column], column, nlp, stopwords_file_path)\n",
    "            display_lemma_stats(lemma_stats, column)\n",
    "        \n",
    "        # Analyser les mots pour chaque colonne\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSE DES MOTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for column in ['texte', 'discours']:\n",
    "            print(f\"\\nAnalyse des mots dans la colonne '{column}'...\")\n",
    "            word_results = analyze_words(df[column], column, stopwords_file_path)\n",
    "            display_top_results(word_results, f\"TOP 30 - MOTS - Colonne '{column}'\", 30)\n",
    "        \n",
    "        # Analyser les lemmes pour chaque colonne\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSE DES LEMMES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for column in ['texte', 'discours']:\n",
    "            print(f\"\\nAnalyse des lemmes dans la colonne '{column}'...\")\n",
    "            lemma_results = analyze_lemmas(df[column], column, nlp, stopwords_file_path)\n",
    "            display_top_results(lemma_results, f\"TOP 30 - LEMMES - Colonne '{column}'\", 30)\n",
    "        \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur: Le fichier '{file_path}' est introuvable.\")\n",
    "        print(\"Vérifiez que le fichier est dans le même répertoire que le script.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389eb777-f205-44ee-9ced-f6ccca5ac124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a7d64-5019-411f-a939-5dafed1665b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7061f-9164-448b-80db-547944e33ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8067f139-968f-474d-8510-3b77ba50bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE LEMMES-MOTS - DISCOURS POLITIQUES\n",
      "==================================================\n",
      "Chargement du modèle spaCy français...\n",
      "Modèle spaCy chargé avec succès\n",
      "Chargement des données...\n",
      "Nombre de discours chargés : 303\n",
      "Extraction des lemmes et mots associés...\n",
      "Stopwords chargés depuis /Users/charlielezin/Desktop/stopwords.txt: 707 mots\n",
      "  Traitement: 50/303 discours\n",
      "  Traitement: 100/303 discours\n",
      "  Traitement: 150/303 discours\n",
      "  Traitement: 200/303 discours\n",
      "  Traitement: 250/303 discours\n",
      "  Traitement: 300/303 discours\n",
      "Extraction terminée. 3306 lemmes collectés\n",
      "AFFICHAGE DE 50 LEMMES ALÉATOIRES AVEC LEURS MOTS\n",
      "============================================================\n",
      " 1. accidenté       -> accidentés\n",
      " 2. dresser         -> dressions, dressé, dressée\n",
      " 3. mœur            -> mœurs\n",
      " 4. fédéré          -> fédérés\n",
      " 5. eur             -> eure\n",
      " 6. remerciement    -> remerciements\n",
      " 7. régir           -> régie\n",
      " 8. identique       -> identiques\n",
      " 9. ranger          -> rangés\n",
      "10. gouverner       -> gouvernent, gouverné, gouvernée, gouvernés\n",
      "11. disloquer       -> disloquait\n",
      "12. mouvement       -> mouvements\n",
      "13. séparer         -> séparent, séparerait, séparé, séparées\n",
      "14. formel          -> formelle\n",
      "15. indemniser      -> indemnisés\n",
      "16. déverser        -> déversées\n",
      "17. alourdir        -> alourdie\n",
      "18. intégrant       -> intégrante\n",
      "19. contrat         -> contrats\n",
      "20. art             -> arts\n",
      "21. convalescent    -> convalescente\n",
      "22. gisèl           -> gisèle\n",
      "23. balayer         -> balayez\n",
      "24. patent          -> patente, patentes\n",
      "25. loreu           -> loreux\n",
      "26. questionnaire   -> questionnaires\n",
      "27. collectivité    -> collectivités\n",
      "28. salarier        -> salarié, salariée, salariés\n",
      "29. promettre       -> promet, promets, promettant, promettent, promettons, promettront, promis, promise, promises\n",
      "30. atomique        -> atomiques\n",
      "31. dilapider       -> dilapidés\n",
      "32. consommateur    -> consommateurs\n",
      "33. animer          -> animé, animée, animés\n",
      "34. unifier         -> unifié\n",
      "35. auteur          -> auteurs\n",
      "36. monsieur        -> madame, messieurs\n",
      "37. tragique        -> tragiques\n",
      "38. amitié          -> amitiés\n",
      "39. capituler       -> capitulant\n",
      "40. initial         -> initiale\n",
      "41. coopérative     -> coopératives\n",
      "42. chiraquienne    -> chiraquiennes\n",
      "43. adhérent        -> adhérents\n",
      "44. adaptation      -> adaptations\n",
      "45. mécanisme       -> mécanismes\n",
      "46. duper           -> dupes\n",
      "47. obstacle        -> obstacles\n",
      "48. électrifier     -> électrifiées\n",
      "49. heure           -> heures\n",
      "50. limitatif       -> limitative\n",
      "\n",
      "Total de lemmes disponibles : 3306\n",
      "Lemmes affichés : 50\n",
      "Analyse terminée\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script pour afficher 50 lemmes au hasard avec leurs mots associés\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "class LemmaWordMapper:\n",
    "    def __init__(self, filepath='/Users/charlielezin/Desktop/Candidatures58-81-290825.csv'):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "        self.nlp = None\n",
    "        self.lemma_to_words = defaultdict(set)\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Charge les données des discours\"\"\"\n",
    "        print(\"Chargement des données...\")\n",
    "        self.df = pd.read_csv(self.filepath, delimiter=';', encoding='utf-8')\n",
    "        \n",
    "        # Filtrer pour les tours 1 et 2 avec discours\n",
    "        self.df = self.df[self.df['tour'].isin([1, 2])].copy()\n",
    "        self.df = self.df[\n",
    "            (self.df['discours'].notna()) & \n",
    "            (self.df['discours'].str.strip() != '') &\n",
    "            (self.df['discours'].str.len() > 100)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Nombre de discours chargés : {len(self.df)}\")\n",
    "        return self.df\n",
    "    \n",
    "    def load_spacy_model(self):\n",
    "        \"\"\"Charge le modèle spaCy français\"\"\"\n",
    "        print(\"Chargement du modèle spaCy français...\")\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"fr_core_news_lg\")\n",
    "            print(\"Modèle spaCy chargé avec succès\")\n",
    "        except OSError:\n",
    "            print(\"Erreur: Le modèle spaCy français n'est pas installé.\")\n",
    "            print(\"Installez-le avec: python -m spacy download fr_core_news_lg\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Nettoie le texte\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def get_french_stopwords(self, stopwords_file_path='/Users/charlielezin/Desktop/stopwords.txt'):\n",
    "        \"\"\"Charge les mots vides français depuis un fichier\"\"\"\n",
    "        stopwords = set()\n",
    "        \n",
    "        try:\n",
    "            with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    word = line.strip().lower()\n",
    "                    if word:\n",
    "                        stopwords.add(word)\n",
    "            print(f\"Stopwords chargés depuis {stopwords_file_path}: {len(stopwords)} mots\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Attention: Le fichier {stopwords_file_path} n'a pas été trouvé.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier stopwords: {e}\")\n",
    "        \n",
    "        return stopwords\n",
    "    \n",
    "    def extract_lemma_word_mapping(self):\n",
    "        \"\"\"Extrait le mapping lemme -> mots pour tous les discours\"\"\"\n",
    "        print(\"Extraction des lemmes et mots associés...\")\n",
    "        \n",
    "        stopwords = self.get_french_stopwords()\n",
    "        \n",
    "        for i, row in self.df.iterrows():\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"  Traitement: {i+1}/{len(self.df)} discours\")\n",
    "            \n",
    "            text = self.preprocess_text(row['discours'])\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            # Traiter par chunks pour éviter les limites de spaCy\n",
    "            max_length = 500000\n",
    "            if len(text) > max_length:\n",
    "                chunks = [text[j:j+max_length] for j in range(0, len(text), max_length)]\n",
    "            else:\n",
    "                chunks = [text]\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                try:\n",
    "                    doc = self.nlp(chunk)\n",
    "                    \n",
    "                    for token in doc:\n",
    "                        if (not token.is_stop and \n",
    "                            not token.is_punct and \n",
    "                            not token.is_space and \n",
    "                            len(token.text) > 2 and\n",
    "                            len(token.lemma_) > 2 and\n",
    "                            token.text.lower() not in stopwords and\n",
    "                            token.lemma_.lower() not in stopwords and\n",
    "                            token.text.isalpha() and\n",
    "                            token.lemma_.isalpha()):\n",
    "                            \n",
    "                            lemma = token.lemma_.lower()\n",
    "                            word = token.text.lower()\n",
    "                            \n",
    "                            # Ajouter seulement si le mot est différent du lemme\n",
    "                            if word != lemma:\n",
    "                                self.lemma_to_words[lemma].add(word)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du traitement d'un chunk: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Extraction terminée. {len(self.lemma_to_words)} lemmes collectés\")\n",
    "    \n",
    "    def display_random_lemmas(self, n=50):\n",
    "        \"\"\"Affiche n lemmes au hasard avec leurs mots associés\"\"\"\n",
    "        print(f\"AFFICHAGE DE {n} LEMMES ALÉATOIRES AVEC LEURS MOTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Filtrer les lemmes qui ont au moins un mot associé différent\n",
    "        valid_lemmas = {lemma: words for lemma, words in self.lemma_to_words.items() if len(words) > 0}\n",
    "        \n",
    "        if len(valid_lemmas) == 0:\n",
    "            print(\"Aucun lemme avec des mots associés trouvé\")\n",
    "            return\n",
    "        \n",
    "        # Sélectionner aléatoirement n lemmes\n",
    "        selected_lemmas = random.sample(list(valid_lemmas.keys()), min(n, len(valid_lemmas)))\n",
    "        \n",
    "        for i, lemma in enumerate(selected_lemmas, 1):\n",
    "            associated_words = sorted(list(valid_lemmas[lemma]))\n",
    "            words_str = \", \".join(associated_words)\n",
    "            \n",
    "            print(f\"{i:2d}. {lemma:<15} -> {words_str}\")\n",
    "        \n",
    "        print(f\"\\nTotal de lemmes disponibles : {len(valid_lemmas)}\")\n",
    "        print(f\"Lemmes affichés : {len(selected_lemmas)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Analyse principale\"\"\"\n",
    "    print(\"ANALYSE LEMMES-MOTS - DISCOURS POLITIQUES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        mapper = LemmaWordMapper()\n",
    "        \n",
    "        # Chargement du modèle spaCy\n",
    "        if not mapper.load_spacy_model():\n",
    "            return\n",
    "        \n",
    "        # Chargement des données\n",
    "        mapper.load_data()\n",
    "        \n",
    "        if len(mapper.df) == 0:\n",
    "            print(\"Aucun discours trouvé!\")\n",
    "            return\n",
    "        \n",
    "        # Extraction du mapping lemmes-mots\n",
    "        mapper.extract_lemma_word_mapping()\n",
    "        \n",
    "        # Affichage aléatoire\n",
    "        mapper.display_random_lemmas(50)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2da667-fa0a-4ed5-89e1-b37a113dca91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
