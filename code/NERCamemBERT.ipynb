{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d524f7-599b-40aa-8444-9545c934969d",
   "metadata": {},
   "source": [
    "## Test du modèle NERCamemBERT de Loïck Bourdois\n",
    "Fonctionne sur trois types d'entités : personnes (PER), lieux (LOC), organisations (ORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dae0f8-ee5f-45c1-8e8c-cfeb0a354fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf3c34-dd16-48f6-979a-028fab2f982c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c7375f-18be-432f-85f2-506d0a198529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlielezin/miniconda3/envs/camembert-ner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/charlielezin/miniconda3/envs/camembert-ner/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/charlielezin/miniconda3/envs/camembert-ner/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/charlielezin/miniconda3/envs/camembert-ner/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <29934C5C-3B9F-3E3A-B719-1C007C303C01> /Users/charlielezin/miniconda3/envs/camembert-ner/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyse complète avec filtrage:\n",
      "results = process_csv_ner(csv_file, min_confidence=0.65)\n",
      "df_results = save_results_simple(results)\n",
      "Code avec filtrage par confiance et nettoyage prêt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlielezin/miniconda3/envs/camembert-ner/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/charlielezin/miniconda3/envs/camembert-ner/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "def clean_entity_text(text):\n",
    "    \"\"\"Nettoie le texte d'une entité en supprimant les caractères spéciaux\"\"\"\n",
    "    # Supprimer les caractères spéciaux\n",
    "    text = re.sub(r'^[^\\w\\s]+|[^\\w\\s]+$', '', text)\n",
    "    # Supprimer les caractères de ponctuation isolés\n",
    "    text = re.sub(r'\\s+[^\\w\\s]+\\s+', ' ', text)\n",
    "    # Normaliser les espaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def filter_entities(entities, min_confidence=0.65):\n",
    "    \"\"\"Filtre les entités par confiance et nettoie le texte\"\"\"\n",
    "    filtered_entities = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        # Vérifier la confiance\n",
    "        if entity['score'] >= min_confidence:\n",
    "            # Nettoyer le texte de l'entité\n",
    "            cleaned_text = clean_entity_text(entity['word'])\n",
    "            \n",
    "            # Ne garder que si le texte nettoyé n'est pas vide et fait plus d'1 caractère\n",
    "            if cleaned_text and len(cleaned_text) > 1:\n",
    "                entity_copy = entity.copy()\n",
    "                entity_copy['word'] = cleaned_text\n",
    "                filtered_entities.append(entity_copy)\n",
    "    \n",
    "    return filtered_entities\n",
    "\n",
    "def process_csv_ner(csv_file, min_confidence=0.65):\n",
    "    \"\"\"Traite tous les textes du CSV avec filtrage par confiance\"\"\"\n",
    "    \n",
    "    # Charger le pipeline NER\n",
    "    print(\"Chargement du modèle NER...\")\n",
    "    ner = pipeline(\n",
    "        'token-classification', \n",
    "        model='CATIE-AQ/Moderncamembert_3entities', \n",
    "        tokenizer='CATIE-AQ/Moderncamembert_3entities', \n",
    "        aggregation_strategy=\"simple\"\n",
    "    )\n",
    "    \n",
    "    # Charger le CSV\n",
    "    print(\"Chargement du CSV...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, sep=';', encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_file, sep=';', encoding='latin1')\n",
    "    \n",
    "    # Filtrer les textes valides qui ont un ID\n",
    "    df_valid = df[df['texte'].notna() & (df['texte'].str.len() > 10) & df['id'].notna()].copy()\n",
    "    \n",
    "    print(f\"Traitement de {len(df_valid)} textes...\")\n",
    "    print(f\"Confiance minimum: {min_confidence}\")\n",
    "    \n",
    "    # Résultats\n",
    "    all_results = []\n",
    "    \n",
    "    for i, (idx, row) in enumerate(df_valid.iterrows()):\n",
    "        text = row['texte']\n",
    "        text_id = row['id']  # Récupérer l'ID du CSV original\n",
    "        \n",
    "        try:\n",
    "            # Appliquer le NER\n",
    "            raw_results = ner(text)\n",
    "            \n",
    "            # Filtrer par confiance et nettoyer\n",
    "            filtered_results = filter_entities(raw_results, min_confidence)\n",
    "            \n",
    "            # Stocker les résultats avec text_id\n",
    "            all_results.append({\n",
    "                'text_index': idx,  # Index pandas (pour compatibilité)\n",
    "                'text_id': text_id,  # ID du CSV original\n",
    "                'text_length': len(text),\n",
    "                'entities': filtered_results,\n",
    "                'entities_raw_count': len(raw_results),\n",
    "                'entities_filtered_count': len(filtered_results)\n",
    "            })\n",
    "            \n",
    "            # Afficher progression tous les 50 textes\n",
    "            if (i + 1) % 50 == 0:\n",
    "                progress = ((i + 1) / len(df_valid)) * 100\n",
    "                print(f\"Progression: {i + 1}/{len(df_valid)} ({progress:.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur texte {text_id} (index {idx}): {e}\")\n",
    "            all_results.append({\n",
    "                'text_index': idx,\n",
    "                'text_id': text_id,\n",
    "                'text_length': len(text),\n",
    "                'entities': [],\n",
    "                'entities_raw_count': 0,\n",
    "                'entities_filtered_count': 0\n",
    "            })\n",
    "    \n",
    "    # Statistiques de filtrage\n",
    "    total_raw = sum(r['entities_raw_count'] for r in all_results)\n",
    "    total_filtered = sum(r['entities_filtered_count'] for r in all_results)\n",
    "    print(f\"Filtrage terminé: {total_raw} -> {total_filtered} entités (conf >= {min_confidence})\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def save_results_simple(results, output_file='ner_results.csv'):\n",
    "    \"\"\"Sauvegarde les résultats en CSV avec text_id\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for result in results:\n",
    "        text_id = result.get('text_id', result['text_index'])  # Utiliser text_id si disponible\n",
    "        text_len = result['text_length']\n",
    "        \n",
    "        if result['entities']:\n",
    "            for entity in result['entities']:\n",
    "                data.append({\n",
    "                    'text_id': text_id,\n",
    "                    'text_length': text_len,\n",
    "                    'entity': entity['word'],\n",
    "                    'label': entity['entity_group'],\n",
    "                    'confidence': entity['score'],\n",
    "                    'start': entity['start'],\n",
    "                    'end': entity['end']\n",
    "                })\n",
    "        else:\n",
    "            # Ligne vide si pas d'entités\n",
    "            data.append({\n",
    "                'text_id': text_id,\n",
    "                'text_length': text_len,\n",
    "                'entity': '',\n",
    "                'label': '',\n",
    "                'confidence': 0,\n",
    "                'start': 0,\n",
    "                'end': 0\n",
    "            })\n",
    "    \n",
    "    # Créer DataFrame et sauvegarder\n",
    "    df_results = pd.DataFrame(data)\n",
    "    df_results.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Résultats sauvegardés dans {output_file}\")\n",
    "    \n",
    "    # Statistiques\n",
    "    entities_only = df_results[df_results['entity'] != '']\n",
    "    total_entities = len(entities_only)\n",
    "    per_count = sum(entities_only['label'] == 'PER')\n",
    "    org_count = sum(entities_only['label'] == 'ORG')\n",
    "    loc_count = sum(entities_only['label'] == 'LOC')\n",
    "    \n",
    "    print(f\"Statistiques:\")\n",
    "    print(f\"- Total entités: {total_entities}\")\n",
    "    print(f\"- Personnes (PER): {per_count}\")\n",
    "    print(f\"- Organisations (ORG): {org_count}\")  \n",
    "    print(f\"- Lieux (LOC): {loc_count}\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Usage simple\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = 'Candidatures58-81-290825.csv'\n",
    "    \n",
    "    print(\"\\nAnalyse complète avec filtrage:\")\n",
    "    print(\"results = process_csv_ner(csv_file, min_confidence=0.65)\")\n",
    "    print(\"df_results = save_results_simple(results)\")\n",
    "\n",
    "print(\"Code avec filtrage par confiance et nettoyage prêt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40aa983-0f64-4ff3-a878-44ff5b156750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle NER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du CSV...\n",
      "Traitement de 303 textes...\n",
      "Confiance minimum: 0.65\n",
      "Progression: 50/303 (16.5%)\n",
      "Progression: 100/303 (33.0%)\n",
      "Progression: 150/303 (49.5%)\n",
      "Progression: 200/303 (66.0%)\n",
      "Progression: 250/303 (82.5%)\n",
      "Progression: 300/303 (99.0%)\n",
      "Filtrage terminé: 5646 -> 4051 entités (conf >= 0.65)\n"
     ]
    }
   ],
   "source": [
    "results = process_csv_ner(csv_file, min_confidence=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9f6576-e02a-4003-8f57-694e6baf9b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats sauvegardés dans ner_results.csv\n",
      "Statistiques:\n",
      "- Total entités: 4051\n",
      "- Personnes (PER): 1432\n",
      "- Organisations (ORG): 1062\n",
      "- Lieux (LOC): 1557\n"
     ]
    }
   ],
   "source": [
    "df_results = save_results_simple(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff354eb0-54ae-43ba-b860-1d3e5d9c1546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c36f11f-d7e9-420a-8ed6-08a51db91773",
   "metadata": {},
   "source": [
    "## Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2295cf-e442-4f4a-b4fd-c0b26acb5384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "916d5d57-e81a-45cd-9b21-d1c3675b8b96",
   "metadata": {},
   "source": [
    "## Métriques avant vérité terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d81a452-69df-4b5d-8206-da328733a997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE DES PRÉDICTIONS NER - AVANT VÉRITÉ TERRAIN\n",
      "==================================================\n",
      "\n",
      "Pour analyser les prédictions du modèle:\n",
      "stats = analyze_predictions_before_ground_truth('ner_results.csv')\n",
      "\n",
      "Pour voir des échantillons de prédictions:\n",
      "show_prediction_samples('ner_results.csv', n_samples=5)\n",
      "\n",
      "Pour analyser les scores de confiance:\n",
      "analyze_confidence_scores('ner_results.csv')\n",
      "Script d'analyse des prédictions NER (avant vérité terrain) prêt\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_predictions_before_ground_truth(csv_results_file):\n",
    "    \"\"\"Analyse les prédictions du modèle NER avant comparaison avec la vérité terrain\"\"\"\n",
    "    \n",
    "    print(\"Chargement du fichier de résultats...\")\n",
    "    \n",
    "    # Charger les résultats NER\n",
    "    try:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='latin1')\n",
    "    \n",
    "    print(\"Analyse des prédictions du modèle...\")\n",
    "    \n",
    "    # Statistiques générales\n",
    "    total_texts = df_results['text_id'].nunique()\n",
    "    total_predictions = len(df_results[df_results['entity'].notna() & (df_results['entity'] != '')])\n",
    "    \n",
    "    print(f\"\\nSTATISTIQUES GÉNÉRALES:\")\n",
    "    print(f\"  Nombre total de textes traités: {total_texts}\")\n",
    "    print(f\"  Nombre total d'entités prédites: {total_predictions}\")\n",
    "    print(f\"  Moyenne d'entités par texte: {total_predictions/total_texts:.2f}\")\n",
    "    \n",
    "    # Analyse par type d'entité\n",
    "    df_entities = df_results[df_results['entity'].notna() & (df_results['entity'] != '')]\n",
    "    \n",
    "    if not df_entities.empty:\n",
    "        entity_counts = df_entities['label'].value_counts()\n",
    "        \n",
    "        print(f\"\\nRÉPARTITION PAR TYPE D'ENTITÉ:\")\n",
    "        for entity_type, count in entity_counts.items():\n",
    "            percentage = (count / total_predictions) * 100\n",
    "            print(f\"  {entity_type}: {count} entités ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Analyse de la longueur des entités\n",
    "        entity_lengths = df_entities['entity'].str.len()\n",
    "        \n",
    "        print(f\"\\nANALYSE DE LA LONGUEUR DES ENTITÉS:\")\n",
    "        print(f\"  Longueur moyenne: {entity_lengths.mean():.1f} caractères\")\n",
    "        print(f\"  Longueur médiane: {entity_lengths.median():.1f} caractères\")\n",
    "        print(f\"  Longueur min: {entity_lengths.min()} caractères\")\n",
    "        print(f\"  Longueur max: {entity_lengths.max()} caractères\")\n",
    "        \n",
    "        # Top entités les plus fréquentes par type\n",
    "        print(f\"\\nTOP 5 DES ENTITÉS LES PLUS FRÉQUENTES PAR TYPE:\")\n",
    "        for entity_type in ['PER', 'ORG', 'LOC']:\n",
    "            type_entities = df_entities[df_entities['label'] == entity_type]['entity']\n",
    "            if not type_entities.empty:\n",
    "                top_entities = type_entities.value_counts().head(5)\n",
    "                print(f\"\\n  {entity_type}:\")\n",
    "                for entity, count in top_entities.items():\n",
    "                    print(f\"    '{entity}': {count} occurrences\")\n",
    "            else:\n",
    "                print(f\"\\n  {entity_type}: Aucune entité détectée\")\n",
    "        \n",
    "        # Analyse de la distribution par texte\n",
    "        entities_per_text = df_entities.groupby('text_id').size()\n",
    "        \n",
    "        print(f\"\\nDISTRIBUTION DES ENTITÉS PAR TEXTE:\")\n",
    "        print(f\"  Textes sans entité: {total_texts - len(entities_per_text)} ({((total_texts - len(entities_per_text))/total_texts)*100:.1f}%)\")\n",
    "        print(f\"  Textes avec entités: {len(entities_per_text)} ({(len(entities_per_text)/total_texts)*100:.1f}%)\")\n",
    "        \n",
    "        if len(entities_per_text) > 0:\n",
    "            print(f\"  Nombre moyen d'entités (textes non-vides): {entities_per_text.mean():.2f}\")\n",
    "            print(f\"  Nombre médian d'entités (textes non-vides): {entities_per_text.median():.2f}\")\n",
    "            print(f\"  Max entités dans un texte: {entities_per_text.max()}\")\n",
    "        \n",
    "        # Analyse par type d'entité et par texte\n",
    "        print(f\"\\nDISTRIBUTION PAR TYPE ET PAR TEXTE:\")\n",
    "        for entity_type in ['PER', 'ORG', 'LOC']:\n",
    "            type_data = df_entities[df_entities['label'] == entity_type]\n",
    "            if not type_data.empty:\n",
    "                entities_per_text_type = type_data.groupby('text_id').size()\n",
    "                texts_with_type = len(entities_per_text_type)\n",
    "                print(f\"  {entity_type}: {texts_with_type} textes contiennent ce type ({(texts_with_type/total_texts)*100:.1f}%)\")\n",
    "                print(f\"    Moyenne par texte (non-vides): {entities_per_text_type.mean():.2f}\")\n",
    "            else:\n",
    "                print(f\"  {entity_type}: 0 textes contiennent ce type\")\n",
    "    \n",
    "    else:\n",
    "        print(\"ATTENTION: Aucune entité trouvée dans les prédictions!\")\n",
    "    \n",
    "    return {\n",
    "        'total_texts': total_texts,\n",
    "        'total_predictions': total_predictions,\n",
    "        'entity_counts': entity_counts.to_dict() if not df_entities.empty else {},\n",
    "        'entities_per_text_stats': entities_per_text.describe().to_dict() if not df_entities.empty else {}\n",
    "    }\n",
    "\n",
    "def show_prediction_samples(csv_results_file, n_samples=3):\n",
    "    \"\"\"Affiche des échantillons de prédictions pour inspection manuelle\"\"\"\n",
    "    \n",
    "    print(\"Chargement du fichier de résultats...\")\n",
    "    \n",
    "    try:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='latin1')\n",
    "    \n",
    "    # Prendre quelques échantillons de textes avec entités\n",
    "    df_entities = df_results[df_results['entity'].notna() & (df_results['entity'] != '')]\n",
    "    \n",
    "    if df_entities.empty:\n",
    "        print(\"Aucune entité trouvée pour afficher des échantillons.\")\n",
    "        return\n",
    "    \n",
    "    sample_text_ids = df_entities['text_id'].unique()[:n_samples]\n",
    "    \n",
    "    print(f\"\\nÉCHANTILLONS DE PRÉDICTIONS ({len(sample_text_ids)} textes):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, text_id in enumerate(sample_text_ids, 1):\n",
    "        text_entities = df_entities[df_entities['text_id'] == text_id]\n",
    "        \n",
    "        print(f\"\\nÉchantillon {i} - Text ID: {text_id}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Récupérer le texte original si disponible\n",
    "        text_row = df_results[df_results['text_id'] == text_id].iloc[0]\n",
    "        if 'text' in text_row:\n",
    "            text = str(text_row['text'])\n",
    "            print(f\"Texte ({len(text)} chars): {text[:150]}{'...' if len(text) > 150 else ''}\")\n",
    "        \n",
    "        print(\"Entités détectées:\")\n",
    "        for _, entity_row in text_entities.iterrows():\n",
    "            entity_text = entity_row['entity']\n",
    "            entity_label = entity_row['label']\n",
    "            confidence = entity_row.get('confidence', 'N/A')\n",
    "            print(f\"  - {entity_label}: '{entity_text}' (conf: {confidence})\")\n",
    "\n",
    "def analyze_confidence_scores(csv_results_file):\n",
    "    \"\"\"Analyse les scores de confiance si disponibles\"\"\"\n",
    "    \n",
    "    print(\"Analyse des scores de confiance...\")\n",
    "    \n",
    "    try:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='latin1')\n",
    "    \n",
    "    if 'confidence' in df_results.columns:\n",
    "     analyze_confidence_scores('ner_results.csv')   df_entities = df_results[df_results['entity'].notna() & (df_results['entity'] != '')]\n",
    "        \n",
    "        if not df_entities.empty and df_entities['confidence'].notna().any():\n",
    "            confidence_scores = df_entities['confidence'].dropna()\n",
    "            \n",
    "            print(f\"\\nANALYSE DES SCORES DE CONFIANCE:\")\n",
    "            print(f\"  Nombre d'entités avec score: {len(confidence_scores)}\")\n",
    "            print(f\"  Score moyen: {confidence_scores.mean():.3f}\")\n",
    "            print(f\"  Score médian: {confidence_scores.median():.3f}\")\n",
    "            print(f\"  Score minimum: {confidence_scores.min():.3f}\")\n",
    "            print(f\"  Score maximum: {confidence_scores.max():.3f}\")\n",
    "            print(f\"  Écart-type: {confidence_scores.std():.3f}\")\n",
    "            \n",
    "            # Distribution par type d'entité\n",
    "            print(f\"\\nSCORES PAR TYPE D'ENTITÉ:\")\n",
    "            for entity_type in ['PER', 'ORG', 'LOC']:\n",
    "                type_scores = df_entities[df_entities['label'] == entity_type]['confidence'].dropna()\n",
    "                if not type_scores.empty:\n",
    "                    print(f\"  {entity_type}: moyenne={type_scores.mean():.3f}, médiane={type_scores.median():.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {entity_type}: Pas de scores disponibles\")\n",
    "            \n",
    "            # Entités avec faible confiance\n",
    "            low_confidence = df_entities[df_entities['confidence'] < 0.5]\n",
    "            if not low_confidence.empty:\n",
    "                print(f\"\\nENTITÉS AVEC FAIBLE CONFIANCE (<0.5): {len(low_confidence)}\")\n",
    "                print(\"Échantillons:\")\n",
    "                for _, row in low_confidence.head(5).iterrows():\n",
    "                    print(f\"  - {row['label']}: '{row['entity']}' (conf: {row['confidence']:.3f})\")\n",
    "        else:\n",
    "            print(\"Aucun score de confiance valide trouvé.\")\n",
    "    else:\n",
    "        print(\"Colonne 'confidence' non trouvée dans le fichier.\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ANALYSE DES PRÉDICTIONS NER - AVANT VÉRITÉ TERRAIN\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nPour analyser les prédictions du modèle:\")\n",
    "    print(\"stats = analyze_predictions_before_ground_truth('ner_results.csv')\")\n",
    "    \n",
    "    print(\"\\nPour voir des échantillons de prédictions:\")\n",
    "    print(\"show_prediction_samples('ner_results.csv', n_samples=5)\")\n",
    "    \n",
    "    print(\"\\nPour analyser les scores de confiance:\")\n",
    "    print(\"analyze_confidence_scores('ner_results.csv')\")\n",
    "\n",
    "print(\"Script d'analyse des prédictions NER (avant vérité terrain) prêt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d51a5bc-59bb-4455-b1d8-200a4894a3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier de résultats...\n",
      "Analyse des prédictions du modèle...\n",
      "\n",
      "STATISTIQUES GÉNÉRALES:\n",
      "  Nombre total de textes traités: 303\n",
      "  Nombre total d'entités prédites: 4051\n",
      "  Moyenne d'entités par texte: 13.37\n",
      "\n",
      "RÉPARTITION PAR TYPE D'ENTITÉ:\n",
      "  LOC: 1557 entités (38.4%)\n",
      "  PER: 1432 entités (35.3%)\n",
      "  ORG: 1062 entités (26.2%)\n",
      "\n",
      "ANALYSE DE LA LONGUEUR DES ENTITÉS:\n",
      "  Longueur moyenne: 14.2 caractères\n",
      "  Longueur médiane: 13.0 caractères\n",
      "  Longueur min: 2 caractères\n",
      "  Longueur max: 78 caractères\n",
      "\n",
      "TOP 5 DES ENTITÉS LES PLUS FRÉQUENTES PAR TYPE:\n",
      "\n",
      "  PER:\n",
      "    'Général de Gaulle': 52 occurrences\n",
      "    'Maurice PERCHE': 27 occurrences\n",
      "    'Guy Mollet': 25 occurrences\n",
      "    'Edmond THORAILLER': 20 occurrences\n",
      "    'Gérard YVON': 19 occurrences\n",
      "\n",
      "  ORG:\n",
      "    'Parti Communiste Français': 81 occurrences\n",
      "    'Parti Socialiste': 62 occurrences\n",
      "    'RÉPUBLIQUE FRANÇAISE': 47 occurrences\n",
      "    'Union de la Gauche': 33 occurrences\n",
      "    'Parti Communiste': 23 occurrences\n",
      "\n",
      "  LOC:\n",
      "    'Eure et Loir': 103 occurrences\n",
      "    'Blois': 82 occurrences\n",
      "    'Chartres': 81 occurrences\n",
      "    'Loir et Cher': 80 occurrences\n",
      "    'Vendôme': 72 occurrences\n",
      "\n",
      "DISTRIBUTION DES ENTITÉS PAR TEXTE:\n",
      "  Textes sans entité: 0 (0.0%)\n",
      "  Textes avec entités: 303 (100.0%)\n",
      "  Nombre moyen d'entités (textes non-vides): 13.37\n",
      "  Nombre médian d'entités (textes non-vides): 12.00\n",
      "  Max entités dans un texte: 54\n",
      "\n",
      "DISTRIBUTION PAR TYPE ET PAR TEXTE:\n",
      "  PER: 298 textes contiennent ce type (98.3%)\n",
      "    Moyenne par texte (non-vides): 4.81\n",
      "  ORG: 279 textes contiennent ce type (92.1%)\n",
      "    Moyenne par texte (non-vides): 3.81\n",
      "  LOC: 291 textes contiennent ce type (96.0%)\n",
      "    Moyenne par texte (non-vides): 5.35\n"
     ]
    }
   ],
   "source": [
    "stats = analyze_predictions_before_ground_truth('ner_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3945d-3ca9-4f47-94ac-8f03ed49b57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f654e7-7921-4e67-937f-d6e9f3d4367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier de résultats...\n",
      "\n",
      "ÉCHANTILLONS DE PRÉDICTIONS (5 textes):\n",
      "============================================================\n",
      "\n",
      "Échantillon 1 - Text ID: EL043_L_1967_03_028_03_1_PF_05\n",
      "------------------------------\n",
      "Entités détectées:\n",
      "  - PER: 'Paul Antier' (conf: 0.98235863)\n",
      "  - PER: 'Antoine de LAYRE' (conf: 0.86638623)\n",
      "  - PER: 'Antoine de LAYRE' (conf: 0.86842537)\n",
      "  - LOC: 'Luisant' (conf: 0.70305324)\n",
      "\n",
      "Échantillon 2 - Text ID: EL009_L_1958_11_028_03_2_PF_02\n",
      "------------------------------\n",
      "Entités détectées:\n",
      "  - PER: 'François A. de MONTFORT' (conf: 0.8047714)\n",
      "  - LOC: 'Ravensbrück' (conf: 0.9091263)\n",
      "  - PER: 'Charles de GAULLE' (conf: 0.95995605)\n",
      "  - LOC: 'Bonneval' (conf: 0.8568747)\n",
      "  - LOC: 'Illiers' (conf: 0.9163698)\n",
      "  - LOC: 'Châteaudun' (conf: 0.9182324)\n",
      "  - LOC: 'Brou' (conf: 0.875824)\n",
      "  - LOC: 'Cloyes' (conf: 0.9345669)\n",
      "  - LOC: 'Authon' (conf: 0.9116843)\n",
      "  - LOC: 'Nogent le Rotrou' (conf: 0.9633028)\n",
      "  - LOC: 'Thiron' (conf: 0.8277435)\n",
      "  - LOC: 'La Loupe' (conf: 0.9085297)\n",
      "  - LOC: 'Dunois' (conf: 0.8644455)\n",
      "  - LOC: 'Perche' (conf: 0.7694365)\n",
      "  - PER: 'Général DE GAULLE' (conf: 0.87784684)\n",
      "  - PER: 'Guy CHESNEAU' (conf: 0.800865)\n",
      "  - ORG: 'pour la Nouvelle République' (conf: 0.6844417)\n",
      "  - PER: 'A. de MONTF' (conf: 0.6914021)\n",
      "  - ORG: 'Union pour la Nouvelle République' (conf: 0.82136905)\n",
      "  - PER: 'Guy CHESNEAU' (conf: 0.87494916)\n",
      "\n",
      "Échantillon 3 - Text ID: EL009_L_1958_11_028_03_1_PF_06\n",
      "------------------------------\n",
      "Entités détectées:\n",
      "  - ORG: '23' (conf: 0.87338454)\n",
      "  - ORG: 'UNION POUR LA NOUVELLE RÉPUBLIQUE' (conf: 0.8053085)\n",
      "  - LOC: 'Eure et Loir' (conf: 0.905842)\n",
      "  - PER: 'Charles de GAULLE' (conf: 0.9819484)\n",
      "  - LOC: 'BEAUCE' (conf: 0.7584958)\n",
      "  - LOC: 'PERCHE' (conf: 0.86063004)\n",
      "  - ORG: 'Fédération Nationale des Syndicats et Exploitants Agricoles' (conf: 0.6712533)\n",
      "  - LOC: 'France' (conf: 0.69904757)\n",
      "  - ORG: 'IVème République' (conf: 0.7022634)\n",
      "  - ORG: 'Union Nationale des Associations Familiales' (conf: 0.70458734)\n",
      "  - PER: 'Général de GAULLE' (conf: 0.9337534)\n",
      "  - PER: 'François A. de MONTFORT' (conf: 0.9341656)\n",
      "  - ORG: 'Union pour la Nouvelle République' (conf: 0.85478956)\n",
      "  - PER: 'Guy CHESNEAU' (conf: 0.87917316)\n",
      "\n",
      "Échantillon 4 - Text ID: EL027_L_1962_11_028_03_1_PF_03\n",
      "------------------------------\n",
      "Entités détectées:\n",
      "  - ORG: '1962' (conf: 0.666643)\n",
      "  - LOC: 'Circonscription d' Eure et Loir' (conf: 0.72196025)\n",
      "  - PER: 'François de MONTFORT' (conf: 0.97474605)\n",
      "  - LOC: 'Châteaudun' (conf: 0.84351003)\n",
      "  - LOC: 'DUNOIS' (conf: 0.76797915)\n",
      "  - LOC: 'BEAUCE' (conf: 0.85323596)\n",
      "  - LOC: 'PERCHE' (conf: 0.824879)\n",
      "  - LOC: 'Eure et Loir' (conf: 0.7846885)\n",
      "  - LOC: 'Eure et Loir' (conf: 0.74150765)\n",
      "  - ORG: 'IVe République' (conf: 0.80511373)\n",
      "  - PER: 'Fidel Castro' (conf: 0.9507194)\n",
      "  - PER: 'Kroutchev' (conf: 0.8407426)\n",
      "  - LOC: 'Alger' (conf: 0.7584832)\n",
      "  - LOC: 'Oran' (conf: 0.68478864)\n",
      "  - PER: 'François de MONTFORT' (conf: 0.88605994)\n",
      "  - LOC: 'Nogent le Rotrou' (conf: 0.99062437)\n",
      "\n",
      "Échantillon 5 - Text ID: EL066_L_1973_03_041_02_1_PF_06\n",
      "------------------------------\n",
      "Entités détectées:\n",
      "  - LOC: 'Loir et Cher' (conf: 0.9170443)\n",
      "  - LOC: 'Circonscription de ROMORANTIN' (conf: 0.7300164)\n",
      "  - PER: 'Jean AUGER' (conf: 0.9269204)\n",
      "  - ORG: 'Parti Communiste Français' (conf: 0.98062867)\n",
      "  - ORG: 'Union Populaire et la Victoire du Programme Commun' (conf: 0.779539)\n",
      "  - PER: 'Bernard PAUMIER' (conf: 0.97068393)\n",
      "  - LOC: 'Chémery' (conf: 0.89592594)\n",
      "  - ORG: 'Communi' (conf: 0.6864098)\n",
      "  - ORG: 'PCF' (conf: 0.918379)\n",
      "  - PER: 'Jean AUGER' (conf: 0.9303891)\n",
      "  - ORG: 'Parti Communiste Français' (conf: 0.66405004)\n",
      "  - ORG: 'Union Populaire et la Victoire du Programme Commun' (conf: 0.89302593)\n",
      "  - PER: 'Bernard PAUMIER' (conf: 0.82721233)\n"
     ]
    }
   ],
   "source": [
    "show_prediction_samples('ner_results.csv', n_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbafe42b-5d84-478b-b218-c9eef2553692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse des scores de confiance...\n",
      "\n",
      "ANALYSE DES SCORES DE CONFIANCE:\n",
      "  Nombre d'entités avec score: 4051\n",
      "  Score moyen: 0.856\n",
      "  Score médian: 0.873\n",
      "  Score minimum: 0.650\n",
      "  Score maximum: 1.000\n",
      "  Écart-type: 0.101\n",
      "\n",
      "SCORES PAR TYPE D'ENTITÉ:\n",
      "  PER: moyenne=0.870, médiane=0.892\n",
      "  ORG: moyenne=0.835, médiane=0.838\n",
      "  LOC: moyenne=0.856, médiane=0.874\n"
     ]
    }
   ],
   "source": [
    "analyze_confidence_scores('ner_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a8473-aefd-43e7-b838-0724a3d8030b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87211f9a-c77b-494d-8de6-2814a85dc748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17821e-e22b-48b2-8cc4-e95818fef174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e2fbde-1c27-4e65-8a45-29bef6bd1fcb",
   "metadata": {},
   "source": [
    "## Métriques après vérification avec vérité terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487f543-57d4-4405-9564-9705cb06e043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65a6a57-130d-49ed-b0cd-1c4a63747c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÉVALUATION DES MÉTRIQUES NER\n",
      "==============================\n",
      "\n",
      "Pour évaluer un fichier de résultats:\n",
      "metrics = evaluate_metrics_simple('ner_results.csv', 'Candidatures58-81-290825.csv')\n",
      "\n",
      "Pour debugger la correspondance:\n",
      "debug_matching('ner_results.csv', 'Candidatures58-81-290825.csv')\n",
      "Script d'évaluation des métriques prêt\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def evaluate_metrics_simple(csv_results_file, csv_original_file):\n",
    "    \"\"\"Évalue les métriques de performance NER\"\"\"\n",
    "    \n",
    "    print(\"Chargement des fichiers...\")\n",
    "    \n",
    "    # Charger les résultats NER\n",
    "    try:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='latin1')\n",
    "    \n",
    "    # Charger le CSV original\n",
    "    try:\n",
    "        df_original = pd.read_csv(csv_original_file, sep=';', encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df_original = pd.read_csv(csv_original_file, sep=';', encoding='latin1')\n",
    "    \n",
    "    # Créer la vérité terrain\n",
    "    print(\"Création de la vérité terrain...\")\n",
    "    ground_truth = create_ground_truth_simple(df_original)\n",
    "    \n",
    "    # Extraire les prédictions\n",
    "    predictions = extract_predictions_simple(df_results)\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    print(\"Calcul des métriques...\")\n",
    "    metrics = calculate_metrics_simple(predictions, ground_truth)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    display_metrics(metrics)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def create_ground_truth_simple(df_original):\n",
    "    \"\"\"Crée la vérité terrain à partir du CSV original\"\"\"\n",
    "    \n",
    "    ground_truth = {}\n",
    "    \n",
    "    for _, row in df_original.iterrows():\n",
    "        if pd.notna(row.get('id')) and pd.notna(row.get('texte')):\n",
    "            # Utiliser l'ID du CSV original comme clé\n",
    "            text_id = str(row['id'])  # Convertir en string pour cohérence\n",
    "            text = str(row['texte']).lower()\n",
    "            \n",
    "            entities = {'PER': set(), 'ORG': set(), 'LOC': set()}\n",
    "            \n",
    "            # Personnes\n",
    "            if pd.notna(row.get('nom_titulaire')):\n",
    "                nom = str(row['nom_titulaire']).strip().lower()\n",
    "                if nom and nom in text:\n",
    "                    entities['PER'].add(nom)\n",
    "                    \n",
    "            if pd.notna(row.get('prenom_titulaire')) and pd.notna(row.get('nom_titulaire')):\n",
    "                prenom_nom = f\"{row['prenom_titulaire']} {row['nom_titulaire']}\".strip().lower()\n",
    "                if prenom_nom and prenom_nom in text:\n",
    "                    entities['PER'].add(prenom_nom)\n",
    "            \n",
    "            if pd.notna(row.get('nom_suppleant')):\n",
    "                nom = str(row['nom_suppleant']).strip().lower()\n",
    "                if nom and nom in text:\n",
    "                    entities['PER'].add(nom)\n",
    "                    \n",
    "            if pd.notna(row.get('prenom_suppleant')) and pd.notna(row.get('nom_suppleant')):\n",
    "                prenom_nom = f\"{row['prenom_suppleant']} {row['nom_suppleant']}\".strip().lower()\n",
    "                if prenom_nom and prenom_nom in text:\n",
    "                    entities['PER'].add(prenom_nom)\n",
    "            \n",
    "            # Organisations\n",
    "            parti_columns = ['parti_titulaire_1', 'parti_titulaire_2', 'parti_titulaire_3', 'parti_titulaire_4', \n",
    "                           'parti_suppleant_1', 'parti_suppleant_2', 'parti_suppleant_3', 'parti_suppleant_4', 'parti_suppleant_5']\n",
    "            \n",
    "            for col in parti_columns:\n",
    "                if pd.notna(row.get(col)):\n",
    "                    parti = str(row[col]).strip().lower()\n",
    "                    if parti and parti in text:\n",
    "                        entities['ORG'].add(parti)\n",
    "            \n",
    "            # Lieux\n",
    "            if pd.notna(row.get('departement_nom')):\n",
    "                dept = str(row['departement_nom']).strip().lower()\n",
    "                if dept and dept in text:\n",
    "                    entities['LOC'].add(dept)\n",
    "            \n",
    "            lieu_columns = ['lieu_naissance_titulaire', 'lieu_residence_titulaire', \n",
    "                           'lieu_naissance_suppleant', 'lieu_residence_suppleant']\n",
    "            \n",
    "            for col in lieu_columns:\n",
    "                if pd.notna(row.get(col)):\n",
    "                    lieu = str(row[col]).strip().lower()\n",
    "                    if lieu and lieu in text:\n",
    "                        entities['LOC'].add(lieu)\n",
    "            \n",
    "            ground_truth[text_id] = entities\n",
    "    \n",
    "    print(f\"Vérité terrain créée pour {len(ground_truth)} textes\")\n",
    "    return ground_truth\n",
    "\n",
    "def extract_predictions_simple(df_results):\n",
    "    \"\"\"Extrait les prédictions du fichier de résultats\"\"\"\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    # Filtrer les entités non vides\n",
    "    df_entities = df_results[df_results['entity'].notna() & (df_results['entity'] != '')]\n",
    "    \n",
    "    # Récupérer tous les text_id uniques (même ceux sans entités)\n",
    "    all_text_ids = df_results['text_id'].unique()\n",
    "    \n",
    "    for text_id in all_text_ids:\n",
    "        # Convertir en string pour cohérence avec ground_truth\n",
    "        text_id_str = str(text_id)\n",
    "        \n",
    "        # Filtrer les entités pour ce texte\n",
    "        text_entities = df_entities[df_entities['text_id'] == text_id]\n",
    "        \n",
    "        entities = {'PER': set(), 'ORG': set(), 'LOC': set()}\n",
    "        \n",
    "        for _, row in text_entities.iterrows():\n",
    "            entity_text = str(row['entity']).strip().lower()\n",
    "            entity_label = row['label']\n",
    "            \n",
    "            if entity_label in entities and entity_text:\n",
    "                entities[entity_label].add(entity_text)\n",
    "        \n",
    "        predictions[text_id_str] = entities\n",
    "    \n",
    "    print(f\"Prédictions extraites pour {len(predictions)} textes\")\n",
    "    return predictions\n",
    "\n",
    "def calculate_metrics_simple(predictions, ground_truth):\n",
    "    \"\"\"Calcule les métriques de performance\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Debug: vérifier la correspondance des IDs\n",
    "    pred_ids = set(predictions.keys())\n",
    "    truth_ids = set(ground_truth.keys())\n",
    "    common_ids = pred_ids & truth_ids\n",
    "    \n",
    "    print(f\"IDs dans prédictions: {len(pred_ids)}\")\n",
    "    print(f\"IDs dans vérité terrain: {len(truth_ids)}\")\n",
    "    print(f\"IDs communs: {len(common_ids)}\")\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        print(\"ATTENTION: Aucun ID commun trouvé!\")\n",
    "        print(f\"Échantillon prédictions: {list(pred_ids)[:5]}\")\n",
    "        print(f\"Échantillon vérité terrain: {list(truth_ids)[:5]}\")\n",
    "    \n",
    "    for entity_type in ['PER', 'ORG', 'LOC']:\n",
    "        tp = 0  # True Positives\n",
    "        fp = 0  # False Positives\n",
    "        fn = 0  # False Negatives\n",
    "        \n",
    "        # Utiliser tous les IDs disponibles\n",
    "        all_text_ids = pred_ids | truth_ids\n",
    "        \n",
    "        for text_id in all_text_ids:\n",
    "            pred_entities = predictions.get(text_id, {}).get(entity_type, set())\n",
    "            true_entities = ground_truth.get(text_id, {}).get(entity_type, set())\n",
    "            \n",
    "            # True Positives: entités prédites ET dans la vérité terrain\n",
    "            tp += len(pred_entities & true_entities)\n",
    "            \n",
    "            # False Positives: entités prédites mais PAS dans la vérité terrain\n",
    "            fp += len(pred_entities - true_entities)\n",
    "            \n",
    "            # False Negatives: entités dans la vérité terrain mais PAS prédites\n",
    "            fn += len(true_entities - pred_entities)\n",
    "        \n",
    "        # Calcul des métriques\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics[entity_type] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def display_metrics(metrics):\n",
    "    \"\"\"Affiche les métriques\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RÉSULTATS D'ÉVALUATION NER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for entity_type, scores in metrics.items():\n",
    "        print(f\"\\n{entity_type} (Entités de type {entity_type}):\")\n",
    "        print(f\"  Précision:  {scores['precision']:.3f} ({scores['tp']}/{scores['tp'] + scores['fp']})\")\n",
    "        print(f\"  Rappel:     {scores['recall']:.3f} ({scores['tp']}/{scores['tp'] + scores['fn']})\")\n",
    "        print(f\"  F1-Score:   {scores['f1_score']:.3f}\")\n",
    "        print(f\"  Détails:    TP={scores['tp']}, FP={scores['fp']}, FN={scores['fn']}\")\n",
    "    \n",
    "    # Score moyen\n",
    "    avg_precision = sum(s['precision'] for s in metrics.values()) / len(metrics)\n",
    "    avg_recall = sum(s['recall'] for s in metrics.values()) / len(metrics)\n",
    "    avg_f1 = sum(s['f1_score'] for s in metrics.values()) / len(metrics)\n",
    "    \n",
    "    print(f\"\\nMOYENNES:\")\n",
    "    print(f\"  Précision moyenne: {avg_precision:.3f}\")\n",
    "    print(f\"  Rappel moyen:      {avg_recall:.3f}\")\n",
    "    print(f\"  F1-Score moyen:    {avg_f1:.3f}\")\n",
    "\n",
    "def debug_matching(csv_results_file, csv_original_file, text_id_sample=None):\n",
    "    \"\"\"Fonction de debug pour vérifier la correspondance des entités\"\"\"\n",
    "    \n",
    "    # Charger les fichiers\n",
    "    try:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df_results = pd.read_csv(csv_results_file, encoding='latin1')\n",
    "    \n",
    "    try:\n",
    "        df_original = pd.read_csv(csv_original_file, sep=';', encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df_original = pd.read_csv(csv_original_file, sep=';', encoding='latin1')\n",
    "    \n",
    "    ground_truth = create_ground_truth_simple(df_original)\n",
    "    predictions = extract_predictions_simple(df_results)\n",
    "    \n",
    "    # Si aucun text_id spécifié, prendre un ID commun au hasard\n",
    "    if text_id_sample is None:\n",
    "        common_ids = set(predictions.keys()) & set(ground_truth.keys())\n",
    "        if common_ids:\n",
    "            import random\n",
    "            text_id_sample = random.choice(list(common_ids))\n",
    "        else:\n",
    "            print(\"Aucun ID commun trouvé pour le debug!\")\n",
    "            return\n",
    "    \n",
    "    text_id_str = str(text_id_sample)\n",
    "    \n",
    "    print(f\"\\nDEBUG pour text_id: {text_id_str}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if text_id_str in ground_truth:\n",
    "        print(\"VÉRITÉ TERRAIN:\")\n",
    "        for entity_type, entities in ground_truth[text_id_str].items():\n",
    "            print(f\"  {entity_type}: {entities}\")\n",
    "    else:\n",
    "        print(\"VÉRITÉ TERRAIN: Non trouvée\")\n",
    "    \n",
    "    if text_id_str in predictions:\n",
    "        print(\"\\nPRÉDICTIONS:\")\n",
    "        for entity_type, entities in predictions[text_id_str].items():\n",
    "            print(f\"  {entity_type}: {entities}\")\n",
    "    else:\n",
    "        print(\"\\nPRÉDICTIONS: Non trouvées\")\n",
    "    \n",
    "    # Afficher le texte original si possible\n",
    "    original_row = df_original[df_original['id'].astype(str) == text_id_str]\n",
    "    if not original_row.empty:\n",
    "        text = str(original_row.iloc[0]['texte'])\n",
    "        print(f\"\\nTEXTE ORIGINAL ({len(text)} chars):\")\n",
    "        print(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ÉVALUATION DES MÉTRIQUES NER\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    print(\"\\nPour évaluer un fichier de résultats:\")\n",
    "    print(\"metrics = evaluate_metrics_simple('ner_results.csv', 'Candidatures58-81-290825.csv')\")\n",
    "    \n",
    "    print(\"\\nPour debugger la correspondance:\")\n",
    "    print(\"debug_matching('ner_results.csv', 'Candidatures58-81-290825.csv')\")\n",
    "\n",
    "print(\"Script d'évaluation des métriques prêt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ef4011-7112-4fd4-a2f9-0a86778238e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des fichiers...\n",
      "Création de la vérité terrain...\n",
      "Vérité terrain créée pour 303 textes\n",
      "Prédictions extraites pour 303 textes\n",
      "Calcul des métriques...\n",
      "IDs dans prédictions: 303\n",
      "IDs dans vérité terrain: 303\n",
      "IDs communs: 303\n",
      "\n",
      "============================================================\n",
      "RÉSULTATS D'ÉVALUATION NER\n",
      "============================================================\n",
      "\n",
      "PER (Entités de type PER):\n",
      "  Précision:  0.429 (456/1063)\n",
      "  Rappel:     0.410 (456/1112)\n",
      "  F1-Score:   0.419\n",
      "  Détails:    TP=456, FP=607, FN=656\n",
      "\n",
      "ORG (Entités de type ORG):\n",
      "  Précision:  0.154 (137/887)\n",
      "  Rappel:     0.537 (137/255)\n",
      "  F1-Score:   0.240\n",
      "  Détails:    TP=137, FP=750, FN=118\n",
      "\n",
      "LOC (Entités de type LOC):\n",
      "  Précision:  0.151 (174/1155)\n",
      "  Rappel:     0.274 (174/634)\n",
      "  F1-Score:   0.195\n",
      "  Détails:    TP=174, FP=981, FN=460\n",
      "\n",
      "MOYENNES:\n",
      "  Précision moyenne: 0.245\n",
      "  Rappel moyen:      0.407\n",
      "  F1-Score moyen:    0.285\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_metrics_simple('ner_results.csv', 'Candidatures58-81-290825.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3916303-65fc-4566-ba8a-23cba7e94e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérité terrain créée pour 303 textes\n",
      "Prédictions extraites pour 303 textes\n",
      "\n",
      "DEBUG pour text_id: EL103_L_1978_03_041_03_1_PF_03\n",
      "==================================================\n",
      "VÉRITÉ TERRAIN:\n",
      "  PER: {'girard', 'bernard hemme', 'hemme', 'pierre girard'}\n",
      "  ORG: set()\n",
      "  LOC: {'loir-et-cher', 'saint-arnoult', 'vendôme'}\n",
      "\n",
      "PRÉDICTIONS:\n",
      "  PER: {'bernard hemme', 'pierre girard'}\n",
      "  ORG: {'république française département de loir et cher', 'parti communiste'}\n",
      "  LOC: {'la ville aux clercs', 'saint arnoult', 'vendômois', 'vendôme', 'france'}\n",
      "\n",
      "TEXTE ORIGINAL (5818 chars):\n",
      "\n",
      "\n",
      "\n",
      "RÉPUBLIQUE FRANÇAISE - DÉPARTEMENT DE LOIR-ET-CHER\n",
      "\n",
      "CIRCONSCRIPTION DE VENDÔME\n",
      "\n",
      "\n",
      "\n",
      "ÉLECTIONS LÉGISLATIVES DU 12 MARS 1978\n",
      "\n",
      "\n",
      "\n",
      "MADAME, MADEMOISELLE, MONSIEUR,\n",
      "\n",
      "Le 12 mars et le 19 éventuellement, vous...\n"
     ]
    }
   ],
   "source": [
    "debug_matching('ner_results.csv', 'Candidatures58-81-290825.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc0f8f-1541-4b0f-a12d-1074911a1423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76badffb-c135-41ae-aba0-0a30494f0976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CamemBERT NER",
   "language": "python",
   "name": "camembert-ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
