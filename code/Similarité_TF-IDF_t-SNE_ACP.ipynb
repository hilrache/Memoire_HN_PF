{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53073524-cb22-4872-8e1b-a71ac1b85a1d",
   "metadata": {},
   "source": [
    "# Script de comparaison des textes de PF par similarité cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa2bc6-8d99-44c8-8bad-67c6d33f04c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41dd03a-684d-472d-a918-fbea09d50c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049fb48-efa0-41bd-8479-4c26b957efb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7af75859-fdc0-4fb7-94bb-ca793f2168eb",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132c17c-7653-4899-9df2-cb64acd318cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede05606-c0c3-4748-8b55-a8e76bea9812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE TF-IDF TOURS 1+2 COMBINÉS - TEXTES ANONYMISÉS\n",
      "============================================================\n",
      "Chargement des données (Tours 1+2)...\n",
      "Nombre total de lignes : 303\n",
      "Filtrage tours 1+2 : 303 candidatures\n",
      "Nombre de candidatures avec discours : 303\n",
      "Répartition par tour :\n",
      "tour\n",
      "1    217\n",
      "2     86\n",
      "Name: count, dtype: int64\n",
      "Répartition par nuance politique :\n",
      "nuance_titulaire\n",
      "GAUL     62\n",
      "SOC      58\n",
      "COM      54\n",
      "CEN      36\n",
      "RAD      21\n",
      "EXG      15\n",
      "CEN-D    15\n",
      "PSU      11\n",
      "AGR      11\n",
      "EXD      10\n",
      "Name: count, dtype: int64\n",
      "Anonymisation des textes...\n",
      "Anonymisation:\n",
      "   Noms/prénoms à supprimer: 370\n",
      "   Termes de partis à supprimer: 135\n",
      "Nombre de discours après anonymisation : 303\n",
      "Calcul de la matrice TF-IDF sur textes anonymisés...\n",
      "Matrice TF-IDF : (303, 1000)\n",
      "Calcul de la similarité cosinus...\n",
      "Statistiques de similarité (textes anonymisés) :\n",
      "  - Moyenne : 0.165\n",
      "  - Médiane : 0.156\n",
      "  - Max : 1.000\n",
      "  - Min : 0.003\n",
      "Calcul du t-SNE...\n",
      "TOP 50 ASSOCIATIONS SIMILAIRES (TEXTES ANONYMISÉS)\n",
      "============================================================\n",
      "Analyse avec seuil >= 0.6\n",
      "Trouvé: 7 groupes, 63 paires similaires\n",
      "GROUPES DE DISCOURS SIMILAIRES (>=3 documents):\n",
      "--------------------------------------------------\n",
      "\n",
      "1. GROUPE DE 6 DOCUMENTS SIMILAIRES:\n",
      "   Similarité moyenne interne: 0.992\n",
      "   - Lucien Lanchon (EXG, 1978, T1)\n",
      "   - Claude Bedu (EXG, 1978, T1)\n",
      "   - Ludovic Szotowski (EXG, 1978, T1)\n",
      "   - Nicole Giraud (EXG, 1978, T1)\n",
      "   - José Duménil (EXG, 1978, T1)\n",
      "   - Nicole Lheron (EXG, 1978, T1)\n",
      "\n",
      "2. GROUPE DE 3 DOCUMENTS SIMILAIRES:\n",
      "   Similarité moyenne interne: 0.893\n",
      "   - Marcel Chartrain (COM, 1958, T2)\n",
      "   - Charles Hubert (COM, 1958, T2)\n",
      "   - Maurice Perche (COM, 1958, T2)\n",
      "\n",
      "3. GROUPE DE 3 DOCUMENTS SIMILAIRES:\n",
      "   Similarité moyenne interne: 0.748\n",
      "   - Marcel Chartrain (COM, 1958, T1)\n",
      "   - Maurice Perche (COM, 1958, T1)\n",
      "   - Charles Hubert (COM, 1958, T1)\n",
      "\n",
      "4. GROUPE DE 5 DOCUMENTS SIMILAIRES:\n",
      "   Similarité moyenne interne: 0.734\n",
      "   - Bernard Paumier (COM, 1962, T1)\n",
      "   - Marcel Chartrain (COM, 1962, T1)\n",
      "   - Roger Leclerc (COM, 1962, T1)\n",
      "   - Maurice Perche (COM, 1962, T1)\n",
      "   - Georges Juillot (COM, 1962, T1)\n",
      "\n",
      "5. GROUPE DE 3 DOCUMENTS SIMILAIRES:\n",
      "   Similarité moyenne interne: 0.805\n",
      "   - Maurice Perche (COM, 1967, T1)\n",
      "   - Maurice Perche (COM, 1968, T1)\n",
      "   - André Essirard (COM, 1968, T1)\n",
      "\n",
      "6. GROUPE DE 3 DOCUMENTS SIMILAIRES:\n",
      "   Similarité moyenne interne: 0.664\n",
      "   - Georges Juillot (COM, 1973, T1)\n",
      "   - Maurice Perche (COM, 1973, T1)\n",
      "   - André Essirard (COM, 1973, T1)\n",
      "\n",
      "7. GROUPE DE 3 DOCUMENTS SIMILAIRES:\n",
      "   Similarité moyenne interne: 1.000\n",
      "   - Francine Mas (EXG, 1978, T1)\n",
      "   - Pascal Rivais (EXG, 1978, T1)\n",
      "   - Lionel Martin (EXG, 1978, T1)\n",
      "\n",
      "PAIRES DE DOCUMENTS TRÈS SIMILAIRES:\n",
      "--------------------------------------------------\n",
      "\n",
      "8. Similarité: 1.000\n",
      "   - Lucien Lanchon (EXG)\n",
      "   - Annie Sornin (EXG)\n",
      "\n",
      "9. Similarité: 1.000\n",
      "   - Lionel Martin (EXG)\n",
      "   - Denis Marx (EXG)\n",
      "\n",
      "10. Similarité: 0.991\n",
      "   - André Mallet (EXD)\n",
      "   - Victor Vaury (EXD)\n",
      "\n",
      "11. Similarité: 0.968\n",
      "   - Georges Domengie (GAUL)\n",
      "   - Roger Goemaere (GAUL)\n",
      "\n",
      "12. Similarité: 0.952\n",
      "   - Marius Bouillon (EXD)\n",
      "   - Raymond Mignan (EXD)\n",
      "\n",
      "13. Similarité: 0.895\n",
      "   - Edmond Thorailler (GAUL)\n",
      "   - Edmond Thorailler (GAUL)\n",
      "\n",
      "14. Similarité: 0.875\n",
      "   - Robert Huwart (RDG)\n",
      "   - Robert Huwart (RDG)\n",
      "\n",
      "15. Similarité: 0.867\n",
      "   - Claude Gerbet (CEN-D)\n",
      "   - Claude Gerbet (CEN-D)\n",
      "\n",
      "16. Similarité: 0.835\n",
      "   - Daniel Cogneau (PSU)\n",
      "   - Jacques Dujardin (PSU)\n",
      "\n",
      "17. Similarité: 0.813\n",
      "   - Yves Cauchon (CEN-D)\n",
      "   - Yves Cauchon (CEN-D)\n",
      "\n",
      "18. Similarité: 0.796\n",
      "   - Jacques Blot (GAUL)\n",
      "   - Jacques Blot (GAUL)\n",
      "\n",
      "19. Similarité: 0.701\n",
      "   - Jeanny Lorgeoux (SOC)\n",
      "   - Jeanny Lorgeoux (SOC)\n",
      "\n",
      "20. Similarité: 0.698\n",
      "   - Lucien Gigaud (SOC)\n",
      "   - Robert Girond (SOC)\n",
      "\n",
      "21. Similarité: 0.694\n",
      "   - Dominique Foucault (EXD)\n",
      "   - Jean-Pierre Stirbois (EXD)\n",
      "\n",
      "22. Similarité: 0.677\n",
      "   - Georges Lemoine (SOC)\n",
      "   - Georges Lemoine (SOC)\n",
      "\n",
      "23. Similarité: 0.677\n",
      "   - Fernand Jupeau (COM)\n",
      "   - Robert Levasseur (COM)\n",
      "\n",
      "24. Similarité: 0.664\n",
      "   - Claude Breton (COM)\n",
      "   - Roger Leclerc (COM)\n",
      "\n",
      "25. Similarité: 0.646\n",
      "   - Emile Vivier (SOC)\n",
      "   - Emile Vivier (SOC)\n",
      "\n",
      "26. Similarité: 0.644\n",
      "   - Georges Juillot (COM)\n",
      "   - Georges Juillot (COM)\n",
      "\n",
      "27. Similarité: 0.642\n",
      "   - Maurice Dousset (GAUL)\n",
      "   - Maurice Dousset (GAUL)\n",
      "\n",
      "28. Similarité: 0.634\n",
      "   - Edmond Thorailler (GAUL)\n",
      "   - Edmond Thorailler (GAUL)\n",
      "\n",
      "29. Similarité: 0.631\n",
      "   - Robert Huwart (SOC)\n",
      "   - Emile Vivier (SOC)\n",
      "\n",
      "30. Similarité: 0.625\n",
      "   - Edmond Thorailler (GAUL)\n",
      "   - Edmond Thorailler (GAUL)\n",
      "\n",
      "31. Similarité: 0.607\n",
      "   - Edmond Thorailler (GAUL)\n",
      "   - Edmond Thorailler (GAUL)\n",
      "\n",
      "RÉSUMÉ (ANALYSE ANONYMISÉE):\n",
      "   Associations affichées: 31\n",
      "   Groupes identifiés: 7\n",
      "SIMILARITÉS CROISÉES TOUR 1 <-> TOUR 2:\n",
      "--------------------------------------------------\n",
      "Tour 1: 217 discours\n",
      "Tour 2: 86 discours\n",
      "Similarité moyenne inter-tours: 0.133\n",
      "Top 10 similarités entre Tour 1 et Tour 2:\n",
      " 1. Similarité: 0.968\n",
      "    T1: Georges Domengie (GAUL)\n",
      "    T2: Roger Goemaere (GAUL)\n",
      " 2. Similarité: 0.875\n",
      "    T1: Robert Huwart (RDG)\n",
      "    T2: Robert Huwart (RDG)\n",
      " 3. Similarité: 0.796\n",
      "    T1: Jacques Blot (GAUL)\n",
      "    T2: Jacques Blot (GAUL)\n",
      " 4. Similarité: 0.677\n",
      "    T1: Georges Lemoine (SOC)\n",
      "    T2: Georges Lemoine (SOC)\n",
      " 5. Similarité: 0.512\n",
      "    T1: Edmond Thorailler (GAUL)\n",
      "    T2: Roger Goemaere (GAUL)\n",
      " 6. Similarité: 0.479\n",
      "    T1: Jacques Thyraud (GAUL)\n",
      "    T2: Roger Goemaere (GAUL)\n",
      " 7. Similarité: 0.478\n",
      "    T1: Georges Juillot (COM)\n",
      "    T2: Maurice Perche (COM)\n",
      " 8. Similarité: 0.448\n",
      "    T1: Jean Desanlis (CEN)\n",
      "    T2: Jean Desanlis (CEN)\n",
      " 9. Similarité: 0.435\n",
      "    T1: André Essirard (COM)\n",
      "    T2: Maurice Perche (COM)\n",
      "10. Similarité: 0.428\n",
      "    T1: Jean Desanlis (CEN)\n",
      "    T2: Jean Desanlis (CEN)\n",
      "Création de la visualisation t-SNE interactive...\n",
      "Visualisation sauvegardée: tsne_tours_combined_anonymized.html\n",
      "Analyse terminée\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script d'analyse de similarité des discours politiques Tours 1+2 combinés\n",
    "Avec anonymisation complète : suppression des noms, prénoms et partis\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CombinedToursAnalyzer:\n",
    "    def __init__(self, filepath='/Users/charlielezin/Desktop/Candidatures58-81-250825.csv'):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "        self.similarity_matrix = None\n",
    "        self.vectorizer = None\n",
    "        self.embeddings_2d = None\n",
    "        \n",
    "    def load_and_filter_data(self):\n",
    "        \"\"\"\n",
    "        Charge les données CSV et filtre pour les tours 1+2\n",
    "        \"\"\"\n",
    "        print(\"Chargement des données (Tours 1+2)...\")\n",
    "        self.df = pd.read_csv(self.filepath, delimiter=';', encoding='utf-8')\n",
    "        \n",
    "        print(f\"Nombre total de lignes : {len(self.df)}\")\n",
    "        \n",
    "        # Filtrage pour les tours 1 et 2\n",
    "        self.df = self.df[self.df['tour'].isin([1, 2])].copy()\n",
    "        print(f\"Filtrage tours 1+2 : {len(self.df)} candidatures\")\n",
    "        \n",
    "        # Filtrer ceux qui ont des discours non vides\n",
    "        self.df = self.df[\n",
    "            (self.df['discours'].notna()) & \n",
    "            (self.df['discours'].str.strip() != '') &\n",
    "            (self.df['discours'].str.len() > 100)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Nombre de candidatures avec discours : {len(self.df)}\")\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Répartition par tour :\")\n",
    "        print(self.df['tour'].value_counts().sort_index())\n",
    "        \n",
    "        print(f\"Répartition par nuance politique :\")\n",
    "        nuances_count = self.df['nuance_titulaire'].fillna('Inconnu').value_counts()\n",
    "        print(nuances_count.head(10))\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def extract_names_and_parties(self):\n",
    "        \"\"\"\n",
    "        Extrait tous les noms, prénoms et mentions de partis à supprimer\n",
    "        \"\"\"\n",
    "        names_to_remove = set()\n",
    "        parties_to_remove = set()\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            # Noms et prénoms titulaire et suppléant\n",
    "            for col in ['prenom_titulaire', 'nom_titulaire', 'prenom_suppleant', 'nom_suppleant']:\n",
    "                if pd.notna(row.get(col)):\n",
    "                    names_to_remove.add(str(row[col]).strip().lower())\n",
    "            \n",
    "            # Partis et sigles\n",
    "            party_columns = [\n",
    "                'parti_titulaire', 'parti_titulaire_1', 'parti_titulaire_2', \n",
    "                'parti_titulaire_3', 'parti_titulaire_4',\n",
    "                'parti_suppleant', 'parti_suppleant_1', 'parti_suppleant_2',\n",
    "                'parti_suppleant_3', 'parti_suppleant_4', 'parti_suppleant_5',\n",
    "                'sigle_titulaire', 'sigle_suppleant', 'nuance_titulaire', 'nuance_suppleant'\n",
    "            ]\n",
    "            \n",
    "            for col in party_columns:\n",
    "                if col in row and pd.notna(row[col]):\n",
    "                    party_value = str(row[col]).strip()\n",
    "                    if party_value and party_value.lower() not in ['nan', 'none', '']:\n",
    "                        party_parts = re.split(r'[+\\-\\s&/]', party_value)\n",
    "                        for part in party_parts:\n",
    "                            if part.strip() and len(part.strip()) > 1:\n",
    "                                parties_to_remove.add(part.strip().lower())\n",
    "        \n",
    "        # Termes de parti communs\n",
    "        common_parties = [\n",
    "            'gaulliste', 'gaullistes', 'socialiste', 'socialistes', 'communiste', 'communistes',\n",
    "            'radical', 'radicaux', 'centriste', 'centristes', 'républicain', 'républicains',\n",
    "            'démocrate', 'démocrates', 'indépendant', 'indépendants', 'paysan', 'paysans',\n",
    "            'union', 'mouvement', 'parti', 'rassemblement', 'front', 'coalition',\n",
    "            'nouvelle', 'république', 'nationale', 'populaire', 'française', 'français',\n",
    "            'gaulle', 'gaulles', 'gaul', 'com', 'soc', 'sfio', 'mrp', 'cnip', 'unr',\n",
    "            'psu', 'agr', 'exg', 'exd', 'div', 'rdg', 'rad', 'cen'\n",
    "        ]\n",
    "        \n",
    "        parties_to_remove.update(common_parties)\n",
    "        \n",
    "        # Nettoyer les sets\n",
    "        names_to_remove = {name for name in names_to_remove if name and len(name) > 1}\n",
    "        parties_to_remove = {party for party in parties_to_remove if party and len(party) > 2}\n",
    "        \n",
    "        print(f\"Anonymisation:\")\n",
    "        print(f\"   Noms/prénoms à supprimer: {len(names_to_remove)}\")\n",
    "        print(f\"   Termes de partis à supprimer: {len(parties_to_remove)}\")\n",
    "        \n",
    "        return names_to_remove, parties_to_remove\n",
    "    \n",
    "    def anonymize_text(self, text, names_to_remove, parties_to_remove):\n",
    "        \"\"\"\n",
    "        Anonymise un texte en supprimant les noms et mentions de partis\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Supprimer les noms et prénoms\n",
    "        for name in names_to_remove:\n",
    "            text = re.sub(rf'\\b{re.escape(name)}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Supprimer les mentions de partis\n",
    "        for party in parties_to_remove:\n",
    "            text = re.sub(rf'\\b{re.escape(party)}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Nettoyage général\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_french_stop_words(self):\n",
    "        \"\"\"\n",
    "        Liste étendue de mots vides français\n",
    "        \"\"\"\n",
    "        stop_words_base = [\n",
    "            'le', 'de', 'et', 'à', 'un', 'il', 'être', 'en', 'avoir', 'que', 'pour',\n",
    "            'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout', 'plus',\n",
    "            'par', 'grand', 'comme', 'mais', 'faire', 'mettre', 'nom', 'dire', 'ces',\n",
    "            'mon', 'où', 'même', 'y', 'aller', 'deux', 'moi', 'si', 'haut', 'bien', 'autre',\n",
    "            'fois', 'très', 'là', 'voir', 'arriver', 'donner', 'elle', 'lui', 'tel', 'quel',\n",
    "            'qui', 'du', 'des', 'aux', 'cette', 'leurs', 'nos', 'votre', 'dont', 'cette'\n",
    "        ]\n",
    "        \n",
    "        stop_words_political = [\n",
    "            'vous', 'nous', 'électeurs', 'électrices', 'france', 'français', 'française',\n",
    "            'république', 'national', 'nationale', 'gouvernement', 'politique', 'pays',\n",
    "            'état', 'public', 'sociale', 'social', 'économique', 'candidat', 'candidats',\n",
    "            'voter', 'vote', 'élection', 'élections', 'législatives', 'député', 'assemblée',\n",
    "            'monsieur', 'madame', 'citoyens', 'citoyennes', 'peuple', 'nation',\n",
    "            'suppléant', 'titulaire', 'liste', 'scrutin', 'circonscription'\n",
    "        ]\n",
    "        \n",
    "        return stop_words_base + stop_words_political\n",
    "    \n",
    "    def create_color_palette(self, nuances):\n",
    "        \"\"\"\n",
    "        Palette de couleurs pour les nuances politiques\n",
    "        \"\"\"\n",
    "        color_map = {\n",
    "            'COM': '#FF0000', 'COMM': '#FF0000', 'GAUL': '#191970', 'SOC': '#FFB6C1',\n",
    "            'CEN': '#87CEEB', 'EXG': '#8B0000', 'RAD': '#DAA520', 'PSU': '#FF8C00',\n",
    "            'AGR': '#008000', 'CEN-D': '#0000FF', 'EXD': '#000000', 'DIV': '#808080',\n",
    "            'RDG': '#FFE4E1', 'SFIO': '#FFB6C1', 'MRP': '#FF8C00', 'UDSR': '#87CEEB',\n",
    "            'RGR': '#FFE4E1', 'DROITE': '#191970', 'DROIT': '#191970', 'IND': '#808080',\n",
    "            'PAYSANS': '#008000'\n",
    "        }\n",
    "        \n",
    "        # Gérer les nuances manquantes\n",
    "        unique_nuances = list(set(nuances))\n",
    "        remaining_nuances = [n for n in unique_nuances if n not in color_map and pd.notna(n)]\n",
    "        \n",
    "        if remaining_nuances:\n",
    "            additional_colors = ['#DDA0DD', '#F0E68C', '#98FB98', '#F5DEB3', \n",
    "                               '#D2B48C', '#BC8F8F', '#CD853F', '#A0522D']\n",
    "            for i, nuance in enumerate(remaining_nuances):\n",
    "                color_map[nuance] = additional_colors[i % len(additional_colors)]\n",
    "        \n",
    "        color_map[np.nan] = '#CCCCCC'\n",
    "        color_map['Inconnu'] = '#CCCCCC'\n",
    "        color_map[None] = '#CCCCCC'\n",
    "        \n",
    "        return color_map\n",
    "    \n",
    "    def calculate_tfidf_similarity(self):\n",
    "        \"\"\"\n",
    "        Calcule la matrice TF-IDF et la similarité sur les textes anonymisés\n",
    "        \"\"\"\n",
    "        print(\"Anonymisation des textes...\")\n",
    "        \n",
    "        # Extraire les noms et partis à supprimer\n",
    "        names_to_remove, parties_to_remove = self.extract_names_and_parties()\n",
    "        \n",
    "        # Anonymiser tous les discours\n",
    "        self.df['discours_anonymized'] = self.df['discours'].apply(\n",
    "            lambda x: self.anonymize_text(x, names_to_remove, parties_to_remove)\n",
    "        )\n",
    "        \n",
    "        # Filtrer les textes trop courts après anonymisation\n",
    "        self.df = self.df[self.df['discours_anonymized'].str.len() > 50].copy()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Nombre de discours après anonymisation : {len(self.df)}\")\n",
    "        \n",
    "        if len(self.df) < 5:\n",
    "            raise ValueError(\"Pas assez de discours après anonymisation!\")\n",
    "        \n",
    "        print(\"Calcul de la matrice TF-IDF sur textes anonymisés...\")\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            stop_words=self.get_french_stop_words(),\n",
    "            ngram_range=(1, 2),\n",
    "            lowercase=True,\n",
    "            strip_accents='unicode'\n",
    "        )\n",
    "        \n",
    "        texts = self.df['discours_anonymized'].tolist()\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        \n",
    "        print(f\"Matrice TF-IDF : {tfidf_matrix.shape}\")\n",
    "        \n",
    "        print(\"Calcul de la similarité cosinus...\")\n",
    "        self.similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # Statistiques\n",
    "        similarity_upper = np.triu(self.similarity_matrix, k=1)\n",
    "        non_zero_similarities = similarity_upper[similarity_upper > 0]\n",
    "        \n",
    "        print(f\"Statistiques de similarité (textes anonymisés) :\")\n",
    "        print(f\"  - Moyenne : {non_zero_similarities.mean():.3f}\")\n",
    "        print(f\"  - Médiane : {np.median(non_zero_similarities):.3f}\")\n",
    "        print(f\"  - Max : {non_zero_similarities.max():.3f}\")\n",
    "        print(f\"  - Min : {non_zero_similarities.min():.3f}\")\n",
    "        \n",
    "        # Calcul du t-SNE\n",
    "        print(\"Calcul du t-SNE...\")\n",
    "        tsne = TSNE(n_components=2, perplexity=min(30, len(self.df)-1), \n",
    "                   random_state=42, verbose=0, max_iter=1000)\n",
    "        self.embeddings_2d = tsne.fit_transform(tfidf_matrix.toarray())\n",
    "        \n",
    "        return tfidf_matrix\n",
    "    \n",
    "    def analyze_similar_groups(self, top_n=50):\n",
    "        \"\"\"\n",
    "        Analyse des groupes de discours similaires\n",
    "        \"\"\"\n",
    "        print(f\"TOP {top_n} ASSOCIATIONS SIMILAIRES (TEXTES ANONYMISÉS)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        high_similarity_threshold = 0.6\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        for i in range(len(self.df)):\n",
    "            G.add_node(i)\n",
    "        \n",
    "        similarity_pairs = []\n",
    "        for i in range(len(self.similarity_matrix)):\n",
    "            for j in range(i+1, len(self.similarity_matrix)):\n",
    "                similarity = self.similarity_matrix[i, j]\n",
    "                if similarity >= high_similarity_threshold:\n",
    "                    G.add_edge(i, j, weight=similarity)\n",
    "                    similarity_pairs.append((similarity, i, j))\n",
    "        \n",
    "        similarity_pairs.sort(reverse=True)\n",
    "        \n",
    "        connected_components = list(nx.connected_components(G))\n",
    "        groups = [comp for comp in connected_components if len(comp) > 2]\n",
    "        \n",
    "        print(f\"Analyse avec seuil >= {high_similarity_threshold}\")\n",
    "        print(f\"Trouvé: {len(groups)} groupes, {len(similarity_pairs)} paires similaires\")\n",
    "        \n",
    "        associations_count = 0\n",
    "        \n",
    "        # Afficher les groupes\n",
    "        if groups and associations_count < top_n:\n",
    "            print(\"GROUPES DE DISCOURS SIMILAIRES (>=3 documents):\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for group in groups:\n",
    "                if associations_count >= top_n:\n",
    "                    break\n",
    "                \n",
    "                group_list = list(group)\n",
    "                group_size = len(group_list)\n",
    "                \n",
    "                print(f\"\\n{associations_count + 1}. GROUPE DE {group_size} DOCUMENTS SIMILAIRES:\")\n",
    "                \n",
    "                # Similarité moyenne interne\n",
    "                internal_similarities = []\n",
    "                for i in range(len(group_list)):\n",
    "                    for j in range(i+1, len(group_list)):\n",
    "                        internal_similarities.append(self.similarity_matrix[group_list[i], group_list[j]])\n",
    "                \n",
    "                avg_similarity = np.mean(internal_similarities)\n",
    "                print(f\"   Similarité moyenne interne: {avg_similarity:.3f}\")\n",
    "                \n",
    "                # Afficher les membres\n",
    "                for idx, doc_idx in enumerate(group_list):\n",
    "                    row_data = self.df.iloc[doc_idx]\n",
    "                    nom = f\"{row_data['prenom_titulaire']} {row_data['nom_titulaire']}\"\n",
    "                    nuance = row_data['nuance_titulaire']\n",
    "                    annee = int(row_data['annee_scrutin']) if pd.notna(row_data['annee_scrutin']) else 'N/A'\n",
    "                    tour = row_data.get('tour', 'N/A')\n",
    "                    \n",
    "                    print(f\"   - {nom} ({nuance}, {annee}, T{tour})\")\n",
    "                \n",
    "                associations_count += 1\n",
    "        \n",
    "        # Afficher les paires\n",
    "        if associations_count < top_n:\n",
    "            print(f\"\\nPAIRES DE DOCUMENTS TRÈS SIMILAIRES:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for similarity, i, j in similarity_pairs:\n",
    "                if associations_count >= top_n:\n",
    "                    break\n",
    "                \n",
    "                # Vérifier que pas déjà dans un groupe\n",
    "                already_in_group = any(i in group or j in group for group in groups)\n",
    "                if already_in_group:\n",
    "                    continue\n",
    "                \n",
    "                row_data_i = self.df.iloc[i]\n",
    "                row_data_j = self.df.iloc[j]\n",
    "                \n",
    "                nom_i = f\"{row_data_i['prenom_titulaire']} {row_data_i['nom_titulaire']}\"\n",
    "                nuance_i = row_data_i['nuance_titulaire']\n",
    "                \n",
    "                nom_j = f\"{row_data_j['prenom_titulaire']} {row_data_j['nom_titulaire']}\"\n",
    "                nuance_j = row_data_j['nuance_titulaire']\n",
    "                \n",
    "                print(f\"\\n{associations_count + 1}. Similarité: {similarity:.3f}\")\n",
    "                print(f\"   - {nom_i} ({nuance_i})\")\n",
    "                print(f\"   - {nom_j} ({nuance_j})\")\n",
    "                \n",
    "                associations_count += 1\n",
    "        \n",
    "        print(f\"\\nRÉSUMÉ (ANALYSE ANONYMISÉE):\")\n",
    "        print(f\"   Associations affichées: {associations_count}\")\n",
    "        print(f\"   Groupes identifiés: {len(groups)}\")\n",
    "        \n",
    "        return groups, similarity_pairs\n",
    "    \n",
    "    def analyze_cross_tour_similarities(self):\n",
    "        \"\"\"\n",
    "        Analyse spéciale des similarités croisées entre tours\n",
    "        \"\"\"\n",
    "        print(f\"SIMILARITÉS CROISÉES TOUR 1 <-> TOUR 2:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        tours = self.df['tour']\n",
    "        tour1_indices = np.where(tours == 1)[0]\n",
    "        tour2_indices = np.where(tours == 2)[0]\n",
    "        \n",
    "        if len(tour1_indices) == 0 or len(tour2_indices) == 0:\n",
    "            print(\"Pas assez de données pour l'analyse croisée\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Tour 1: {len(tour1_indices)} discours\")\n",
    "        print(f\"Tour 2: {len(tour2_indices)} discours\")\n",
    "        \n",
    "        # Similarité inter-tours\n",
    "        inter_similarities = []\n",
    "        cross_tour_pairs = []\n",
    "        for i in tour1_indices:\n",
    "            for j in tour2_indices:\n",
    "                similarity = self.similarity_matrix[i, j]\n",
    "                inter_similarities.append(similarity)\n",
    "                if similarity > 0.3:\n",
    "                    cross_tour_pairs.append((similarity, i, j))\n",
    "        \n",
    "        avg_inter = np.mean(inter_similarities)\n",
    "        print(f\"Similarité moyenne inter-tours: {avg_inter:.3f}\")\n",
    "        \n",
    "        cross_tour_pairs.sort(reverse=True)\n",
    "        \n",
    "        print(f\"Top 10 similarités entre Tour 1 et Tour 2:\")\n",
    "        for idx, (similarity, i, j) in enumerate(cross_tour_pairs[:10], 1):\n",
    "            row_i = self.df.iloc[i]\n",
    "            row_j = self.df.iloc[j]\n",
    "            \n",
    "            nom_i = f\"{row_i['prenom_titulaire']} {row_i['nom_titulaire']}\"\n",
    "            nuance_i = row_i['nuance_titulaire']\n",
    "            \n",
    "            nom_j = f\"{row_j['prenom_titulaire']} {row_j['nom_titulaire']}\"\n",
    "            nuance_j = row_j['nuance_titulaire']\n",
    "            \n",
    "            print(f\"{idx:2d}. Similarité: {similarity:.3f}\")\n",
    "            print(f\"    T1: {nom_i} ({nuance_i})\")\n",
    "            print(f\"    T2: {nom_j} ({nuance_j})\")\n",
    "    \n",
    "    def create_interactive_tsne(self):\n",
    "        \"\"\"\n",
    "        Visualisation t-SNE interactive\n",
    "        \"\"\"\n",
    "        print(\"Création de la visualisation t-SNE interactive...\")\n",
    "        \n",
    "        nuances = self.df['nuance_titulaire'].fillna('Inconnu')\n",
    "        color_palette = self.create_color_palette(nuances)\n",
    "        \n",
    "        hover_texts = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            nom = f\"{row['prenom_titulaire']} {row['nom_titulaire']}\"\n",
    "            nuance = row['nuance_titulaire'] if pd.notna(row['nuance_titulaire']) else 'Inconnu'\n",
    "            annee = int(row['annee_scrutin']) if pd.notna(row['annee_scrutin']) else 'N/A'\n",
    "            tour = row.get('tour', 'N/A')\n",
    "            departement = row.get('departement_nom', 'N/A')\n",
    "            circonscription = row.get('identifiant_circonscription', 'N/A')\n",
    "            \n",
    "            hover_text = f\"\"\"\n",
    "            <b>{nom}</b><br>\n",
    "            <b>Nuance:</b> {nuance}<br>\n",
    "            <b>Année:</b> {annee}<br>\n",
    "            <b>Tour:</b> {tour}<br>\n",
    "            <b>Département:</b> {departement}<br>\n",
    "            <b>Circonscription:</b> {circonscription}<br>\n",
    "            <i>Analyse sur texte anonymisé</i>\n",
    "            \"\"\"\n",
    "            hover_texts.append(hover_text)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        for nuance in set(nuances):\n",
    "            mask = nuances == nuance\n",
    "            if sum(mask) > 0:\n",
    "                indices = np.where(mask)[0]\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=self.embeddings_2d[mask, 0],\n",
    "                    y=self.embeddings_2d[mask, 1],\n",
    "                    mode='markers',\n",
    "                    name=f'{nuance} (n={sum(mask)})',\n",
    "                    marker=dict(\n",
    "                        color=color_palette.get(nuance, '#CCCCCC'),\n",
    "                        size=8,\n",
    "                        line=dict(width=1, color='black'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    hovertemplate='%{customdata}<extra></extra>',\n",
    "                    customdata=[hover_texts[i] for i in indices],\n",
    "                ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Analyse t-SNE Interactive - Tours 1+2 (Textes Anonymisés)<br><sub>Noms, prénoms et partis supprimés de l\\'analyse</sub>',\n",
    "            xaxis_title='Dimension t-SNE 1',\n",
    "            yaxis_title='Dimension t-SNE 2',\n",
    "            width=1200,\n",
    "            height=800,\n",
    "            hovermode='closest',\n",
    "            showlegend=True,\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=1.01)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Analyse principale Tours 1+2 avec anonymisation\n",
    "    \"\"\"\n",
    "    print(\"ANALYSE TF-IDF TOURS 1+2 COMBINÉS - TEXTES ANONYMISÉS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        analyzer = CombinedToursAnalyzer()\n",
    "        \n",
    "        # 1. Chargement des données\n",
    "        analyzer.load_and_filter_data()\n",
    "        \n",
    "        if len(analyzer.df) == 0:\n",
    "            print(\"Aucun discours trouvé!\")\n",
    "            return\n",
    "        \n",
    "        # 2. Calcul TF-IDF avec anonymisation\n",
    "        analyzer.calculate_tfidf_similarity()\n",
    "        \n",
    "        # 3. Analyse des groupes similaires\n",
    "        analyzer.analyze_similar_groups(top_n=50)\n",
    "        \n",
    "        # 4. Analyse croisée entre tours\n",
    "        analyzer.analyze_cross_tour_similarities()\n",
    "        \n",
    "        # 5. Visualisation interactive\n",
    "        fig_tsne = analyzer.create_interactive_tsne()\n",
    "        fig_tsne.write_html(\"tsne_tours_combined_anonymized.html\")\n",
    "        \n",
    "        print(f\"Visualisation sauvegardée: tsne_tours_combined_anonymized.html\")\n",
    "        print(\"Analyse terminée\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b42c0bb-1745-4bb8-98e4-d90c8cfbda59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE DE CLUSTERING - DISCOURS POLITIQUES TOURS 1+2\n",
      "============================================================\n",
      "Chargement des données (Tours 1+2)...\n",
      "Nombre total de lignes : 303\n",
      "Filtrage tours 1+2 : 303 candidatures\n",
      "Nombre de candidatures avec discours : 303\n",
      "Répartition par tour :\n",
      "tour\n",
      "1    217\n",
      "2     86\n",
      "Name: count, dtype: int64\n",
      "Répartition par nuance politique :\n",
      "nuance_titulaire\n",
      "GAUL     62\n",
      "SOC      58\n",
      "COM      54\n",
      "CEN      36\n",
      "RAD      21\n",
      "EXG      15\n",
      "CEN-D    15\n",
      "PSU      11\n",
      "AGR      11\n",
      "EXD      10\n",
      "Name: count, dtype: int64\n",
      "Anonymisation des textes...\n",
      "Anonymisation:\n",
      "   Noms/prénoms à supprimer: 370\n",
      "   Termes de partis à supprimer: 135\n",
      "Nombre de discours après anonymisation : 303\n",
      "Calcul de la matrice TF-IDF sur textes anonymisés...\n",
      "Matrice TF-IDF : (303, 1000)\n",
      "Calcul de la similarité cosinus...\n",
      "Statistiques de similarité (textes anonymisés) :\n",
      "  - Moyenne : 0.165\n",
      "  - Médiane : 0.156\n",
      "  - Max : 1.000\n",
      "  - Min : 0.003\n",
      "Calcul du t-SNE...\n",
      "Recherche du nombre optimal de clusters...\n",
      "  k=2: Silhouette=0.035\n",
      "  k=3: Silhouette=0.028\n",
      "  k=4: Silhouette=0.027\n",
      "  k=5: Silhouette=0.025\n",
      "  k=6: Silhouette=0.031\n",
      "  k=7: Silhouette=0.028\n",
      "  k=8: Silhouette=0.037\n",
      "  k=9: Silhouette=0.034\n",
      "  k=10: Silhouette=0.034\n",
      "Nombre optimal de clusters : 8 (Silhouette: 0.037)\n",
      "Clustering avec 8 clusters...\n",
      "Répartition des clusters :\n",
      "  Cluster 0: 69 documents\n",
      "  Cluster 1: 24 documents\n",
      "  Cluster 2: 44 documents\n",
      "  Cluster 3: 15 documents\n",
      "  Cluster 4: 37 documents\n",
      "  Cluster 5: 12 documents\n",
      "  Cluster 6: 30 documents\n",
      "  Cluster 7: 72 documents\n",
      "ANALYSE DES 8 CLUSTERS\n",
      "==================================================\n",
      "\n",
      "CLUSTER 0 (69 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - SOC: 19 (27.5%)\n",
      "  - GAUL: 16 (23.2%)\n",
      "  - CEN: 7 (10.1%)\n",
      "Tours:\n",
      "  - Tour 1: 65 (94.2%)\n",
      "  - Tour 2: 4 (5.8%)\n",
      "Exemples:\n",
      "  - Paul Antier (CEN, 1967, T1)\n",
      "  - Jean Bidault (RAD, 1962, T1)\n",
      "  - Jacques Blot (GAUL, 1981, T1)\n",
      "\n",
      "CLUSTER 1 (24 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - COM: 18 (75.0%)\n",
      "  - PSU: 6 (25.0%)\n",
      "Tours:\n",
      "  - Tour 1: 24 (100.0%)\n",
      "Exemples:\n",
      "  - Jean Billeau (PSU, 1968, T1)\n",
      "  - Marcel Chartrain (COM, 1962, T1)\n",
      "  - Daniel Cogneau (PSU, 1967, T1)\n",
      "\n",
      "CLUSTER 2 (44 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - COM: 18 (40.9%)\n",
      "  - SOC: 10 (22.7%)\n",
      "  - PSU: 3 (6.8%)\n",
      "Tours:\n",
      "  - Tour 1: 36 (81.8%)\n",
      "  - Tour 2: 8 (18.2%)\n",
      "Exemples:\n",
      "  - Jean Auger (COM, 1973, T1)\n",
      "  - Claude Breton (COM, 1973, T1)\n",
      "  - Claude Cézard (DIV, 1968, T1)\n",
      "\n",
      "CLUSTER 3 (15 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - EXG: 14 (93.3%)\n",
      "  - PSU: 1 (6.7%)\n",
      "Tours:\n",
      "  - Tour 1: 15 (100.0%)\n",
      "Exemples:\n",
      "  - Claude Bedu (EXG, 1978, T1)\n",
      "  - José Duménil (EXG, 1978, T1)\n",
      "  - Yves Elbory (PSU, 1978, T1)\n",
      "\n",
      "CLUSTER 4 (37 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - SOC: 18 (48.6%)\n",
      "  - GAUL: 7 (18.9%)\n",
      "  - CEN: 7 (18.9%)\n",
      "Tours:\n",
      "  - Tour 2: 36 (97.3%)\n",
      "  - Tour 1: 1 (2.7%)\n",
      "Exemples:\n",
      "  - François Archambault de Montfort (GAUL, 1958, T2)\n",
      "  - André Burlot (CEN, 1962, T2)\n",
      "  - Paul Cormier (CEN, 1968, T2)\n",
      "\n",
      "CLUSTER 5 (12 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - COM: 11 (91.7%)\n",
      "  - SOC: 1 (8.3%)\n",
      "Tours:\n",
      "  - Tour 2: 8 (66.7%)\n",
      "  - Tour 1: 4 (33.3%)\n",
      "Exemples:\n",
      "  - Marcel Chartrain (COM, 1958, T2)\n",
      "  - Marcel Chartrain (COM, 1958, T1)\n",
      "  - André Gagnon (SOC, 1958, T1)\n",
      "\n",
      "CLUSTER 6 (30 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - RAD: 11 (36.7%)\n",
      "  - GAUL: 10 (33.3%)\n",
      "  - CEN-D: 3 (10.0%)\n",
      "Tours:\n",
      "  - Tour 2: 21 (70.0%)\n",
      "  - Tour 1: 9 (30.0%)\n",
      "Exemples:\n",
      "  - Henri Bonnet (RAD, 1958, T1)\n",
      "  - André Burlot (CEN, 1958, T2)\n",
      "  - Michel Castaing (RAD, 1973, T2)\n",
      "\n",
      "CLUSTER 7 (72 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - GAUL: 27 (37.5%)\n",
      "  - CEN: 18 (25.0%)\n",
      "  - SOC: 8 (11.1%)\n",
      "Tours:\n",
      "  - Tour 1: 63 (87.5%)\n",
      "  - Tour 2: 9 (12.5%)\n",
      "Exemples:\n",
      "  - François Archambault de Montfort (GAUL, 1958, T1)\n",
      "  - François Archambault de Montfort (GAUL, 1962, T1)\n",
      "  - Gérard Belorgey (DIV, 1981, T1)\n",
      "Création de la visualisation t-SNE avec clusters...\n",
      "Visualisation avec clusters sauvegardée: clustering_tours_combined.html\n",
      "Analyse de clustering terminée\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script de clustering pour l'analyse des discours politiques Tours 1+2\n",
    "Basé sur le code d'analyse existant avec ajout du clustering et visualisation des clusters\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from plotly.colors import qualitative\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ClusteringToursAnalyzer:\n",
    "    def __init__(self, filepath='/Users/charlielezin/Desktop/Candidatures58-81-250825.csv'):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "        self.similarity_matrix = None\n",
    "        self.vectorizer = None\n",
    "        self.embeddings_2d = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.clusters = None\n",
    "        self.n_clusters = None\n",
    "        \n",
    "    def load_and_filter_data(self):\n",
    "        \"\"\"\n",
    "        Charge les données CSV et filtre pour les tours 1+2\n",
    "        \"\"\"\n",
    "        print(\"Chargement des données (Tours 1+2)...\")\n",
    "        self.df = pd.read_csv(self.filepath, delimiter=';', encoding='utf-8')\n",
    "        \n",
    "        print(f\"Nombre total de lignes : {len(self.df)}\")\n",
    "        \n",
    "        # Filtrage pour les tours 1 et 2\n",
    "        self.df = self.df[self.df['tour'].isin([1, 2])].copy()\n",
    "        print(f\"Filtrage tours 1+2 : {len(self.df)} candidatures\")\n",
    "        \n",
    "        # Filtrer ceux qui ont des discours non vides\n",
    "        self.df = self.df[\n",
    "            (self.df['discours'].notna()) & \n",
    "            (self.df['discours'].str.strip() != '') &\n",
    "            (self.df['discours'].str.len() > 100)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Nombre de candidatures avec discours : {len(self.df)}\")\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Répartition par tour :\")\n",
    "        print(self.df['tour'].value_counts().sort_index())\n",
    "        \n",
    "        print(f\"Répartition par nuance politique :\")\n",
    "        nuances_count = self.df['nuance_titulaire'].fillna('Inconnu').value_counts()\n",
    "        print(nuances_count.head(10))\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def extract_names_and_parties(self):\n",
    "        \"\"\"\n",
    "        Extrait tous les noms, prénoms et mentions de partis à supprimer\n",
    "        \"\"\"\n",
    "        names_to_remove = set()\n",
    "        parties_to_remove = set()\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            # Noms et prénoms titulaire et suppléant\n",
    "            for col in ['prenom_titulaire', 'nom_titulaire', 'prenom_suppleant', 'nom_suppleant']:\n",
    "                if pd.notna(row.get(col)):\n",
    "                    names_to_remove.add(str(row[col]).strip().lower())\n",
    "            \n",
    "            # Partis et sigles\n",
    "            party_columns = [\n",
    "                'parti_titulaire', 'parti_titulaire_1', 'parti_titulaire_2', \n",
    "                'parti_titulaire_3', 'parti_titulaire_4',\n",
    "                'parti_suppleant', 'parti_suppleant_1', 'parti_suppleant_2',\n",
    "                'parti_suppleant_3', 'parti_suppleant_4', 'parti_suppleant_5',\n",
    "                'sigle_titulaire', 'sigle_suppleant', 'nuance_titulaire', 'nuance_suppleant'\n",
    "            ]\n",
    "            \n",
    "            for col in party_columns:\n",
    "                if col in row and pd.notna(row[col]):\n",
    "                    party_value = str(row[col]).strip()\n",
    "                    if party_value and party_value.lower() not in ['nan', 'none', '']:\n",
    "                        party_parts = re.split(r'[+\\-\\s&/]', party_value)\n",
    "                        for part in party_parts:\n",
    "                            if part.strip() and len(part.strip()) > 1:\n",
    "                                parties_to_remove.add(part.strip().lower())\n",
    "        \n",
    "        # Termes de parti communs\n",
    "        common_parties = [\n",
    "            'gaulliste', 'gaullistes', 'socialiste', 'socialistes', 'communiste', 'communistes',\n",
    "            'radical', 'radicaux', 'centriste', 'centristes', 'républicain', 'républicains',\n",
    "            'démocrate', 'démocrates', 'indépendant', 'indépendants', 'paysan', 'paysans',\n",
    "            'union', 'mouvement', 'parti', 'rassemblement', 'front', 'coalition',\n",
    "            'nouvelle', 'république', 'nationale', 'populaire', 'française', 'français',\n",
    "            'gaulle', 'gaulles', 'gaul', 'com', 'soc', 'sfio', 'mrp', 'cnip', 'unr',\n",
    "            'psu', 'agr', 'exg', 'exd', 'div', 'rdg', 'rad', 'cen'\n",
    "        ]\n",
    "        \n",
    "        parties_to_remove.update(common_parties)\n",
    "        \n",
    "        # Nettoyer les sets\n",
    "        names_to_remove = {name for name in names_to_remove if name and len(name) > 1}\n",
    "        parties_to_remove = {party for party in parties_to_remove if party and len(party) > 2}\n",
    "        \n",
    "        print(f\"Anonymisation:\")\n",
    "        print(f\"   Noms/prénoms à supprimer: {len(names_to_remove)}\")\n",
    "        print(f\"   Termes de partis à supprimer: {len(parties_to_remove)}\")\n",
    "        \n",
    "        return names_to_remove, parties_to_remove\n",
    "    \n",
    "    def anonymize_text(self, text, names_to_remove, parties_to_remove):\n",
    "        \"\"\"\n",
    "        Anonymise un texte en supprimant les noms et mentions de partis\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Supprimer les noms et prénoms\n",
    "        for name in names_to_remove:\n",
    "            text = re.sub(rf'\\b{re.escape(name)}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Supprimer les mentions de partis\n",
    "        for party in parties_to_remove:\n",
    "            text = re.sub(rf'\\b{re.escape(party)}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Nettoyage général\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_french_stop_words(self):\n",
    "        \"\"\"\n",
    "        Liste étendue de mots vides français\n",
    "        \"\"\"\n",
    "        stop_words_base = [\n",
    "            'le', 'de', 'et', 'à', 'un', 'il', 'être', 'en', 'avoir', 'que', 'pour',\n",
    "            'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout', 'plus',\n",
    "            'par', 'grand', 'comme', 'mais', 'faire', 'mettre', 'nom', 'dire', 'ces',\n",
    "            'mon', 'où', 'même', 'y', 'aller', 'deux', 'moi', 'si', 'haut', 'bien', 'autre',\n",
    "            'fois', 'très', 'là', 'voir', 'arriver', 'donner', 'elle', 'lui', 'tel', 'quel',\n",
    "            'qui', 'du', 'des', 'aux', 'cette', 'leurs', 'nos', 'votre', 'dont', 'cette'\n",
    "        ]\n",
    "        \n",
    "        stop_words_political = [\n",
    "            'vous', 'nous', 'électeurs', 'électrices', 'france', 'français', 'française',\n",
    "            'république', 'national', 'nationale', 'gouvernement', 'politique', 'pays',\n",
    "            'état', 'public', 'sociale', 'social', 'économique', 'candidat', 'candidats',\n",
    "            'voter', 'vote', 'élection', 'élections', 'législatives', 'député', 'assemblée',\n",
    "            'monsieur', 'madame', 'citoyens', 'citoyennes', 'peuple', 'nation',\n",
    "            'suppléant', 'titulaire', 'liste', 'scrutin', 'circonscription'\n",
    "        ]\n",
    "        \n",
    "        return stop_words_base + stop_words_political\n",
    "    \n",
    "    def create_color_palette(self, nuances):\n",
    "        \"\"\"\n",
    "        Palette de couleurs pour les nuances politiques\n",
    "        \"\"\"\n",
    "        color_map = {\n",
    "            'COM': '#FF0000', 'COMM': '#FF0000', 'GAUL': '#191970', 'SOC': '#FFB6C1',\n",
    "            'CEN': '#87CEEB', 'EXG': '#8B0000', 'RAD': '#DAA520', 'PSU': '#FF8C00',\n",
    "            'AGR': '#008000', 'CEN-D': '#0000FF', 'EXD': '#000000', 'DIV': '#808080',\n",
    "            'RDG': '#FFE4E1', 'SFIO': '#FFB6C1', 'MRP': '#FF8C00', 'UDSR': '#87CEEB',\n",
    "            'RGR': '#FFE4E1', 'DROITE': '#191970', 'DROIT': '#191970', 'IND': '#808080',\n",
    "            'PAYSANS': '#008000'\n",
    "        }\n",
    "        \n",
    "        # Gérer les nuances manquantes\n",
    "        unique_nuances = list(set(nuances))\n",
    "        remaining_nuances = [n for n in unique_nuances if n not in color_map and pd.notna(n)]\n",
    "        \n",
    "        if remaining_nuances:\n",
    "            additional_colors = ['#DDA0DD', '#F0E68C', '#98FB98', '#F5DEB3', \n",
    "                               '#D2B48C', '#BC8F8F', '#CD853F', '#A0522D']\n",
    "            for i, nuance in enumerate(remaining_nuances):\n",
    "                color_map[nuance] = additional_colors[i % len(additional_colors)]\n",
    "        \n",
    "        color_map[np.nan] = '#CCCCCC'\n",
    "        color_map['Inconnu'] = '#CCCCCC'\n",
    "        color_map[None] = '#CCCCCC'\n",
    "        \n",
    "        return color_map\n",
    "    \n",
    "    def calculate_tfidf_similarity(self):\n",
    "        \"\"\"\n",
    "        Calcule la matrice TF-IDF et la similarité sur les textes anonymisés\n",
    "        \"\"\"\n",
    "        print(\"Anonymisation des textes...\")\n",
    "        \n",
    "        # Extraire les noms et partis à supprimer\n",
    "        names_to_remove, parties_to_remove = self.extract_names_and_parties()\n",
    "        \n",
    "        # Anonymiser tous les discours\n",
    "        self.df['discours_anonymized'] = self.df['discours'].apply(\n",
    "            lambda x: self.anonymize_text(x, names_to_remove, parties_to_remove)\n",
    "        )\n",
    "        \n",
    "        # Filtrer les textes trop courts après anonymisation\n",
    "        self.df = self.df[self.df['discours_anonymized'].str.len() > 50].copy()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Nombre de discours après anonymisation : {len(self.df)}\")\n",
    "        \n",
    "        if len(self.df) < 5:\n",
    "            raise ValueError(\"Pas assez de discours après anonymisation!\")\n",
    "        \n",
    "        print(\"Calcul de la matrice TF-IDF sur textes anonymisés...\")\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            stop_words=self.get_french_stop_words(),\n",
    "            ngram_range=(1, 2),\n",
    "            lowercase=True,\n",
    "            strip_accents='unicode'\n",
    "        )\n",
    "        \n",
    "        texts = self.df['discours_anonymized'].tolist()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        \n",
    "        print(f\"Matrice TF-IDF : {self.tfidf_matrix.shape}\")\n",
    "        \n",
    "        print(\"Calcul de la similarité cosinus...\")\n",
    "        self.similarity_matrix = cosine_similarity(self.tfidf_matrix)\n",
    "        \n",
    "        # Statistiques\n",
    "        similarity_upper = np.triu(self.similarity_matrix, k=1)\n",
    "        non_zero_similarities = similarity_upper[similarity_upper > 0]\n",
    "        \n",
    "        print(f\"Statistiques de similarité (textes anonymisés) :\")\n",
    "        print(f\"  - Moyenne : {non_zero_similarities.mean():.3f}\")\n",
    "        print(f\"  - Médiane : {np.median(non_zero_similarities):.3f}\")\n",
    "        print(f\"  - Max : {non_zero_similarities.max():.3f}\")\n",
    "        print(f\"  - Min : {non_zero_similarities.min():.3f}\")\n",
    "        \n",
    "        # Calcul du t-SNE\n",
    "        print(\"Calcul du t-SNE...\")\n",
    "        tsne = TSNE(n_components=2, perplexity=min(30, len(self.df)-1), \n",
    "                   random_state=42, verbose=0, max_iter=1000)\n",
    "        self.embeddings_2d = tsne.fit_transform(self.tfidf_matrix.toarray())\n",
    "        \n",
    "        return self.tfidf_matrix\n",
    "    \n",
    "    def find_optimal_clusters(self, max_clusters=None):\n",
    "        \"\"\"Détermine le nombre optimal de clusters\"\"\"\n",
    "        print(\"Recherche du nombre optimal de clusters...\")\n",
    "        \n",
    "        if max_clusters is None:\n",
    "            max_clusters = min(10, len(self.df) // 3)\n",
    "        \n",
    "        silhouette_scores = []\n",
    "        \n",
    "        for k in range(2, max_clusters + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(self.tfidf_matrix.toarray())\n",
    "            \n",
    "            silhouette_avg = silhouette_score(self.tfidf_matrix.toarray(), cluster_labels)\n",
    "            silhouette_scores.append((k, silhouette_avg))\n",
    "            \n",
    "            print(f\"  k={k}: Silhouette={silhouette_avg:.3f}\")\n",
    "        \n",
    "        # Choisir le k avec le meilleur score de silhouette\n",
    "        best_k = max(silhouette_scores, key=lambda x: x[1])[0]\n",
    "        best_score = max(silhouette_scores, key=lambda x: x[1])[1]\n",
    "        \n",
    "        print(f\"Nombre optimal de clusters : {best_k} (Silhouette: {best_score:.3f})\")\n",
    "        \n",
    "        return best_k\n",
    "    \n",
    "    def perform_clustering(self, n_clusters=None):\n",
    "        \"\"\"Effectue le clustering\"\"\"\n",
    "        if n_clusters is None:\n",
    "            n_clusters = self.find_optimal_clusters()\n",
    "        \n",
    "        print(f\"Clustering avec {n_clusters} clusters...\")\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        self.clusters = kmeans.fit_predict(self.tfidf_matrix.toarray())\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        # Ajouter les clusters au DataFrame\n",
    "        self.df['cluster'] = self.clusters\n",
    "        \n",
    "        print(\"Répartition des clusters :\")\n",
    "        for i in range(n_clusters):\n",
    "            cluster_size = np.sum(self.clusters == i)\n",
    "            print(f\"  Cluster {i}: {cluster_size} documents\")\n",
    "        \n",
    "        return self.clusters\n",
    "    \n",
    "    def analyze_clusters(self):\n",
    "        \"\"\"Analyse détaillée des clusters\"\"\"\n",
    "        print(f\"ANALYSE DES {self.n_clusters} CLUSTERS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            cluster_docs = self.df[self.df['cluster'] == cluster_id]\n",
    "            \n",
    "            print(f\"\\nCLUSTER {cluster_id} ({len(cluster_docs)} documents)\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Répartition par nuance\n",
    "            nuances = cluster_docs['nuance_titulaire'].fillna('Inconnu').value_counts()\n",
    "            print(f\"Nuances principales:\")\n",
    "            for nuance, count in nuances.head(3).items():\n",
    "                pct = (count / len(cluster_docs)) * 100\n",
    "                print(f\"  - {nuance}: {count} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Répartition par tour\n",
    "            tours = cluster_docs['tour'].value_counts()\n",
    "            print(f\"Tours:\")\n",
    "            for tour, count in tours.items():\n",
    "                pct = (count / len(cluster_docs)) * 100\n",
    "                print(f\"  - Tour {tour}: {count} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Exemples de candidats\n",
    "            print(\"Exemples:\")\n",
    "            for _, row in cluster_docs.head(3).iterrows():\n",
    "                nom = f\"{row['prenom_titulaire']} {row['nom_titulaire']}\"\n",
    "                nuance = row['nuance_titulaire'] if pd.notna(row['nuance_titulaire']) else 'Inconnu'\n",
    "                tour = row.get('tour', 'N/A')\n",
    "                annee = int(row['annee_scrutin']) if pd.notna(row['annee_scrutin']) else 'N/A'\n",
    "                print(f\"  - {nom} ({nuance}, {annee}, T{tour})\")\n",
    "    \n",
    "    def get_cluster_hull_points(self, cluster_id):\n",
    "        \"\"\"Calcule l'enveloppe des points d'un cluster\"\"\"\n",
    "        cluster_mask = self.clusters == cluster_id\n",
    "        if np.sum(cluster_mask) < 3:\n",
    "            return None\n",
    "        \n",
    "        cluster_points = self.embeddings_2d[cluster_mask]\n",
    "        \n",
    "        try:\n",
    "            from scipy.spatial import ConvexHull\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            hull_points = np.vstack([hull_points, hull_points[0]])\n",
    "            return hull_points\n",
    "        except:\n",
    "            # Utiliser un cercle approximatif si ConvexHull échoue\n",
    "            center = np.mean(cluster_points, axis=0)\n",
    "            distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "            radius = np.max(distances) * 1.2\n",
    "            \n",
    "            angles = np.linspace(0, 2*np.pi, 30)\n",
    "            circle_points = np.column_stack([\n",
    "                center[0] + radius * np.cos(angles),\n",
    "                center[1] + radius * np.sin(angles)\n",
    "            ])\n",
    "            return circle_points\n",
    "    \n",
    "    def create_interactive_tsne_with_clusters(self):\n",
    "        \"\"\"\n",
    "        Visualisation t-SNE interactive avec clusters entourés\n",
    "        \"\"\"\n",
    "        print(\"Création de la visualisation t-SNE avec clusters...\")\n",
    "        \n",
    "        nuances = self.df['nuance_titulaire'].fillna('Inconnu')\n",
    "        color_palette = self.create_color_palette(nuances)\n",
    "        \n",
    "        # Couleurs pour les enveloppes de clusters\n",
    "        cluster_colors = qualitative.Plotly[:self.n_clusters]\n",
    "        \n",
    "        hover_texts = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            nom = f\"{row['prenom_titulaire']} {row['nom_titulaire']}\"\n",
    "            nuance = row['nuance_titulaire'] if pd.notna(row['nuance_titulaire']) else 'Inconnu'\n",
    "            annee = int(row['annee_scrutin']) if pd.notna(row['annee_scrutin']) else 'N/A'\n",
    "            tour = row.get('tour', 'N/A')\n",
    "            cluster_id = row.get('cluster', 'N/A')\n",
    "            departement = row.get('departement_nom', 'N/A')\n",
    "            circonscription = row.get('identifiant_circonscription', 'N/A')\n",
    "            \n",
    "            hover_text = (f\"<b>{nom}</b><br>\"\n",
    "                         f\"<b>Nuance:</b> {nuance}<br>\"\n",
    "                         f\"<b>Cluster:</b> {cluster_id}<br>\"\n",
    "                         f\"<b>Année:</b> {annee}<br>\"\n",
    "                         f\"<b>Tour:</b> {tour}<br>\"\n",
    "                         f\"<b>Département:</b> {departement}<br>\"\n",
    "                         f\"<b>Circonscription:</b> {circonscription}<br>\"\n",
    "                         f\"<i>Analyse sur texte anonymisé</i>\")\n",
    "            hover_texts.append(hover_text)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Ajouter d'abord les enveloppes des clusters\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            hull_points = self.get_cluster_hull_points(cluster_id)\n",
    "            if hull_points is not None:\n",
    "                cluster_size = np.sum(self.clusters == cluster_id)\n",
    "                color_hex = cluster_colors[cluster_id]\n",
    "                \n",
    "                # Convertir la couleur hex en RGB pour le fill\n",
    "                r = int(color_hex[1:3], 16)\n",
    "                g = int(color_hex[3:5], 16)\n",
    "                b = int(color_hex[5:7], 16)\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=hull_points[:, 0],\n",
    "                    y=hull_points[:, 1],\n",
    "                    mode='lines',\n",
    "                    line=dict(color=color_hex, width=2, dash='dash'),\n",
    "                    name=f'Cluster {cluster_id} ({cluster_size} docs)',\n",
    "                    fill='toself',\n",
    "                    fillcolor=f'rgba({r}, {g}, {b}, 0.1)',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=True\n",
    "                ))\n",
    "        \n",
    "        # Ajouter ensuite les points par nuance politique\n",
    "        for nuance in set(nuances):\n",
    "            mask = nuances == nuance\n",
    "            if sum(mask) > 0:\n",
    "                indices = np.where(mask)[0]\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=self.embeddings_2d[mask, 0],\n",
    "                    y=self.embeddings_2d[mask, 1],\n",
    "                    mode='markers',\n",
    "                    name=f'{nuance} (n={sum(mask)})',\n",
    "                    marker=dict(\n",
    "                        color=color_palette.get(nuance, '#CCCCCC'),\n",
    "                        size=8,\n",
    "                        line=dict(width=1, color='black'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    hovertemplate='%{customdata}<extra></extra>',\n",
    "                    customdata=[hover_texts[i] for i in indices]\n",
    "                ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Analyse t-SNE avec {self.n_clusters} Clusters - Tours 1+2<br><sub>Clusters entourés sur textes anonymisés</sub>',\n",
    "            xaxis_title='Dimension t-SNE 1',\n",
    "            yaxis_title='Dimension t-SNE 2',\n",
    "            width=1400,\n",
    "            height=900,\n",
    "            hovermode='closest',\n",
    "            showlegend=True,\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=1.01),\n",
    "            plot_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Analyse principale avec clustering\n",
    "    \"\"\"\n",
    "    print(\"ANALYSE DE CLUSTERING - DISCOURS POLITIQUES TOURS 1+2\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        analyzer = ClusteringToursAnalyzer()\n",
    "        \n",
    "        # 1. Chargement des données\n",
    "        analyzer.load_and_filter_data()\n",
    "        \n",
    "        if len(analyzer.df) == 0:\n",
    "            print(\"Aucun discours trouvé!\")\n",
    "            return\n",
    "        \n",
    "        # 2. Calcul TF-IDF avec anonymisation\n",
    "        analyzer.calculate_tfidf_similarity()\n",
    "        \n",
    "        # 3. Clustering\n",
    "        analyzer.perform_clustering()\n",
    "        \n",
    "        # 4. Analyse des clusters\n",
    "        analyzer.analyze_clusters()\n",
    "        \n",
    "        # 5. Visualisation interactive avec clusters entourés\n",
    "        fig = analyzer.create_interactive_tsne_with_clusters()\n",
    "        fig.write_html(\"clustering_tours_combined.html\")\n",
    "        \n",
    "        print(f\"Visualisation avec clusters sauvegardée: clustering_tours_combined.html\")\n",
    "        print(\"Analyse de clustering terminée\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9ef2c-7084-4e2c-90d1-6c3f7e9f1227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a261bdc8-ca4c-41c8-aeff-bb08ab95178a",
   "metadata": {},
   "source": [
    "## Analyse avec DBSCAN directement sur la matrice de similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f4148c-e400-4ca1-b7c3-2656c68940d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE DE CLUSTERING - DISCOURS POLITIQUES TOURS 1+2\n",
      "============================================================\n",
      "Chargement des données (Tours 1+2)...\n",
      "Nombre total de lignes : 303\n",
      "Filtrage tours 1+2 : 303 candidatures\n",
      "Nombre de candidatures avec discours : 303\n",
      "Répartition par tour :\n",
      "tour\n",
      "1    217\n",
      "2     86\n",
      "Name: count, dtype: int64\n",
      "Répartition par nuance politique :\n",
      "nuance_titulaire\n",
      "GAUL     62\n",
      "SOC      58\n",
      "COM      54\n",
      "CEN      36\n",
      "RAD      21\n",
      "EXG      15\n",
      "CEN-D    15\n",
      "PSU      11\n",
      "AGR      11\n",
      "EXD      10\n",
      "Name: count, dtype: int64\n",
      "Anonymisation des textes...\n",
      "Anonymisation:\n",
      "   Noms/prénoms à supprimer: 370\n",
      "   Termes de partis à supprimer: 135\n",
      "Nombre de discours après anonymisation : 303\n",
      "Calcul de la matrice TF-IDF sur textes anonymisés...\n",
      "Matrice TF-IDF : (303, 1000)\n",
      "Calcul de la similarité cosinus...\n",
      "Statistiques de similarité (textes anonymisés) :\n",
      "  - Moyenne : 0.165\n",
      "  - Médiane : 0.156\n",
      "  - Max : 1.000\n",
      "  - Min : 0.003\n",
      "Calcul du t-SNE...\n",
      "Clustering DBSCAN sur l'espace TF-IDF haute dimension...\n",
      "  eps=0.3: 1 clusters, 98.0% bruit\n",
      "  eps=0.4: 1 clusters, 98.0% bruit\n",
      "  eps=0.5: 1 clusters, 98.0% bruit\n",
      "  eps=0.6: 1 clusters, 98.0% bruit\n",
      "  eps=0.7: 1 clusters, 98.0% bruit\n",
      "DBSCAN n'a pas trouvé de bonne configuration, utilisation de K-means avec k=4\n",
      "Clustering K-means avec 4 clusters...\n",
      "Répartition des clusters :\n",
      "  Cluster 0: 102 documents\n",
      "  Cluster 1: 98 documents\n",
      "  Cluster 2: 54 documents\n",
      "  Cluster 3: 49 documents\n",
      "ANALYSE DES CLUSTERS\n",
      "==================================================\n",
      "\n",
      "CLUSTER 0 (102 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - GAUL: 26 (25.5%)\n",
      "  - CEN: 24 (23.5%)\n",
      "  - SOC: 17 (16.7%)\n",
      "Tours:\n",
      "  - Tour 1: 93 (91.2%)\n",
      "  - Tour 2: 9 (8.8%)\n",
      "Exemples:\n",
      "  - François Archambault de Montfort (GAUL, 1958, T1)\n",
      "  - François Archambault de Montfort (GAUL, 1962, T1)\n",
      "  - Gérard Belorgey (DIV, 1981, T1)\n",
      "\n",
      "CLUSTER 1 (98 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - COM: 35 (35.7%)\n",
      "  - SOC: 16 (16.3%)\n",
      "  - GAUL: 14 (14.3%)\n",
      "Tours:\n",
      "  - Tour 1: 93 (94.9%)\n",
      "  - Tour 2: 5 (5.1%)\n",
      "Exemples:\n",
      "  - Paul Antier (CEN, 1967, T1)\n",
      "  - Jean Auger (COM, 1973, T1)\n",
      "  - Jean Billeau (PSU, 1968, T1)\n",
      "\n",
      "CLUSTER 2 (54 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - GAUL: 19 (35.2%)\n",
      "  - SOC: 16 (29.6%)\n",
      "  - CEN: 8 (14.8%)\n",
      "Tours:\n",
      "  - Tour 2: 52 (96.3%)\n",
      "  - Tour 1: 2 (3.7%)\n",
      "Exemples:\n",
      "  - François Archambault de Montfort (GAUL, 1958, T2)\n",
      "  - André Burlot (CEN, 1958, T2)\n",
      "  - André Burlot (CEN, 1962, T2)\n",
      "\n",
      "CLUSTER 3 (49 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - COM: 17 (34.7%)\n",
      "  - EXG: 14 (28.6%)\n",
      "  - SOC: 9 (18.4%)\n",
      "Tours:\n",
      "  - Tour 1: 29 (59.2%)\n",
      "  - Tour 2: 20 (40.8%)\n",
      "Exemples:\n",
      "  - Claude Bedu (EXG, 1978, T1)\n",
      "  - Marcel Chartrain (COM, 1958, T2)\n",
      "  - Roger Corrèze (GAUL, 1968, T2)\n",
      "Création de la visualisation t-SNE avec clusters...\n",
      "Visualisation avec clusters sauvegardée: clustering_tours_combined.html\n",
      "Analyse de clustering terminée\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script de clustering pour l'analyse des discours politiques Tours 1+2\n",
    "Basé sur le code d'analyse existant avec ajout du clustering et visualisation des clusters\n",
    "\"\"\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ClusteringToursAnalyzer:\n",
    "    def __init__(self, filepath='/Users/charlielezin/Desktop/Candidatures58-81-250825.csv'):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "        self.similarity_matrix = None\n",
    "        self.vectorizer = None\n",
    "        self.embeddings_2d = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.clusters = None\n",
    "        self.n_clusters = None\n",
    "        \n",
    "    def load_and_filter_data(self):\n",
    "        \"\"\"\n",
    "        Charge les données CSV et filtre pour les tours 1+2\n",
    "        \"\"\"\n",
    "        print(\"Chargement des données (Tours 1+2)...\")\n",
    "        self.df = pd.read_csv(self.filepath, delimiter=';', encoding='utf-8')\n",
    "        \n",
    "        print(f\"Nombre total de lignes : {len(self.df)}\")\n",
    "        \n",
    "        # Filtrage pour les tours 1 et 2\n",
    "        self.df = self.df[self.df['tour'].isin([1, 2])].copy()\n",
    "        print(f\"Filtrage tours 1+2 : {len(self.df)} candidatures\")\n",
    "        \n",
    "        # Filtrer ceux qui ont des discours non vides\n",
    "        self.df = self.df[\n",
    "            (self.df['discours'].notna()) & \n",
    "            (self.df['discours'].str.strip() != '') &\n",
    "            (self.df['discours'].str.len() > 100)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Nombre de candidatures avec discours : {len(self.df)}\")\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Répartition par tour :\")\n",
    "        print(self.df['tour'].value_counts().sort_index())\n",
    "        \n",
    "        print(f\"Répartition par nuance politique :\")\n",
    "        nuances_count = self.df['nuance_titulaire'].fillna('Inconnu').value_counts()\n",
    "        print(nuances_count.head(10))\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def extract_names_and_parties(self):\n",
    "        \"\"\"\n",
    "        Extrait tous les noms, prénoms et mentions de partis à supprimer\n",
    "        \"\"\"\n",
    "        names_to_remove = set()\n",
    "        parties_to_remove = set()\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            # Noms et prénoms titulaire et suppléant\n",
    "            for col in ['prenom_titulaire', 'nom_titulaire', 'prenom_suppleant', 'nom_suppleant']:\n",
    "                if pd.notna(row.get(col)):\n",
    "                    names_to_remove.add(str(row[col]).strip().lower())\n",
    "            \n",
    "            # Partis et sigles\n",
    "            party_columns = [\n",
    "                'parti_titulaire', 'parti_titulaire_1', 'parti_titulaire_2', \n",
    "                'parti_titulaire_3', 'parti_titulaire_4',\n",
    "                'parti_suppleant', 'parti_suppleant_1', 'parti_suppleant_2',\n",
    "                'parti_suppleant_3', 'parti_suppleant_4', 'parti_suppleant_5',\n",
    "                'sigle_titulaire', 'sigle_suppleant', 'nuance_titulaire', 'nuance_suppleant'\n",
    "            ]\n",
    "            \n",
    "            for col in party_columns:\n",
    "                if col in row and pd.notna(row[col]):\n",
    "                    party_value = str(row[col]).strip()\n",
    "                    if party_value and party_value.lower() not in ['nan', 'none', '']:\n",
    "                        party_parts = re.split(r'[+\\-\\s&/]', party_value)\n",
    "                        for part in party_parts:\n",
    "                            if part.strip() and len(part.strip()) > 1:\n",
    "                                parties_to_remove.add(part.strip().lower())\n",
    "        \n",
    "        # Termes de parti communs\n",
    "        common_parties = [\n",
    "            'gaulliste', 'gaullistes', 'socialiste', 'socialistes', 'communiste', 'communistes',\n",
    "            'radical', 'radicaux', 'centriste', 'centristes', 'républicain', 'républicains',\n",
    "            'démocrate', 'démocrates', 'indépendant', 'indépendants', 'paysan', 'paysans',\n",
    "            'union', 'mouvement', 'parti', 'rassemblement', 'front', 'coalition',\n",
    "            'nouvelle', 'république', 'nationale', 'populaire', 'française', 'français',\n",
    "            'gaulle', 'gaulles', 'gaul', 'com', 'soc', 'sfio', 'mrp', 'cnip', 'unr',\n",
    "            'psu', 'agr', 'exg', 'exd', 'div', 'rdg', 'rad', 'cen'\n",
    "        ]\n",
    "        \n",
    "        parties_to_remove.update(common_parties)\n",
    "        \n",
    "        # Nettoyer les sets\n",
    "        names_to_remove = {name for name in names_to_remove if name and len(name) > 1}\n",
    "        parties_to_remove = {party for party in parties_to_remove if party and len(party) > 2}\n",
    "        \n",
    "        print(f\"Anonymisation:\")\n",
    "        print(f\"   Noms/prénoms à supprimer: {len(names_to_remove)}\")\n",
    "        print(f\"   Termes de partis à supprimer: {len(parties_to_remove)}\")\n",
    "        \n",
    "        return names_to_remove, parties_to_remove\n",
    "    \n",
    "    def anonymize_text(self, text, names_to_remove, parties_to_remove):\n",
    "        \"\"\"\n",
    "        Anonymise un texte en supprimant les noms et mentions de partis\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Supprimer les noms et prénoms\n",
    "        for name in names_to_remove:\n",
    "            text = re.sub(rf'\\b{re.escape(name)}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Supprimer les mentions de partis\n",
    "        for party in parties_to_remove:\n",
    "            text = re.sub(rf'\\b{re.escape(party)}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Nettoyage général\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_french_stop_words(self):\n",
    "        \"\"\"\n",
    "        Liste étendue de mots vides français\n",
    "        \"\"\"\n",
    "        stop_words_base = [\n",
    "            'le', 'de', 'et', 'à', 'un', 'il', 'être', 'en', 'avoir', 'que', 'pour',\n",
    "            'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout', 'plus',\n",
    "            'par', 'grand', 'comme', 'mais', 'faire', 'mettre', 'nom', 'dire', 'ces',\n",
    "            'mon', 'où', 'même', 'y', 'aller', 'deux', 'moi', 'si', 'haut', 'bien', 'autre',\n",
    "            'fois', 'très', 'là', 'voir', 'arriver', 'donner', 'elle', 'lui', 'tel', 'quel',\n",
    "            'qui', 'du', 'des', 'aux', 'cette', 'leurs', 'nos', 'votre', 'dont', 'cette'\n",
    "        ]\n",
    "        \n",
    "        stop_words_political = [\n",
    "            'vous', 'nous', 'électeurs', 'électrices', 'france', 'français', 'française',\n",
    "            'république', 'national', 'nationale', 'gouvernement', 'politique', 'pays',\n",
    "            'état', 'public', 'sociale', 'social', 'économique', 'candidat', 'candidats',\n",
    "            'voter', 'vote', 'élection', 'élections', 'législatives', 'député', 'assemblée',\n",
    "            'monsieur', 'madame', 'citoyens', 'citoyennes', 'peuple', 'nation',\n",
    "            'suppléant', 'titulaire', 'liste', 'scrutin', 'circonscription'\n",
    "        ]\n",
    "        \n",
    "        return stop_words_base + stop_words_political\n",
    "    \n",
    "    def create_color_palette(self, nuances):\n",
    "        \"\"\"\n",
    "        Palette de couleurs pour les nuances politiques\n",
    "        \"\"\"\n",
    "        color_map = {\n",
    "            'COM': '#FF0000', 'COMM': '#FF0000', 'GAUL': '#191970', 'SOC': '#FFB6C1',\n",
    "            'CEN': '#87CEEB', 'EXG': '#8B0000', 'RAD': '#DAA520', 'PSU': '#FF8C00',\n",
    "            'AGR': '#008000', 'CEN-D': '#0000FF', 'EXD': '#000000', 'DIV': '#808080',\n",
    "            'RDG': '#FFE4E1', 'SFIO': '#FFB6C1', 'MRP': '#FF8C00', 'UDSR': '#87CEEB',\n",
    "            'RGR': '#FFE4E1', 'DROITE': '#191970', 'DROIT': '#191970', 'IND': '#808080',\n",
    "            'PAYSANS': '#008000'\n",
    "        }\n",
    "        \n",
    "        # Gérer les nuances manquantes\n",
    "        unique_nuances = list(set(nuances))\n",
    "        remaining_nuances = [n for n in unique_nuances if n not in color_map and pd.notna(n)]\n",
    "        \n",
    "        if remaining_nuances:\n",
    "            additional_colors = ['#DDA0DD', '#F0E68C', '#98FB98', '#F5DEB3', \n",
    "                               '#D2B48C', '#BC8F8F', '#CD853F', '#A0522D']\n",
    "            for i, nuance in enumerate(remaining_nuances):\n",
    "                color_map[nuance] = additional_colors[i % len(additional_colors)]\n",
    "        \n",
    "        color_map[np.nan] = '#CCCCCC'\n",
    "        color_map['Inconnu'] = '#CCCCCC'\n",
    "        color_map[None] = '#CCCCCC'\n",
    "        \n",
    "        return color_map\n",
    "    \n",
    "    def calculate_tfidf_similarity(self):\n",
    "        \"\"\"\n",
    "        Calcule la matrice TF-IDF et la similarité sur les textes anonymisés\n",
    "        \"\"\"\n",
    "        print(\"Anonymisation des textes...\")\n",
    "        \n",
    "        # Extraire les noms et partis à supprimer\n",
    "        names_to_remove, parties_to_remove = self.extract_names_and_parties()\n",
    "        \n",
    "        # Anonymiser tous les discours\n",
    "        self.df['discours_anonymized'] = self.df['discours'].apply(\n",
    "            lambda x: self.anonymize_text(x, names_to_remove, parties_to_remove)\n",
    "        )\n",
    "        \n",
    "        # Filtrer les textes trop courts après anonymisation\n",
    "        self.df = self.df[self.df['discours_anonymized'].str.len() > 50].copy()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Nombre de discours après anonymisation : {len(self.df)}\")\n",
    "        \n",
    "        if len(self.df) < 5:\n",
    "            raise ValueError(\"Pas assez de discours après anonymisation!\")\n",
    "        \n",
    "        print(\"Calcul de la matrice TF-IDF sur textes anonymisés...\")\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            stop_words=self.get_french_stop_words(),\n",
    "            ngram_range=(1, 2),\n",
    "            lowercase=True,\n",
    "            strip_accents='unicode'\n",
    "        )\n",
    "        \n",
    "        texts = self.df['discours_anonymized'].tolist()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        \n",
    "        print(f\"Matrice TF-IDF : {self.tfidf_matrix.shape}\")\n",
    "        \n",
    "        print(\"Calcul de la similarité cosinus...\")\n",
    "        self.similarity_matrix = cosine_similarity(self.tfidf_matrix)\n",
    "        \n",
    "        # Statistiques\n",
    "        similarity_upper = np.triu(self.similarity_matrix, k=1)\n",
    "        non_zero_similarities = similarity_upper[similarity_upper > 0]\n",
    "        \n",
    "        print(f\"Statistiques de similarité (textes anonymisés) :\")\n",
    "        print(f\"  - Moyenne : {non_zero_similarities.mean():.3f}\")\n",
    "        print(f\"  - Médiane : {np.median(non_zero_similarities):.3f}\")\n",
    "        print(f\"  - Max : {non_zero_similarities.max():.3f}\")\n",
    "        print(f\"  - Min : {non_zero_similarities.min():.3f}\")\n",
    "        \n",
    "        # Calcul du t-SNE\n",
    "        print(\"Calcul du t-SNE...\")\n",
    "        tsne = TSNE(n_components=2, perplexity=min(30, len(self.df)-1), \n",
    "                   random_state=42, verbose=0, max_iter=1000)\n",
    "        self.embeddings_2d = tsne.fit_transform(self.tfidf_matrix.toarray())\n",
    "        \n",
    "        return self.tfidf_matrix\n",
    "    \n",
    "    def find_optimal_clusters_dbscan(self):\n",
    "        \"\"\"Utilise DBSCAN pour identifier les clusters denses\"\"\"\n",
    "        print(\"Clustering DBSCAN sur l'espace TF-IDF haute dimension...\")\n",
    "        \n",
    "        # Tester différents paramètres eps pour DBSCAN\n",
    "        eps_values = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        best_eps = None\n",
    "        best_n_clusters = 0\n",
    "        best_noise_ratio = 1.0\n",
    "        \n",
    "        for eps in eps_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "            cluster_labels = dbscan.fit_predict(self.tfidf_matrix.toarray())\n",
    "            \n",
    "            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "            noise_ratio = np.sum(cluster_labels == -1) / len(cluster_labels)\n",
    "            \n",
    "            print(f\"  eps={eps}: {n_clusters} clusters, {noise_ratio:.1%} bruit\")\n",
    "            \n",
    "            # Choisir la configuration avec un bon équilibre\n",
    "            if n_clusters >= 2 and n_clusters <= 8 and noise_ratio < 0.5:\n",
    "                if best_eps is None or (n_clusters > best_n_clusters and noise_ratio < best_noise_ratio):\n",
    "                    best_eps = eps\n",
    "                    best_n_clusters = n_clusters\n",
    "                    best_noise_ratio = noise_ratio\n",
    "        \n",
    "        if best_eps is None:\n",
    "            print(\"DBSCAN n'a pas trouvé de bonne configuration, utilisation de K-means avec k=4\")\n",
    "            return 4, \"kmeans\"\n",
    "        \n",
    "        print(f\"Meilleur paramètre DBSCAN : eps={best_eps} ({best_n_clusters} clusters)\")\n",
    "        return best_eps, \"dbscan\"\n",
    "    \n",
    "    def perform_clustering(self, n_clusters=None):\n",
    "        \"\"\"Effectue le clustering sur l'espace TF-IDF haute dimension\"\"\"\n",
    "        \n",
    "        # Déterminer la méthode et paramètres optimaux\n",
    "        param, method = self.find_optimal_clusters_dbscan()\n",
    "        \n",
    "        if method == \"dbscan\":\n",
    "            print(f\"Clustering DBSCAN avec eps={param}...\")\n",
    "            dbscan = DBSCAN(eps=param, min_samples=5)\n",
    "            cluster_labels = dbscan.fit_predict(self.tfidf_matrix.toarray())\n",
    "            \n",
    "            # Traiter le bruit (-1) comme un cluster séparé\n",
    "            self.clusters = cluster_labels\n",
    "            unique_clusters = set(cluster_labels)\n",
    "            self.n_clusters = len(unique_clusters)\n",
    "            \n",
    "        else:  # kmeans fallback\n",
    "            print(f\"Clustering K-means avec {param} clusters...\")\n",
    "            kmeans = KMeans(n_clusters=param, random_state=42, n_init=10)\n",
    "            self.clusters = kmeans.fit_predict(self.tfidf_matrix.toarray())\n",
    "            self.n_clusters = param\n",
    "        \n",
    "        # Ajouter les clusters au DataFrame\n",
    "        self.df['cluster'] = self.clusters\n",
    "        \n",
    "        print(\"Répartition des clusters :\")\n",
    "        unique_labels = set(self.clusters)\n",
    "        for label in sorted(unique_labels):\n",
    "            cluster_size = np.sum(self.clusters == label)\n",
    "            if label == -1:\n",
    "                print(f\"  Bruit: {cluster_size} documents\")\n",
    "            else:\n",
    "                print(f\"  Cluster {label}: {cluster_size} documents\")\n",
    "        \n",
    "        return self.clusters\n",
    "    \n",
    "    def analyze_clusters(self):\n",
    "        \"\"\"Analyse détaillée des clusters\"\"\"\n",
    "        print(f\"ANALYSE DES CLUSTERS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        unique_labels = set(self.clusters)\n",
    "        for label in sorted(unique_labels):\n",
    "            if label == -1:\n",
    "                cluster_docs = self.df[self.df['cluster'] == -1]\n",
    "                print(f\"\\nBRUIT ({len(cluster_docs)} documents)\")\n",
    "                print(\"-\" * 30)\n",
    "            else:\n",
    "                cluster_docs = self.df[self.df['cluster'] == label]\n",
    "                print(f\"\\nCLUSTER {label} ({len(cluster_docs)} documents)\")\n",
    "                print(\"-\" * 30)\n",
    "            \n",
    "            # Répartition par nuance\n",
    "            nuances = cluster_docs['nuance_titulaire'].fillna('Inconnu').value_counts()\n",
    "            print(f\"Nuances principales:\")\n",
    "            for nuance, count in nuances.head(3).items():\n",
    "                pct = (count / len(cluster_docs)) * 100\n",
    "                print(f\"  - {nuance}: {count} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Répartition par tour\n",
    "            tours = cluster_docs['tour'].value_counts()\n",
    "            print(f\"Tours:\")\n",
    "            for tour, count in tours.items():\n",
    "                pct = (count / len(cluster_docs)) * 100\n",
    "                print(f\"  - Tour {tour}: {count} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Exemples de candidats\n",
    "            print(\"Exemples:\")\n",
    "            for _, row in cluster_docs.head(3).iterrows():\n",
    "                nom = f\"{row['prenom_titulaire']} {row['nom_titulaire']}\"\n",
    "                nuance = row['nuance_titulaire'] if pd.notna(row['nuance_titulaire']) else 'Inconnu'\n",
    "                tour = row.get('tour', 'N/A')\n",
    "                annee = int(row['annee_scrutin']) if pd.notna(row['annee_scrutin']) else 'N/A'\n",
    "                print(f\"  - {nom} ({nuance}, {annee}, T{tour})\")\n",
    "    \n",
    "    def get_cluster_hull_points(self, cluster_id):\n",
    "        \"\"\"Calcule l'enveloppe des points d'un cluster\"\"\"\n",
    "        if cluster_id == -1:\n",
    "            # Pour le bruit, pas d'enveloppe\n",
    "            return None\n",
    "            \n",
    "        cluster_mask = self.clusters == cluster_id\n",
    "        if np.sum(cluster_mask) < 3:\n",
    "            return None\n",
    "        \n",
    "        cluster_points = self.embeddings_2d[cluster_mask]\n",
    "        \n",
    "        try:\n",
    "            from scipy.spatial import ConvexHull\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            hull_points = np.vstack([hull_points, hull_points[0]])\n",
    "            return hull_points\n",
    "        except:\n",
    "            # Utiliser un cercle approximatif si ConvexHull échoue\n",
    "            center = np.mean(cluster_points, axis=0)\n",
    "            distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "            radius = np.max(distances) * 1.2\n",
    "            \n",
    "            angles = np.linspace(0, 2*np.pi, 30)\n",
    "            circle_points = np.column_stack([\n",
    "                center[0] + radius * np.cos(angles),\n",
    "                center[1] + radius * np.sin(angles)\n",
    "            ])\n",
    "            return circle_points\n",
    "    \n",
    "    def create_interactive_tsne_with_clusters(self):\n",
    "        \"\"\"\n",
    "        Visualisation t-SNE interactive avec clusters entourés\n",
    "        \"\"\"\n",
    "        print(\"Création de la visualisation t-SNE avec clusters...\")\n",
    "        \n",
    "        nuances = self.df['nuance_titulaire'].fillna('Inconnu')\n",
    "        color_palette = self.create_color_palette(nuances)\n",
    "        \n",
    "        # Couleurs pour les enveloppes de clusters\n",
    "        cluster_colors = qualitative.Plotly[:self.n_clusters]\n",
    "        \n",
    "        hover_texts = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            nom = f\"{row['prenom_titulaire']} {row['nom_titulaire']}\"\n",
    "            nuance = row['nuance_titulaire'] if pd.notna(row['nuance_titulaire']) else 'Inconnu'\n",
    "            annee = int(row['annee_scrutin']) if pd.notna(row['annee_scrutin']) else 'N/A'\n",
    "            tour = row.get('tour', 'N/A')\n",
    "            cluster_id = row.get('cluster', 'N/A')\n",
    "            departement = row.get('departement_nom', 'N/A')\n",
    "            circonscription = row.get('identifiant_circonscription', 'N/A')\n",
    "            \n",
    "            hover_text = f\"<b>{nom}</b><br><b>Nuance:</b> {nuance}<br><b>Cluster:</b> {cluster_id}<br><b>Année:</b> {annee}<br><b>Tour:</b> {tour}<br><b>Département:</b> {departement}<br><b>Circonscription:</b> {circonscription}<br><i>Analyse sur texte anonymisé</i>\"\n",
    "            hover_texts.append(hover_text)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Ajouter d'abord les enveloppes des clusters valides\n",
    "        unique_labels = set(self.clusters)\n",
    "        cluster_colors = qualitative.Plotly[:len(unique_labels)]\n",
    "        color_idx = 0\n",
    "        \n",
    "        for cluster_id in sorted(unique_labels):\n",
    "            if cluster_id != -1:  # Ignorer le bruit pour les enveloppes\n",
    "                hull_points = self.get_cluster_hull_points(cluster_id)\n",
    "                if hull_points is not None:\n",
    "                    cluster_size = np.sum(self.clusters == cluster_id)\n",
    "                    color_hex = cluster_colors[color_idx]\n",
    "                    \n",
    "                    # Convertir la couleur hex en RGB pour le fill\n",
    "                    r = int(color_hex[1:3], 16)\n",
    "                    g = int(color_hex[3:5], 16)\n",
    "                    b = int(color_hex[5:7], 16)\n",
    "                    \n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=hull_points[:, 0],\n",
    "                        y=hull_points[:, 1],\n",
    "                        mode='lines',\n",
    "                        line=dict(color=color_hex, width=2, dash='dash'),\n",
    "                        name=f'Cluster {cluster_id} ({cluster_size} docs)',\n",
    "                        fill='toself',\n",
    "                        fillcolor=f'rgba({r}, {g}, {b}, 0.1)',\n",
    "                        opacity=0.6,\n",
    "                        showlegend=True\n",
    "                    ))\n",
    "                color_idx += 1\n",
    "        \n",
    "        # Ajouter ensuite les points par nuance politique\n",
    "        for nuance in set(nuances):\n",
    "            mask = nuances == nuance\n",
    "            if sum(mask) > 0:\n",
    "                indices = np.where(mask)[0]\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=self.embeddings_2d[mask, 0],\n",
    "                    y=self.embeddings_2d[mask, 1],\n",
    "                    mode='markers',\n",
    "                    name=f'{nuance} (n={sum(mask)})',\n",
    "                    marker=dict(\n",
    "                        color=color_palette.get(nuance, '#CCCCCC'),\n",
    "                        size=8,\n",
    "                        line=dict(width=1, color='black'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    hovertemplate='%{customdata}<extra></extra>',\n",
    "                    customdata=[hover_texts[i] for i in indices]\n",
    "                ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Analyse t-SNE avec {self.n_clusters} Clusters - Tours 1+2<br><sub>Clusters entourés sur textes anonymisés</sub>',\n",
    "            xaxis_title='Dimension t-SNE 1',\n",
    "            yaxis_title='Dimension t-SNE 2',\n",
    "            width=1400,\n",
    "            height=900,\n",
    "            hovermode='closest',\n",
    "            showlegend=True,\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=1.01),\n",
    "            plot_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Analyse principale avec clustering\n",
    "    \"\"\"\n",
    "    print(\"ANALYSE DE CLUSTERING - DISCOURS POLITIQUES TOURS 1+2\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        analyzer = ClusteringToursAnalyzer()\n",
    "        \n",
    "        # 1. Chargement des données\n",
    "        analyzer.load_and_filter_data()\n",
    "        \n",
    "        if len(analyzer.df) == 0:\n",
    "            print(\"Aucun discours trouvé!\")\n",
    "            return\n",
    "        \n",
    "        # 2. Calcul TF-IDF avec anonymisation\n",
    "        analyzer.calculate_tfidf_similarity()\n",
    "        \n",
    "        # 3. Clustering\n",
    "        analyzer.perform_clustering()\n",
    "        \n",
    "        # 4. Analyse des clusters\n",
    "        analyzer.analyze_clusters()\n",
    "        \n",
    "        # 5. Visualisation interactive avec clusters entourés\n",
    "        fig = analyzer.create_interactive_tsne_with_clusters()\n",
    "        fig.write_html(\"clustering_tours_combined.html\")\n",
    "        \n",
    "        print(f\"Visualisation avec clusters sauvegardée: clustering_tours_combined.html\")\n",
    "        print(\"Analyse de clustering terminée\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7bdaaf-5e38-40a8-8e56-071421e34aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "565c3945-27b2-46ca-bb61-dd2950d135a7",
   "metadata": {},
   "source": [
    "## Analyse tridimentionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc53627f-fab3-4a07-8c3c-21868a191ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE DE CLUSTERING - DISCOURS POLITIQUES TOURS 1+2\n",
      "============================================================\n",
      "Chargement des données (Tours 1+2)...\n",
      "Nombre total de lignes : 303\n",
      "Filtrage tours 1+2 : 303 candidatures\n",
      "Nombre de candidatures avec discours : 303\n",
      "Répartition par tour :\n",
      "tour\n",
      "1    217\n",
      "2     86\n",
      "Name: count, dtype: int64\n",
      "Répartition par nuance politique :\n",
      "nuance_titulaire\n",
      "GAUL     62\n",
      "SOC      58\n",
      "COM      54\n",
      "CEN      36\n",
      "RAD      21\n",
      "EXG      15\n",
      "CEN-D    15\n",
      "PSU      11\n",
      "AGR      11\n",
      "EXD      10\n",
      "Name: count, dtype: int64\n",
      "Anonymisation des textes...\n",
      "Anonymisation:\n",
      "   Noms/prénoms à supprimer: 370\n",
      "   Termes de partis à supprimer: 135\n",
      "Nombre de discours après anonymisation : 303\n",
      "Calcul de la matrice TF-IDF sur textes anonymisés...\n",
      "Matrice TF-IDF : (303, 1000)\n",
      "Calcul de la similarité cosinus...\n",
      "Statistiques de similarité (textes anonymisés) :\n",
      "  - Moyenne : 0.165\n",
      "  - Médiane : 0.156\n",
      "  - Max : 1.000\n",
      "  - Min : 0.003\n",
      "Calcul du t-SNE 3D...\n",
      "Clustering DBSCAN sur l'espace TF-IDF haute dimension...\n",
      "  eps=0.3: 1 clusters, 98.0% bruit\n",
      "  eps=0.4: 1 clusters, 98.0% bruit\n",
      "  eps=0.5: 1 clusters, 98.0% bruit\n",
      "  eps=0.6: 1 clusters, 98.0% bruit\n",
      "  eps=0.7: 1 clusters, 98.0% bruit\n",
      "DBSCAN n'a pas trouvé de bonne configuration, utilisation de K-means avec k=4\n",
      "Clustering K-means avec 4 clusters...\n",
      "Répartition des clusters :\n",
      "  Cluster 0: 102 documents\n",
      "  Cluster 1: 98 documents\n",
      "  Cluster 2: 54 documents\n",
      "  Cluster 3: 49 documents\n",
      "ANALYSE DES CLUSTERS\n",
      "==================================================\n",
      "\n",
      "CLUSTER 0 (102 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - GAUL: 26 (25.5%)\n",
      "  - CEN: 24 (23.5%)\n",
      "  - SOC: 17 (16.7%)\n",
      "Tours:\n",
      "  - Tour 1: 93 (91.2%)\n",
      "  - Tour 2: 9 (8.8%)\n",
      "Exemples:\n",
      "  - François Archambault de Montfort (GAUL, 1958, T1)\n",
      "  - François Archambault de Montfort (GAUL, 1962, T1)\n",
      "  - Gérard Belorgey (DIV, 1981, T1)\n",
      "\n",
      "CLUSTER 1 (98 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - COM: 35 (35.7%)\n",
      "  - SOC: 16 (16.3%)\n",
      "  - GAUL: 14 (14.3%)\n",
      "Tours:\n",
      "  - Tour 1: 93 (94.9%)\n",
      "  - Tour 2: 5 (5.1%)\n",
      "Exemples:\n",
      "  - Paul Antier (CEN, 1967, T1)\n",
      "  - Jean Auger (COM, 1973, T1)\n",
      "  - Jean Billeau (PSU, 1968, T1)\n",
      "\n",
      "CLUSTER 2 (54 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - GAUL: 19 (35.2%)\n",
      "  - SOC: 16 (29.6%)\n",
      "  - CEN: 8 (14.8%)\n",
      "Tours:\n",
      "  - Tour 2: 52 (96.3%)\n",
      "  - Tour 1: 2 (3.7%)\n",
      "Exemples:\n",
      "  - François Archambault de Montfort (GAUL, 1958, T2)\n",
      "  - André Burlot (CEN, 1958, T2)\n",
      "  - André Burlot (CEN, 1962, T2)\n",
      "\n",
      "CLUSTER 3 (49 documents)\n",
      "------------------------------\n",
      "Nuances principales:\n",
      "  - COM: 17 (34.7%)\n",
      "  - EXG: 14 (28.6%)\n",
      "  - SOC: 9 (18.4%)\n",
      "Tours:\n",
      "  - Tour 1: 29 (59.2%)\n",
      "  - Tour 2: 20 (40.8%)\n",
      "Exemples:\n",
      "  - Claude Bedu (EXG, 1978, T1)\n",
      "  - Marcel Chartrain (COM, 1958, T2)\n",
      "  - Roger Corrèze (GAUL, 1968, T2)\n",
      "Création de la visualisation t-SNE 3D avec clusters...\n",
      "Visualisation 3D avec clusters sauvegardée: clustering3D.html\n",
      "Analyse de clustering terminée\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script de clustering pour l'analyse des discours politiques Tours 1+2\n",
    "Basé sur le code d'analyse existant avec ajout du clustering et visualisation des clusters\n",
    "\"\"\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ClusteringToursAnalyzer:\n",
    "    def __init__(self, filepath='/Users/charlielezin/Desktop/Candidatures58-81-250825.csv'):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "        self.similarity_matrix = None\n",
    "        self.vectorizer = None\n",
    "        self.embeddings_3d = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.clusters = None\n",
    "        self.n_clusters = None\n",
    "        \n",
    "    def load_and_filter_data(self):\n",
    "        \"\"\"\n",
    "        Charge les données CSV et filtre pour les tours 1+2\n",
    "        \"\"\"\n",
    "        print(\"Chargement des données (Tours 1+2)...\")\n",
    "        self.df = pd.read_csv(self.filepath, delimiter=';', encoding='utf-8')\n",
    "        \n",
    "        print(f\"Nombre total de lignes : {len(self.df)}\")\n",
    "        \n",
    "        # Filtrage pour les tours 1 et 2\n",
    "        self.df = self.df[self.df['tour'].isin([1, 2])].copy()\n",
    "        print(f\"Filtrage tours 1+2 : {len(self.df)} candidatures\")\n",
    "        \n",
    "        # Filtrer ceux qui ont des discours non vides\n",
    "        self.df = self.df[\n",
    "            (self.df['discours'].notna()) & \n",
    "            (self.df['discours'].str.strip() != '') &\n",
    "            (self.df['discours'].str.len() > 100)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Nombre de candidatures avec discours : {len(self.df)}\")\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Répartition par tour :\")\n",
    "        print(self.df['tour'].value_counts().sort_index())\n",
    "        \n",
    "        print(f\"Répartition par nuance politique :\")\n",
    "        nuances_count = self.df['nuance_titulaire'].fillna('Inconnu').value_counts()\n",
    "        print(nuances_count.head(10))\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def extract_names_and_parties(self):\n",
    "        \"\"\"\n",
    "        Extrait tous les noms, prénoms et mentions de partis à supprimer\n",
    "        \"\"\"\n",
    "        names_to_remove = set()\n",
    "        parties_to_remove = set()\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            # Noms et prénoms titulaire et suppléant\n",
    "            for col in ['prenom_titulaire', 'nom_titulaire', 'prenom_suppleant', 'nom_suppleant']:\n",
    "                if pd.notna(row.get(col)):\n",
    "                    names_to_remove.add(str(row[col]).strip().lower())\n",
    "            \n",
    "            # Partis et sigles\n",
    "            party_columns = [\n",
    "                'parti_titulaire', 'parti_titulaire_1', 'parti_titulaire_2', \n",
    "                'parti_titulaire_3', 'parti_titulaire_4',\n",
    "                'parti_suppleant', 'parti_suppleant_1', 'parti_suppleant_2',\n",
    "                'parti_suppleant_3', 'parti_suppleant_4', 'parti_suppleant_5',\n",
    "                'sigle_titulaire', 'sigle_suppleant', 'nuance_titulaire', 'nuance_suppleant'\n",
    "            ]\n",
    "            \n",
    "            for col in party_columns:\n",
    "                if col in row and pd.notna(row[col]):\n",
    "                    party_value = str(row[col]).strip()\n",
    "                    if party_value and party_value.lower() not in ['nan', 'none', '']:\n",
    "                        party_parts = re.split(r'[+\\-\\s&/]', party_value)\n",
    "                        for part in party_parts:\n",
    "                            if part.strip() and len(part.strip()) > 1:\n",
    "                                parties_to_remove.add(part.strip().lower())\n",
    "        \n",
    "        # Termes de parti communs\n",
    "        common_parties = [\n",
    "            'gaulliste', 'gaullistes', 'socialiste', 'socialistes', 'communiste', 'communistes',\n",
    "            'radical', 'radicaux', 'centriste', 'centristes', 'républicain', 'républicains',\n",
    "            'démocrate', 'démocrates', 'indépendant', 'indépendants', 'paysan', 'paysans',\n",
    "            'union', 'mouvement', 'parti', 'rassemblement', 'front', 'coalition',\n",
    "            'nouvelle', 'république', 'nationale', 'populaire', 'française', 'français',\n",
    "            'gaulle', 'gaulles', 'gaul', 'com', 'soc', 'sfio', 'mrp', 'cnip', 'unr',\n",
    "            'psu', 'agr', 'exg', 'exd', 'div', 'rdg', 'rad', 'cen'\n",
    "        ]\n",
    "        \n",
    "        parties_to_remove.update(common_parties)\n",
    "        \n",
    "        # Nettoyer les sets\n",
    "        names_to_remove = {name for name in names_to_remove if name and len(name) > 1}\n",
    "        parties_to_remove = {party for party in parties_to_remove if party and len(party) > 2}\n",
    "        \n",
    "        print(f\"Anonymisation:\")\n",
    "        print(f\"   Noms/prénoms à supprimer: {len(names_to_remove)}\")\n",
    "        print(f\"   Termes de partis à supprimer: {len(parties_to_remove)}\")\n",
    "        \n",
    "        return names_to_remove, parties_to_remove\n",
    "    \n",
    "    def anonymize_text(self, text, names_to_remove, parties_to_remove):\n",
    "        \"\"\"\n",
    "        Anonymise un texte en supprimant les noms et mentions de partis\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Supprimer les noms et prénoms\n",
    "        for name in names_to_remove:\n",
    "            text = re.sub(rf'\\b{re.escape(name)}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Supprimer les mentions de partis\n",
    "        for party in parties_to_remove:\n",
    "            text = re.sub(rf'\\b{re.escape(party)}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Nettoyage général\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_french_stop_words(self):\n",
    "        \"\"\"\n",
    "        Liste étendue de mots vides français\n",
    "        \"\"\"\n",
    "        stop_words_base = [\n",
    "            'le', 'de', 'et', 'à', 'un', 'il', 'être', 'en', 'avoir', 'que', 'pour',\n",
    "            'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout', 'plus',\n",
    "            'par', 'grand', 'comme', 'mais', 'faire', 'mettre', 'nom', 'dire', 'ces',\n",
    "            'mon', 'où', 'même', 'y', 'aller', 'deux', 'moi', 'si', 'haut', 'bien', 'autre',\n",
    "            'fois', 'très', 'là', 'voir', 'arriver', 'donner', 'elle', 'lui', 'tel', 'quel',\n",
    "            'qui', 'du', 'des', 'aux', 'cette', 'leurs', 'nos', 'votre', 'dont', 'cette'\n",
    "        ]\n",
    "        \n",
    "        stop_words_political = [\n",
    "            'vous', 'nous', 'électeurs', 'électrices', 'france', 'français', 'française',\n",
    "            'république', 'national', 'nationale', 'gouvernement', 'politique', 'pays',\n",
    "            'état', 'public', 'sociale', 'social', 'économique', 'candidat', 'candidats',\n",
    "            'voter', 'vote', 'élection', 'élections', 'législatives', 'député', 'assemblée',\n",
    "            'monsieur', 'madame', 'citoyens', 'citoyennes', 'peuple', 'nation',\n",
    "            'suppléant', 'titulaire', 'liste', 'scrutin', 'circonscription'\n",
    "        ]\n",
    "        \n",
    "        return stop_words_base + stop_words_political\n",
    "    \n",
    "    def create_color_palette(self, nuances):\n",
    "        \"\"\"\n",
    "        Palette de couleurs pour les nuances politiques\n",
    "        \"\"\"\n",
    "        color_map = {\n",
    "            'COM': '#FF0000', 'COMM': '#FF0000', 'GAUL': '#191970', 'SOC': '#FFB6C1',\n",
    "            'CEN': '#87CEEB', 'EXG': '#8B0000', 'RAD': '#DAA520', 'PSU': '#FF8C00',\n",
    "            'AGR': '#008000', 'CEN-D': '#0000FF', 'EXD': '#000000', 'DIV': '#808080',\n",
    "            'RDG': '#FFE4E1', 'SFIO': '#FFB6C1', 'MRP': '#FF8C00', 'UDSR': '#87CEEB',\n",
    "            'RGR': '#FFE4E1', 'DROITE': '#191970', 'DROIT': '#191970', 'IND': '#808080',\n",
    "            'PAYSANS': '#008000'\n",
    "        }\n",
    "        \n",
    "        # Gérer les nuances manquantes\n",
    "        unique_nuances = list(set(nuances))\n",
    "        remaining_nuances = [n for n in unique_nuances if n not in color_map and pd.notna(n)]\n",
    "        \n",
    "        if remaining_nuances:\n",
    "            additional_colors = ['#DDA0DD', '#F0E68C', '#98FB98', '#F5DEB3', \n",
    "                               '#D2B48C', '#BC8F8F', '#CD853F', '#A0522D']\n",
    "            for i, nuance in enumerate(remaining_nuances):\n",
    "                color_map[nuance] = additional_colors[i % len(additional_colors)]\n",
    "        \n",
    "        color_map[np.nan] = '#CCCCCC'\n",
    "        color_map['Inconnu'] = '#CCCCCC'\n",
    "        color_map[None] = '#CCCCCC'\n",
    "        \n",
    "        return color_map\n",
    "    \n",
    "    def calculate_tfidf_similarity(self):\n",
    "        \"\"\"\n",
    "        Calcule la matrice TF-IDF et la similarité sur les textes anonymisés\n",
    "        \"\"\"\n",
    "        print(\"Anonymisation des textes...\")\n",
    "        \n",
    "        # Extraire les noms et partis à supprimer\n",
    "        names_to_remove, parties_to_remove = self.extract_names_and_parties()\n",
    "        \n",
    "        # Anonymiser tous les discours\n",
    "        self.df['discours_anonymized'] = self.df['discours'].apply(\n",
    "            lambda x: self.anonymize_text(x, names_to_remove, parties_to_remove)\n",
    "        )\n",
    "        \n",
    "        # Filtrer les textes trop courts après anonymisation\n",
    "        self.df = self.df[self.df['discours_anonymized'].str.len() > 50].copy()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Nombre de discours après anonymisation : {len(self.df)}\")\n",
    "        \n",
    "        if len(self.df) < 5:\n",
    "            raise ValueError(\"Pas assez de discours après anonymisation!\")\n",
    "        \n",
    "        print(\"Calcul de la matrice TF-IDF sur textes anonymisés...\")\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            stop_words=self.get_french_stop_words(),\n",
    "            ngram_range=(1, 2),\n",
    "            lowercase=True,\n",
    "            strip_accents='unicode'\n",
    "        )\n",
    "        \n",
    "        texts = self.df['discours_anonymized'].tolist()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        \n",
    "        print(f\"Matrice TF-IDF : {self.tfidf_matrix.shape}\")\n",
    "        \n",
    "        print(\"Calcul de la similarité cosinus...\")\n",
    "        self.similarity_matrix = cosine_similarity(self.tfidf_matrix)\n",
    "        \n",
    "        # Statistiques\n",
    "        similarity_upper = np.triu(self.similarity_matrix, k=1)\n",
    "        non_zero_similarities = similarity_upper[similarity_upper > 0]\n",
    "        \n",
    "        print(f\"Statistiques de similarité (textes anonymisés) :\")\n",
    "        print(f\"  - Moyenne : {non_zero_similarities.mean():.3f}\")\n",
    "        print(f\"  - Médiane : {np.median(non_zero_similarities):.3f}\")\n",
    "        print(f\"  - Max : {non_zero_similarities.max():.3f}\")\n",
    "        print(f\"  - Min : {non_zero_similarities.min():.3f}\")\n",
    "        \n",
    "        # Calcul du t-SNE en 3D\n",
    "        print(\"Calcul du t-SNE 3D...\")\n",
    "        tsne = TSNE(n_components=3, perplexity=min(30, len(self.df)-1), \n",
    "                   random_state=42, verbose=0, max_iter=1000)\n",
    "        self.embeddings_3d = tsne.fit_transform(self.tfidf_matrix.toarray())\n",
    "        \n",
    "        return self.tfidf_matrix\n",
    "    \n",
    "    def find_optimal_clusters_dbscan(self):\n",
    "        \"\"\"Utilise DBSCAN pour identifier les clusters denses\"\"\"\n",
    "        print(\"Clustering DBSCAN sur l'espace TF-IDF haute dimension...\")\n",
    "        \n",
    "        # Tester différents paramètres eps pour DBSCAN\n",
    "        eps_values = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        best_eps = None\n",
    "        best_n_clusters = 0\n",
    "        best_noise_ratio = 1.0\n",
    "        \n",
    "        for eps in eps_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "            cluster_labels = dbscan.fit_predict(self.tfidf_matrix.toarray())\n",
    "            \n",
    "            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "            noise_ratio = np.sum(cluster_labels == -1) / len(cluster_labels)\n",
    "            \n",
    "            print(f\"  eps={eps}: {n_clusters} clusters, {noise_ratio:.1%} bruit\")\n",
    "            \n",
    "            # Choisir la configuration avec un bon équilibre\n",
    "            if n_clusters >= 2 and n_clusters <= 8 and noise_ratio < 0.5:\n",
    "                if best_eps is None or (n_clusters > best_n_clusters and noise_ratio < best_noise_ratio):\n",
    "                    best_eps = eps\n",
    "                    best_n_clusters = n_clusters\n",
    "                    best_noise_ratio = noise_ratio\n",
    "        \n",
    "        if best_eps is None:\n",
    "            print(\"DBSCAN n'a pas trouvé de bonne configuration, utilisation de K-means avec k=4\")\n",
    "            return 4, \"kmeans\"\n",
    "        \n",
    "        print(f\"Meilleur paramètre DBSCAN : eps={best_eps} ({best_n_clusters} clusters)\")\n",
    "        return best_eps, \"dbscan\"\n",
    "    \n",
    "    def perform_clustering(self, n_clusters=None):\n",
    "        \"\"\"Effectue le clustering sur l'espace TF-IDF haute dimension\"\"\"\n",
    "        \n",
    "        # Déterminer la méthode et paramètres optimaux\n",
    "        param, method = self.find_optimal_clusters_dbscan()\n",
    "        \n",
    "        if method == \"dbscan\":\n",
    "            print(f\"Clustering DBSCAN avec eps={param}...\")\n",
    "            dbscan = DBSCAN(eps=param, min_samples=5)\n",
    "            cluster_labels = dbscan.fit_predict(self.tfidf_matrix.toarray())\n",
    "            \n",
    "            # Traiter le bruit (-1) comme un cluster séparé\n",
    "            self.clusters = cluster_labels\n",
    "            unique_clusters = set(cluster_labels)\n",
    "            self.n_clusters = len(unique_clusters)\n",
    "            \n",
    "        else:  # kmeans fallback\n",
    "            print(f\"Clustering K-means avec {param} clusters...\")\n",
    "            kmeans = KMeans(n_clusters=param, random_state=42, n_init=10)\n",
    "            self.clusters = kmeans.fit_predict(self.tfidf_matrix.toarray())\n",
    "            self.n_clusters = param\n",
    "        \n",
    "        # Ajouter les clusters au DataFrame\n",
    "        self.df['cluster'] = self.clusters\n",
    "        \n",
    "        print(\"Répartition des clusters :\")\n",
    "        unique_labels = set(self.clusters)\n",
    "        for label in sorted(unique_labels):\n",
    "            cluster_size = np.sum(self.clusters == label)\n",
    "            if label == -1:\n",
    "                print(f\"  Bruit: {cluster_size} documents\")\n",
    "            else:\n",
    "                print(f\"  Cluster {label}: {cluster_size} documents\")\n",
    "        \n",
    "        return self.clusters\n",
    "    \n",
    "    def analyze_clusters(self):\n",
    "        \"\"\"Analyse détaillée des clusters\"\"\"\n",
    "        print(f\"ANALYSE DES CLUSTERS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        unique_labels = set(self.clusters)\n",
    "        for label in sorted(unique_labels):\n",
    "            if label == -1:\n",
    "                cluster_docs = self.df[self.df['cluster'] == -1]\n",
    "                print(f\"\\nBRUIT ({len(cluster_docs)} documents)\")\n",
    "                print(\"-\" * 30)\n",
    "            else:\n",
    "                cluster_docs = self.df[self.df['cluster'] == label]\n",
    "                print(f\"\\nCLUSTER {label} ({len(cluster_docs)} documents)\")\n",
    "                print(\"-\" * 30)\n",
    "            \n",
    "            # Répartition par nuance\n",
    "            nuances = cluster_docs['nuance_titulaire'].fillna('Inconnu').value_counts()\n",
    "            print(f\"Nuances principales:\")\n",
    "            for nuance, count in nuances.head(3).items():\n",
    "                pct = (count / len(cluster_docs)) * 100\n",
    "                print(f\"  - {nuance}: {count} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Répartition par tour\n",
    "            tours = cluster_docs['tour'].value_counts()\n",
    "            print(f\"Tours:\")\n",
    "            for tour, count in tours.items():\n",
    "                pct = (count / len(cluster_docs)) * 100\n",
    "                print(f\"  - Tour {tour}: {count} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Exemples de candidats\n",
    "            print(\"Exemples:\")\n",
    "            for _, row in cluster_docs.head(3).iterrows():\n",
    "                nom = f\"{row['prenom_titulaire']} {row['nom_titulaire']}\"\n",
    "                nuance = row['nuance_titulaire'] if pd.notna(row['nuance_titulaire']) else 'Inconnu'\n",
    "                tour = row.get('tour', 'N/A')\n",
    "                annee = int(row['annee_scrutin']) if pd.notna(row['annee_scrutin']) else 'N/A'\n",
    "                print(f\"  - {nom} ({nuance}, {annee}, T{tour})\")\n",
    "    \n",
    "    def get_cluster_hull_points_3d(self, cluster_id):\n",
    "        \"\"\"Calcule l'enveloppe 3D des points d'un cluster\"\"\"\n",
    "        if cluster_id == -1:\n",
    "            return None\n",
    "            \n",
    "        cluster_mask = self.clusters == cluster_id\n",
    "        if np.sum(cluster_mask) < 4:  # Minimum pour une enveloppe 3D\n",
    "            return None\n",
    "        \n",
    "        cluster_points = self.embeddings_3d[cluster_mask]\n",
    "        \n",
    "        try:\n",
    "            from scipy.spatial import ConvexHull\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            # Pour la 3D, on retourne les faces du hull\n",
    "            return cluster_points, hull\n",
    "        except:\n",
    "            # Utiliser une sphère approximative si ConvexHull échoue\n",
    "            center = np.mean(cluster_points, axis=0)\n",
    "            distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "            radius = np.max(distances) * 1.2\n",
    "            return cluster_points, None\n",
    "    \n",
    "    def create_interactive_tsne_3d_with_clusters(self):\n",
    "        \"\"\"\n",
    "        Visualisation t-SNE 3D interactive avec clusters entourés\n",
    "        \"\"\"\n",
    "        print(\"Création de la visualisation t-SNE 3D avec clusters...\")\n",
    "        \n",
    "        nuances = self.df['nuance_titulaire'].fillna('Inconnu')\n",
    "        color_palette = self.create_color_palette(nuances)\n",
    "        \n",
    "        hover_texts = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            nom = f\"{row['prenom_titulaire']} {row['nom_titulaire']}\"\n",
    "            nuance = row['nuance_titulaire'] if pd.notna(row['nuance_titulaire']) else 'Inconnu'\n",
    "            annee = int(row['annee_scrutin']) if pd.notna(row['annee_scrutin']) else 'N/A'\n",
    "            tour = row.get('tour', 'N/A')\n",
    "            cluster_id = row.get('cluster', 'N/A')\n",
    "            departement = row.get('departement_nom', 'N/A')\n",
    "            circonscription = row.get('identifiant_circonscription', 'N/A')\n",
    "            \n",
    "            hover_text = f\"<b>{nom}</b><br><b>Nuance:</b> {nuance}<br><b>Cluster:</b> {cluster_id}<br><b>Année:</b> {annee}<br><b>Tour:</b> {tour}<br><b>Département:</b> {departement}<br><b>Circonscription:</b> {circonscription}<br><i>Analyse sur texte anonymisé</i>\"\n",
    "            hover_texts.append(hover_text)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Ajouter les points par nuance politique en 3D\n",
    "        for nuance in set(nuances):\n",
    "            mask = nuances == nuance\n",
    "            if sum(mask) > 0:\n",
    "                indices = np.where(mask)[0]\n",
    "                \n",
    "                fig.add_trace(go.Scatter3d(\n",
    "                    x=self.embeddings_3d[mask, 0],\n",
    "                    y=self.embeddings_3d[mask, 1],\n",
    "                    z=self.embeddings_3d[mask, 2],\n",
    "                    mode='markers',\n",
    "                    name=f'{nuance} (n={sum(mask)})',\n",
    "                    marker=dict(\n",
    "                        color=color_palette.get(nuance, '#CCCCCC'),\n",
    "                        size=6,\n",
    "                        line=dict(width=1, color='black'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    hovertemplate='%{customdata}<extra></extra>',\n",
    "                    customdata=[hover_texts[i] for i in indices]\n",
    "                ))\n",
    "        \n",
    "        # Ajouter des sphères pour représenter les clusters\n",
    "        cluster_colors = qualitative.Plotly[:len(set(self.clusters))]\n",
    "        color_idx = 0\n",
    "        \n",
    "        for cluster_id in sorted(set(self.clusters)):\n",
    "            if cluster_id != -1:\n",
    "                cluster_mask = self.clusters == cluster_id\n",
    "                if np.sum(cluster_mask) >= 4:\n",
    "                    cluster_points = self.embeddings_3d[cluster_mask]\n",
    "                    cluster_size = np.sum(cluster_mask)\n",
    "                    \n",
    "                    # Calculer le centre et le rayon du cluster\n",
    "                    center = np.mean(cluster_points, axis=0)\n",
    "                    distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "                    radius = np.max(distances) * 1.1\n",
    "                    \n",
    "                    # Créer une sphère transparente pour délimiter le cluster\n",
    "                    u = np.linspace(0, 2 * np.pi, 20)\n",
    "                    v = np.linspace(0, np.pi, 20)\n",
    "                    x_sphere = center[0] + radius * np.outer(np.cos(u), np.sin(v))\n",
    "                    y_sphere = center[1] + radius * np.outer(np.sin(u), np.sin(v))\n",
    "                    z_sphere = center[2] + radius * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "                    \n",
    "                    color_hex = cluster_colors[color_idx]\n",
    "                    r = int(color_hex[1:3], 16)\n",
    "                    g = int(color_hex[3:5], 16)\n",
    "                    b = int(color_hex[5:7], 16)\n",
    "                    \n",
    "                    fig.add_trace(go.Surface(\n",
    "                        x=x_sphere,\n",
    "                        y=y_sphere,\n",
    "                        z=z_sphere,\n",
    "                        opacity=0.2,\n",
    "                        colorscale=[[0, f'rgba({r}, {g}, {b}, 0.2)'], [1, f'rgba({r}, {g}, {b}, 0.2)']],\n",
    "                        showscale=False,\n",
    "                        name=f'Cluster {cluster_id} ({cluster_size} docs)',\n",
    "                        hoverinfo='name'\n",
    "                    ))\n",
    "                    \n",
    "                color_idx += 1\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Analyse t-SNE 3D avec Clusters - Tours 1+2<br><sub>Clusters délimités par des sphères transparentes sur textes anonymisés</sub>',\n",
    "            width=1400,\n",
    "            height=900,\n",
    "            scene=dict(\n",
    "                xaxis_title='Dimension t-SNE 1',\n",
    "                yaxis_title='Dimension t-SNE 2',\n",
    "                zaxis_title='Dimension t-SNE 3',\n",
    "                camera=dict(\n",
    "                    eye=dict(x=1.2, y=1.2, z=1.2)\n",
    "                )\n",
    "            ),\n",
    "            showlegend=True,\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=1.01)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Analyse principale avec clustering\n",
    "    \"\"\"\n",
    "    print(\"ANALYSE DE CLUSTERING - DISCOURS POLITIQUES TOURS 1+2\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        analyzer = ClusteringToursAnalyzer()\n",
    "        \n",
    "        # 1. Chargement des données\n",
    "        analyzer.load_and_filter_data()\n",
    "        \n",
    "        if len(analyzer.df) == 0:\n",
    "            print(\"Aucun discours trouvé!\")\n",
    "            return\n",
    "        \n",
    "        # 2. Calcul TF-IDF avec anonymisation\n",
    "        analyzer.calculate_tfidf_similarity()\n",
    "        \n",
    "        # 3. Clustering\n",
    "        analyzer.perform_clustering()\n",
    "        \n",
    "        # 4. Analyse des clusters\n",
    "        analyzer.analyze_clusters()\n",
    "        \n",
    "        # 5. Visualisation interactive 3D avec clusters entourés\n",
    "        fig = analyzer.create_interactive_tsne_3d_with_clusters()\n",
    "        fig.write_html(\"clustering3D.html\")\n",
    "        \n",
    "        print(f\"Visualisation 3D avec clusters sauvegardée: clustering3D.html\")\n",
    "        print(\"Analyse de clustering terminée\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984fa6d-9fc5-4e59-9cd1-4be6321e2a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
